# Pipeline Parallel (PP) å®ç°è¯¦è§£

## ç›®å½•
- [1. ä»€ä¹ˆæ˜¯ Pipeline Parallelï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-pipeline-parallel)
- [2. æ¬æ¡Œå­çš„æµæ°´çº¿æ¯”å–»](#2-æ¬æ¡Œå­çš„æµæ°´çº¿æ¯”å–»)
- [3. Pipeline Schedule è¯¦è§£](#3-pipeline-schedule-è¯¦è§£)
- [4. æºç å®ç°è¯¦è§£](#4-æºç å®ç°è¯¦è§£)
- [5. æ€§èƒ½åˆ†æ](#5-æ€§èƒ½åˆ†æ)
- [6. ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ](#6-ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯ Pipeline Parallelï¼Ÿ

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ Pipeline Parallelï¼Ÿ

å›é¡¾æˆ‘ä»¬å­¦è¿‡çš„å¹¶è¡Œæ–¹å¼ï¼š

| å¹¶è¡Œæ–¹å¼ | åˆ‡åˆ†å¯¹è±¡ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|
| Data Parallel (FSDP) | æ•°æ® + å‚æ•° | é€šç”¨ |
| Tensor Parallel (TP) | å•å±‚æƒé‡ | å•å±‚å¤ªå¤§ |
| Context Parallel (CP) | åºåˆ— | åºåˆ—å¤ªé•¿ |

**ä½†è¿˜æœ‰ä¸€ä¸ªé—®é¢˜**ï¼šå³ä½¿å•å±‚èƒ½æ”¾è¿› GPUï¼Œ**æ‰€æœ‰å±‚åŠ èµ·æ¥è¿˜æ˜¯å¤ªå¤§**ï¼

```python
# Llama3 405B æ¨¡å‹
num_layers = 126
hidden_dim = 16384
intermediate_size = 53248

# å•å±‚å‚æ•°é‡
params_per_layer = 4 * hidden_dim^2 + 3 * hidden_dim * intermediate_size
                 â‰ˆ 3.6B parameters

# æ€»å‚æ•°é‡
total_params = 126 * 3.6B â‰ˆ 405B parameters

# å†…å­˜éœ€æ±‚ (fp16)
memory = 405B * 2 bytes = 810 GB

# H100 80GB éœ€è¦ 810 / 80 = 10+ GPUs æ‰èƒ½æ”¾ä¸‹å‚æ•°ï¼
```

**Pipeline Parallel çš„æ€è·¯**ï¼šæŠŠæ¨¡å‹**æŒ‰å±‚åˆ‡åˆ†**ï¼Œæ¯ä¸ª GPU è´Ÿè´£ä¸€éƒ¨åˆ†å±‚ã€‚

### 1.2 Pipeline Parallel çš„æ ¸å¿ƒæ€æƒ³

**æŠŠ Transformer çš„å±‚åˆ†æˆå¤šä¸ª stageï¼Œæ¯ä¸ª stage æ”¾åœ¨ä¸åŒçš„ GPU ä¸Š**

```
åŸå§‹æ¨¡å‹ (32 layers):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding â†’ Layer0 â†’ ... â†’ Layer31 â†’ Output â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    å…¨éƒ¨åœ¨ GPU 0 (æ”¾ä¸ä¸‹ï¼)

Pipeline Parallel (PP = 4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding â”‚ â†’ â”‚ Layer8-15 â”‚ â†’ â”‚ Layer16-23â”‚ â†’ â”‚ Layer24-31â”‚
â”‚ Layer0-7  â”‚   â”‚           â”‚   â”‚           â”‚   â”‚ Norm,Outputâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   GPU 0          GPU 1          GPU 2          GPU 3
   (Stage 0)      (Stage 1)      (Stage 2)      (Stage 3)
```

**æ¯ä¸ª GPU åªéœ€è¦å­˜å‚¨ 1/4 çš„æ¨¡å‹å‚æ•°ï¼**

### 1.3 Pipeline Parallel vs å…¶ä»–å¹¶è¡Œ

| ç‰¹æ€§ | FSDP | TP | CP | PP |
|------|------|----|----|-----|
| **åˆ‡åˆ†å¯¹è±¡** | å‚æ•° (scatter) | å‚æ•° (partition) | åºåˆ— | å±‚ |
| **é€šä¿¡ç±»å‹** | All-Gather / Reduce-Scatter | All-Reduce | Ring (KV) | P2P (activations) |
| **é€šä¿¡é‡** | å¤§ | ä¸­ | å° | å° |
| **å†…å­˜èŠ‚çœ** | å‚æ•° | å‚æ•° + æ¿€æ´» | æ¿€æ´» | å‚æ•° |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨ | å•å±‚å¤ªå¤§ | åºåˆ—å¤ªé•¿ | å±‚æ•°å¤ªå¤š |

**PP çš„ç‰¹ç‚¹**ï¼š
- âœ… **é€šä¿¡é‡å°**ï¼šåªä¼ é€’å±‚ä¹‹é—´çš„æ¿€æ´»å€¼
- âœ… **å®ç°ç®€å•**ï¼šæŒ‰å±‚åˆ‡åˆ†ï¼Œä¸éœ€è¦ä¿®æ”¹å•å±‚é€»è¾‘
- âŒ **æœ‰ Bubble**ï¼šGPU ä¼šæœ‰ç©ºé—²æ—¶é—´

---

## 2. æ¬æ¡Œå­çš„æµæ°´çº¿æ¯”å–»

### 2.1 åœºæ™¯è®¾å®š

ç»§ç»­ç”¨æ¬æ¡Œå­çš„æ¯”å–»ã€‚è¿™æ¬¡æƒ³è±¡ä½ è¦ç»„è£…ä¸€å¼ **è¶…çº§å¤§çš„æ¡Œå­**ï¼Œéœ€è¦ 4 ä¸ªæ­¥éª¤ï¼š

```
æ­¥éª¤ 1: åˆ‡å‰²æœ¨æ (Embedding + Layer 0-7)
æ­¥éª¤ 2: æ‰“ç£¨æœ¨æ (Layer 8-15)
æ­¥éª¤ 3: ä¸Šæ¼† (Layer 16-23)
æ­¥éª¤ 4: ç»„è£… (Layer 24-31 + Output)
```

### 2.2 ä¼ ç»Ÿæ–¹å¼ï¼šä¸€ä¸ªäººå®Œæˆæ‰€æœ‰æ­¥éª¤

```
æ—¶é—´çº¿:
äºº1:  [åˆ‡å‰²æ¡Œå­1] â†’ [æ‰“ç£¨æ¡Œå­1] â†’ [ä¸Šæ¼†æ¡Œå­1] â†’ [ç»„è£…æ¡Œå­1]
                                                     â†“
äºº1:  [åˆ‡å‰²æ¡Œå­2] â†’ [æ‰“ç£¨æ¡Œå­2] â†’ [ä¸Šæ¼†æ¡Œå­2] â†’ [ç»„è£…æ¡Œå­2]

æ€»è€—æ—¶: 2 Ã— 4æ­¥éª¤ = 8 ä¸ªæ—¶é—´å•ä½
```

**é—®é¢˜**ï¼šä¸€ä¸ªäººè¦æŒæ¡æ‰€æœ‰å·¥åºï¼Œæ•ˆç‡ä½ä¸‹ã€‚

### 2.3 Pipeline Parallelï¼šæµæ°´çº¿ä½œä¸š

```
4 ä¸ªäººåˆ†å·¥ï¼Œæ¯äººè´Ÿè´£ä¸€é“å·¥åº

æ—¶é—´çº¿:
æ—¶åˆ»1: äºº1[åˆ‡å‰²æ¡Œå­1]
æ—¶åˆ»2: äºº1[åˆ‡å‰²æ¡Œå­2]  äºº2[æ‰“ç£¨æ¡Œå­1]
æ—¶åˆ»3: äºº1[åˆ‡å‰²æ¡Œå­3]  äºº2[æ‰“ç£¨æ¡Œå­2]  äºº3[ä¸Šæ¼†æ¡Œå­1]
æ—¶åˆ»4: äºº1[åˆ‡å‰²æ¡Œå­4]  äºº2[æ‰“ç£¨æ¡Œå­3]  äºº3[ä¸Šæ¼†æ¡Œå­2]  äºº4[ç»„è£…æ¡Œå­1]
æ—¶åˆ»5:                 äºº2[æ‰“ç£¨æ¡Œå­4]  äºº3[ä¸Šæ¼†æ¡Œå­3]  äºº4[ç»„è£…æ¡Œå­2]
æ—¶åˆ»6:                                äºº3[ä¸Šæ¼†æ¡Œå­4]  äºº4[ç»„è£…æ¡Œå­3]
æ—¶åˆ»7:                                               äºº4[ç»„è£…æ¡Œå­4]

4 å¼ æ¡Œå­è€—æ—¶: 7 ä¸ªæ—¶é—´å•ä½
ä¼ ç»Ÿæ–¹å¼: 4 Ã— 4 = 16 ä¸ªæ—¶é—´å•ä½
åŠ é€Ÿæ¯”: 16 / 7 = 2.3x ğŸš€
```

### 2.4 Bubbleï¼šæµæ°´çº¿çš„ç©ºé—²æ—¶é—´

ä»”ç»†çœ‹ä¸Šé¢çš„æ—¶é—´çº¿ï¼Œä½ ä¼šå‘ç°ï¼š

```
æ—¶åˆ»1: äºº1[å·¥ä½œ]  äºº2[ç©ºé—²]  äºº3[ç©ºé—²]  äºº4[ç©ºé—²]  â† å¼€å§‹é˜¶æ®µ
æ—¶åˆ»2: äºº1[å·¥ä½œ]  äºº2[å·¥ä½œ]  äºº3[ç©ºé—²]  äºº4[ç©ºé—²]
æ—¶åˆ»3: äºº1[å·¥ä½œ]  äºº2[å·¥ä½œ]  äºº3[å·¥ä½œ]  äºº4[ç©ºé—²]
æ—¶åˆ»4: äºº1[å·¥ä½œ]  äºº2[å·¥ä½œ]  äºº3[å·¥ä½œ]  äºº4[å·¥ä½œ]  â† æ»¡è½½
æ—¶åˆ»5: äºº1[ç©ºé—²]  äºº2[å·¥ä½œ]  äºº3[å·¥ä½œ]  äºº4[å·¥ä½œ]  â† ç»“æŸé˜¶æ®µ
æ—¶åˆ»6: äºº1[ç©ºé—²]  äºº2[ç©ºé—²]  äºº3[å·¥ä½œ]  äºº4[å·¥ä½œ]
æ—¶åˆ»7: äºº1[ç©ºé—²]  äºº2[ç©ºé—²]  äºº3[ç©ºé—²]  äºº4[å·¥ä½œ]
```

**Bubble = ç©ºé—²æ—¶é—´**
- å¼€å§‹é˜¶æ®µï¼šåé¢çš„ stage åœ¨ç­‰å‰é¢çš„è¾“å‡º
- ç»“æŸé˜¶æ®µï¼šå‰é¢çš„ stage æ²¡æœ‰æ–°ä»»åŠ¡äº†

**Bubble æ¯”ä¾‹**ï¼š
```
æ€»æ—¶é—´æ§½: 4 äºº Ã— 7 æ—¶åˆ» = 28
å®é™…å·¥ä½œ: 4 å¼ æ¡Œå­ Ã— 4 æ­¥éª¤ = 16
Bubble: 28 - 16 = 12
Bubble æ¯”ä¾‹: 12 / 28 = 43%  ğŸ˜°
```

### 2.5 å‡å°‘ Bubble çš„æ–¹æ³•

**æ–¹æ³• 1ï¼šå¢åŠ  Microbatch æ•°é‡**

```
å¢åŠ åˆ° 8 å¼ æ¡Œå­:

æ—¶åˆ»1:  äºº1[æ¡Œå­1]
æ—¶åˆ»2:  äºº1[æ¡Œå­2]  äºº2[æ¡Œå­1]
æ—¶åˆ»3:  äºº1[æ¡Œå­3]  äºº2[æ¡Œå­2]  äºº3[æ¡Œå­1]
æ—¶åˆ»4:  äºº1[æ¡Œå­4]  äºº2[æ¡Œå­3]  äºº3[æ¡Œå­2]  äºº4[æ¡Œå­1]
æ—¶åˆ»5:  äºº1[æ¡Œå­5]  äºº2[æ¡Œå­4]  äºº3[æ¡Œå­3]  äºº4[æ¡Œå­2]
æ—¶åˆ»6:  äºº1[æ¡Œå­6]  äºº2[æ¡Œå­5]  äºº3[æ¡Œå­4]  äºº4[æ¡Œå­3]
æ—¶åˆ»7:  äºº1[æ¡Œå­7]  äºº2[æ¡Œå­6]  äºº3[æ¡Œå­5]  äºº4[æ¡Œå­4]
æ—¶åˆ»8:  äºº1[æ¡Œå­8]  äºº2[æ¡Œå­7]  äºº3[æ¡Œå­6]  äºº4[æ¡Œå­5]
æ—¶åˆ»9:              äºº2[æ¡Œå­8]  äºº3[æ¡Œå­7]  äºº4[æ¡Œå­6]
æ—¶åˆ»10:                        äºº3[æ¡Œå­8]  äºº4[æ¡Œå­7]
æ—¶åˆ»11:                                   äºº4[æ¡Œå­8]

æ€»æ—¶é—´æ§½: 4 Ã— 11 = 44
å®é™…å·¥ä½œ: 8 Ã— 4 = 32
Bubble æ¯”ä¾‹: (44 - 32) / 44 = 27%

æ¯” 4 å¼ æ¡Œå­çš„ 43% å°‘å¤šäº†ï¼
```

**å…¬å¼**ï¼š
```
Bubble æ¯”ä¾‹ = (PP - 1) / (PP - 1 + n_microbatches)

n_microbatches = 4:  Bubble = 3/7 = 43%
n_microbatches = 8:  Bubble = 3/11 = 27%
n_microbatches = 16: Bubble = 3/19 = 16%
n_microbatches = 32: Bubble = 3/35 = 8.5%
```

**æ–¹æ³• 2ï¼šInterleaved Scheduleï¼ˆè™šæ‹Ÿ Stageï¼‰**

```
æ¯ä¸ªäººå­¦ä¼šä¸¤é“å·¥åºï¼

äºº1: åˆ‡å‰² + ç»„è£… (Stage 0 + Stage 3)
äºº2: æ‰“ç£¨ + ä¸Šæ¼† (Stage 1 + Stage 2)

æµç¨‹:
æ¡Œå­1: äºº1åˆ‡å‰² â†’ äºº2æ‰“ç£¨ â†’ äºº2ä¸Šæ¼† â†’ äºº1ç»„è£…
æ¡Œå­2: äºº1åˆ‡å‰² â†’ äºº2æ‰“ç£¨ â†’ äºº2ä¸Šæ¼† â†’ äºº1ç»„è£…

æ—¶é—´çº¿æ›´å¯†é›†ï¼ŒBubble æ›´å°‘ï¼
```

è¿™å°±æ˜¯ **Interleaved 1F1B** çš„æ€æƒ³ã€‚

---

## 3. Pipeline Schedule è¯¦è§£

### 3.1 GPipe Schedule

**æœ€ç®€å•çš„ scheduleï¼šæ‰€æœ‰ Forward å®Œæˆåï¼Œå†åšæ‰€æœ‰ Backward**

```
Forward pass (æ‰€æœ‰ microbatch):
F0 â†’ F1 â†’ F2 â†’ F3 â†’ ...

ç„¶å:
Backward pass (æ‰€æœ‰ microbatch):
B3 â†’ B2 â†’ B1 â†’ B0 â†’ ...

æ—¶é—´çº¿ (4 stages, 4 microbatches):
       Stage0  Stage1  Stage2  Stage3
æ—¶åˆ»1: [F0]
æ—¶åˆ»2: [F1]    [F0]
æ—¶åˆ»3: [F2]    [F1]    [F0]
æ—¶åˆ»4: [F3]    [F2]    [F1]    [F0]
æ—¶åˆ»5:         [F3]    [F2]    [F1]
æ—¶åˆ»6:                 [F3]    [F2]
æ—¶åˆ»7:                         [F3]
æ—¶åˆ»8:                         [B3]    â† Backward å¼€å§‹
æ—¶åˆ»9:                 [B3]    [B2]
æ—¶åˆ»10:        [B3]    [B2]    [B1]
æ—¶åˆ»11:[B3]    [B2]    [B1]    [B0]
æ—¶åˆ»12:[B2]    [B1]    [B0]
æ—¶åˆ»13:[B1]    [B0]
æ—¶åˆ»14:[B0]
```

**é—®é¢˜**ï¼š
- éœ€è¦ä¿å­˜æ‰€æœ‰ microbatch çš„æ¿€æ´»å€¼
- å†…å­˜æ¶ˆè€—å¤§ = O(n_microbatches)
- Bubble æ¯”ä¾‹é«˜

### 3.2 1F1B Schedule

**äº¤æ›¿æ‰§è¡Œ Forward å’Œ Backwardï¼Œå‡å°‘å†…å­˜**

```
æ—¶é—´çº¿ (4 stages, 4 microbatches):
       Stage0  Stage1  Stage2  Stage3
æ—¶åˆ»1: [F0]
æ—¶åˆ»2: [F1]    [F0]
æ—¶åˆ»3: [F2]    [F1]    [F0]
æ—¶åˆ»4: [F3]    [F2]    [F1]    [F0]
æ—¶åˆ»5: [B0]    [F3]    [F2]    [F1]    â† Stage0 å¼€å§‹ Backward
æ—¶åˆ»6: [B1]    [B0]    [F3]    [F2]
æ—¶åˆ»7: [B2]    [B1]    [B0]    [F3]
æ—¶åˆ»8: [B3]    [B2]    [B1]    [B0]    â† ç¨³æ€: 1F1B
æ—¶åˆ»9:         [B3]    [B2]    [B1]
æ—¶åˆ»10:                [B3]    [B2]
æ—¶åˆ»11:                        [B3]
```

**1F1B çš„å«ä¹‰**ï¼š**1 Forward 1 Backward**
- ç¨³æ€é˜¶æ®µï¼šæ¯ä¸ª stage äº¤æ›¿æ‰§è¡Œ 1 æ¬¡ Forward å’Œ 1 æ¬¡ Backward
- å†…å­˜æ¶ˆè€— = O(PP)ï¼Œè€Œä¸æ˜¯ O(n_microbatches)

**å¯¹æ¯” GPipe**ï¼š
| ç‰¹æ€§ | GPipe | 1F1B |
|------|-------|------|
| **å†…å­˜** | O(n_microbatches) | O(PP) |
| **Bubble** | ç›¸åŒ | ç›¸åŒ |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ |

### 3.3 Interleaved 1F1B Schedule

**æ¯ä¸ª GPU æŒæœ‰å¤šä¸ª stageï¼ˆè™šæ‹Ÿ stageï¼‰ï¼Œè¿›ä¸€æ­¥å‡å°‘ Bubble**

```
é…ç½®: PP = 2, æ¯ä¸ª rank æŒæœ‰ 2 ä¸ª stage
      Rank 0: Stage 0, Stage 2
      Rank 1: Stage 1, Stage 3

æ¨¡å‹æµç¨‹: Stage0 â†’ Stage1 â†’ Stage2 â†’ Stage3

æ—¶é—´çº¿ (2 ranks, 4 virtual stages, 4 microbatches):
       Rank0(S0,S2)  Rank1(S1,S3)
æ—¶åˆ»1: [F0_S0]
æ—¶åˆ»2: [F1_S0]       [F0_S1]
æ—¶åˆ»3: [F0_S2]       [F1_S1]       â† Rank0 æ‰§è¡Œ Stage2
æ—¶åˆ»4: [F2_S0]       [F0_S3]
æ—¶åˆ»5: [F1_S2]       [F2_S1]
æ—¶åˆ»6: [F3_S0]       [F1_S3]
æ—¶åˆ»7: [F2_S2]       [F3_S1]
æ—¶åˆ»8: [B0_S2]       [F2_S3]       â† Backward å¼€å§‹
...
```

**ä¸ºä»€ä¹ˆ Bubble æ›´å°‘ï¼Ÿ**

```
æ™®é€š 1F1B (PP = 4, 4 ranks):
Bubble = 3 ä¸ª stage çš„ warm-up + 3 ä¸ª stage çš„ cool-down

Interleaved 1F1B (PP = 2, 2 ranks, 2 stages/rank):
Bubble = 1 ä¸ª rank çš„ warm-up + 1 ä¸ª rank çš„ cool-down
       = åªæœ‰ 1 ä¸ªå•ä½çš„ bubble (è€Œä¸æ˜¯ 3)
```

**å†…å­˜ trade-off**ï¼š
- æ¯ä¸ª rank æŒæœ‰ 2 ä¸ª stage â†’ éœ€è¦å­˜å‚¨ 2 ç»„æ¿€æ´»
- å†…å­˜ = O(PP Ã— stages_per_rank)
- ä½† Bubble å¤§å¹…å‡å°‘

### 3.4 ZeroBubble Schedule

**æ›´æ¿€è¿›çš„è°ƒåº¦ï¼Œç†è®ºä¸Š 0 Bubble**

```
æ ¸å¿ƒæ€æƒ³:
1. æ‹†åˆ† Backward ä¸º B å’Œ W
   - B: Backward è®¡ç®—æ¢¯åº¦
   - W: Weight update (æ¢¯åº¦ä¹˜ä»¥å­¦ä¹ ç‡)

2. é‡æ’ B å’Œ W çš„é¡ºåºï¼Œå¡«æ»¡ Bubble

ä¼ ç»Ÿ 1F1B:
[F F F F B B B B] â†’ æœ‰ Bubble

ZeroBubble:
[F F B F B F B F B W W W W] â†’ æ—  Bubble
```

**å®ç°å¤æ‚åº¦é«˜**ï¼Œéœ€è¦ç²¾ç»†çš„è°ƒåº¦ã€‚TorchTitan æ”¯æŒ `ZBVZeroBubble` å’Œ `InterleavedZeroBubble`ã€‚

### 3.5 Schedule å¯¹æ¯”æ€»ç»“

| Schedule | Bubble æ¯”ä¾‹ | å†…å­˜ | å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|----------|-------------|------|--------|---------|
| **GPipe** | é«˜ | O(n_mb) | ä½ | æ•™å­¦/ç®€å•åœºæ™¯ |
| **1F1B** | ä¸­ | O(PP) | ä¸­ | é»˜è®¤é€‰æ‹© |
| **Interleaved 1F1B** | ä½ | O(PP Ã— stages) | ä¸­é«˜ | å¤§æ¨¡å‹ |
| **ZeroBubble** | ~0 | ä¸­ | é«˜ | æè‡´æ€§èƒ½ |

---

## 4. æºç å®ç°è¯¦è§£

### 4.1 æ ¸å¿ƒå…¥å£ï¼špipeline_llm

```python
# æ¥è‡ª: torchtitan/distributed/pipeline_parallel.py:41-153

def pipeline_llm(
    model: nn.Module,
    parallel_dims: ParallelDims,
    job_config: JobConfig,
    device: torch.device,
    model_args: BaseModelArgs,
    parallelize_fn: ParallelizeFunction,
    loss_fn: LossFunction,
) -> tuple[_PipelineSchedule, list[nn.Module], bool, bool]:
    """
    å°†æ¨¡å‹åˆ‡åˆ†æˆ pipeline stagesï¼Œå¹¶æ„å»º scheduleã€‚

    è¿”å›:
        - pp_schedule: Pipeline schedule
        - model_parts: æ¯ä¸ª stage çš„æ¨¡å‹éƒ¨åˆ†
        - has_first_stage: å½“å‰ rank æ˜¯å¦æœ‰ç¬¬ä¸€ä¸ª stage
        - has_last_stage: å½“å‰ rank æ˜¯å¦æœ‰æœ€åä¸€ä¸ª stage
    """
    pp_mesh = parallel_dims.world_mesh["pp"]

    # 1. ç¡®å®š schedule ç±»å‹
    schedule_class = get_schedule_class(
        job_config.parallelism.pipeline_parallel_schedule
    )
    is_single_stage_schedule = issubclass(schedule_class, PipelineScheduleSingle)

    # 2. è®¡ç®—è™šæ‹Ÿ stage æ•°é‡
    num_layers = model_args.n_layers
    layers_per_stage = job_config.parallelism.pipeline_parallel_layers_per_stage

    if layers_per_stage is not None:
        # æ ¹æ®æ¯ä¸ª stage çš„å±‚æ•°è®¡ç®—æ€» stage æ•°
        num_virtual_stages = math.ceil(num_layers / layers_per_stage)
    else:
        # é»˜è®¤ï¼šå• stage schedule æ¯ä¸ª rank 1 ä¸ª stage
        #       å¤š stage schedule æ¯ä¸ª rank 2 ä¸ª stage
        stages_per_rank = 1 if is_single_stage_schedule else 2
        num_virtual_stages = parallel_dims.pp * stages_per_rank

    # 3. ç”Ÿæˆæ¯ä¸ª stage çš„æ¨¡å—å
    module_names_per_stage = generate_llm_fqn_per_model_part(
        num_virtual_stages, num_layers, input_weight, output_weight
    )

    # 4. åˆ‡åˆ†æ¨¡å‹
    stages, model_parts = pipeline_module_split(
        model,
        pp_mesh,
        job_config.parallelism.pipeline_parallel_schedule,
        device,
        module_names_per_stage,
    )

    # 5. å¯¹æ¯ä¸ª stage åº”ç”¨å…¶ä»–å¹¶è¡ŒåŒ– (FSDP, TP, etc.)
    for i, m in enumerate(model_parts):
        m = parallelize_fn(m, parallel_dims, job_config)
        model_parts[i] = m
        stages[i].submod = m

    # 6. æ„å»º schedule
    pp_schedule = build_pipeline_schedule(job_config, stages, loss_fn)

    # 7. è¿”å›
    has_first_stage = any(stage.is_first for stage in stages)
    has_last_stage = any(stage.is_last for stage in stages)

    return pp_schedule, model_parts, has_first_stage, has_last_stage
```

### 4.2 æ¨¡å‹åˆ‡åˆ†ï¼šgenerate_llm_fqn_per_model_part

```python
# æ¥è‡ª: torchtitan/distributed/pipeline_parallel.py:226-334

def generate_llm_fqn_per_model_part(
    num_stages: int,
    num_layers: int,
    input_weight: int = 1,
    output_weight: int = 1,
) -> list[list[str]]:
    """
    ä¸ºæ¯ä¸ª stage ç”Ÿæˆæ¨¡å—ååˆ—è¡¨ã€‚

    Args:
        num_stages: Pipeline stage æ•°é‡
        num_layers: Transformer å±‚æ•°
        input_weight: Embedding çš„æƒé‡ï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
        output_weight: Output å±‚çš„æƒé‡

    Returns:
        æ¯ä¸ª stage çš„æ¨¡å—ååˆ—è¡¨
    """
    # ä¾‹å¦‚: num_stages=4, num_layers=32

    # ç¬¬ä¸€ä¸ª stage: ["tok_embeddings", "layers.0", ..., "layers.7"]
    # ç¬¬äºŒä¸ª stage: ["layers.8", ..., "layers.15"]
    # ç¬¬ä¸‰ä¸ª stage: ["layers.16", ..., "layers.23"]
    # æœ€åä¸€ä¸ª stage: ["layers.24", ..., "layers.31", "norm", "output"]

    module_names_per_stage = []

    # è®¡ç®—æœ‰æ•ˆå±‚æ•°ï¼ˆåŒ…æ‹¬ embedding å’Œ output çš„æƒé‡ï¼‰
    num_effective_layers = num_layers + input_weight + output_weight

    # å‡åŒ€åˆ†é…
    layers_per_stage = num_effective_layers // num_stages
    extra_layers = num_effective_layers % num_stages

    current_layer = 0

    for stage_idx in range(num_stages):
        stage_modules = []

        # è®¡ç®—è¿™ä¸ª stage çš„å±‚æ•°
        effective_layers_for_stage = layers_per_stage
        if stage_idx < extra_layers:
            effective_layers_for_stage += 1

        if stage_idx == 0:
            # ç¬¬ä¸€ä¸ª stage: åŒ…å« embedding
            stage_modules.append("tok_embeddings")
            remaining = effective_layers_for_stage - input_weight
            for _ in range(remaining):
                stage_modules.append(f"layers.{current_layer}")
                current_layer += 1

        elif stage_idx == num_stages - 1:
            # æœ€åä¸€ä¸ª stage: åŒ…å« output
            remaining = effective_layers_for_stage - output_weight
            for _ in range(remaining):
                stage_modules.append(f"layers.{current_layer}")
                current_layer += 1
            stage_modules.extend(["norm", "output"])

        else:
            # ä¸­é—´ stage: åªæœ‰ transformer å±‚
            for _ in range(effective_layers_for_stage):
                stage_modules.append(f"layers.{current_layer}")
                current_layer += 1

        module_names_per_stage.append(stage_modules)

    return module_names_per_stage
```

**ç¤ºä¾‹**ï¼š
```python
# Llama3 8B, 32 layers, 4 stages
generate_llm_fqn_per_model_part(4, 32)

# è¿”å›:
[
    ["tok_embeddings", "layers.0", ..., "layers.7"],   # Stage 0: 8 layers
    ["layers.8", ..., "layers.15"],                    # Stage 1: 8 layers
    ["layers.16", ..., "layers.23"],                   # Stage 2: 8 layers
    ["layers.24", ..., "layers.31", "norm", "output"], # Stage 3: 8 layers
]
```

### 4.3 å®é™…æ¨¡å‹åˆ‡åˆ†ï¼špipeline_module_split

```python
# æ¥è‡ª: torchtitan/distributed/pipeline_parallel.py:337-475

def pipeline_module_split(
    whole_model: nn.Module,
    pp_mesh: DeviceMesh,
    pp_schedule: str,
    device: torch.device,
    module_names_per_stage: list[list[str]],
) -> tuple[list[PipelineStage], list[nn.Module]]:
    """
    æ ¹æ®æ¨¡å—ååˆ‡åˆ†æ¨¡å‹ï¼Œåˆ›å»º PipelineStageã€‚
    """
    pp_rank = pp_mesh.get_local_rank()
    pp_degree = pp_mesh.size()

    def _build_stage_from_modules(stage_idx, module_names, num_stages):
        # æ·±æ‹·è´æ•´ä¸ªæ¨¡å‹
        model = copy.deepcopy(whole_model)

        # åªä¿ç•™è¿™ä¸ª stage éœ€è¦çš„æ¨¡å—
        modules_to_keep = set(module_names)
        for module_name, module_value in model.named_children():
            if isinstance(module_value, (nn.ModuleDict, nn.ModuleList)):
                # å¤„ç† layers
                layers_to_keep = {...}
                # åˆ é™¤ä¸éœ€è¦çš„å±‚
            elif module_name not in modules_to_keep:
                # è®¾ç½®ä¸º None
                setattr(model, module_name, None)

        # åˆ›å»º PipelineStage
        stage = PipelineStage(
            model,
            stage_idx,
            num_stages,
            device,
            group=pp_mesh.get_group("pp"),
        )
        return stage, model

    # è®¡ç®—å½“å‰ rank è´Ÿè´£å“ªäº› stage
    def _get_stage_indices():
        stages_per_rank = num_stages // pp_degree

        if style == "loop":  # Interleaved schedule
            # Rank 0: Stage 0, 4, 8, ...
            # Rank 1: Stage 1, 5, 9, ...
            return tuple(pp_rank + s * pp_degree for s in range(stages_per_rank))
        elif style == "v":   # ZeroBubble V-shaped
            # Rank 0: Stage 0, Stage (N-1)
            # Rank 1: Stage 1, Stage (N-2)
            return stage_v_pairs[pp_rank]

    # æ„å»º stages
    stages = []
    models = []
    for stage_idx in _get_stage_indices():
        stage, model_chunk = _build_stage_from_modules(...)
        stages.append(stage)
        models.append(model_chunk)

    return stages, models
```

**å…³é”®ç‚¹**ï¼š
- **æ·±æ‹·è´æ¨¡å‹**ï¼šæ¯ä¸ª stage ä»å®Œæ•´æ¨¡å‹æ·±æ‹·è´ï¼Œç„¶ååˆ é™¤ä¸éœ€è¦çš„éƒ¨åˆ†
- **Stage åˆ†é…**ï¼šæ ¹æ® schedule ç±»å‹ï¼ˆloop æˆ– vï¼‰ç¡®å®šæ¯ä¸ª rank è´Ÿè´£å“ªäº› stage
- **PipelineStage**ï¼šPyTorch çš„ `torch.distributed.pipelining.PipelineStage` å°è£…

### 4.4 æ„å»º Scheduleï¼šbuild_pipeline_schedule

```python
# æ¥è‡ª: torchtitan/distributed/pipeline_parallel.py:156-223

def build_pipeline_schedule(
    job_config: JobConfig, stages: list[PipelineStage], loss_fn: Callable
) -> _PipelineSchedule:
    """
    æ ¹æ®é…ç½®æ„å»º pipeline scheduleã€‚
    """
    # è·å– schedule ç±»
    schedule_class = get_schedule_class(
        job_config.parallelism.pipeline_parallel_schedule
    )

    # è®¡ç®— microbatch æ•°é‡
    microbatch_size = job_config.parallelism.pipeline_parallel_microbatch_size
    batch_size = job_config.training.local_batch_size
    n_microbatches = batch_size // microbatch_size

    # éªŒè¯
    if n_microbatches < num_total_stages:
        logger.warning(
            f"Number of microbatches ({n_microbatches}) is less than stages "
            f"({num_total_stages}) which may result in a bubble."
        )

    # åˆ›å»º schedule
    looped_schedule = issubclass(schedule_class, PipelineScheduleMulti)
    schedule = schedule_class(
        stages if looped_schedule else stages[0],
        n_microbatches=n_microbatches,
        loss_fn=rescale_accumulated_loss(loss_fn, n_microbatches),
        scale_grads=False,
    )

    return schedule
```

**å…³é”®å‚æ•°**ï¼š
- **n_microbatches**ï¼š`batch_size / microbatch_size`
  - è¶Šå¤§ï¼ŒBubble è¶Šå°
  - ä½†å†…å­˜è¶Šå¤§

- **rescale_accumulated_loss**ï¼š
  - Loss ä¼šç´¯åŠ  n_microbatches æ¬¡
  - éœ€è¦é™¤ä»¥ n_microbatches å¾—åˆ°å¹³å‡

### 4.5 è®­ç»ƒå¾ªç¯ä¸­çš„ä½¿ç”¨

```python
# æ¥è‡ª: torchtitan/train.py:496-527

if parallel_dims.pp_enabled:
    # Pipeline Parallel forward / backward
    with self.train_context(optional_context_parallel_ctx):
        targets, losses = (
            (labels, []) if self.pp_has_last_stage else (None, None)
        )

        if self.pp_has_first_stage:
            # ç¬¬ä¸€ä¸ª stage: éœ€è¦ä¼ å…¥ input
            self.pp_schedule.step(
                inputs,
                **extra_inputs,
                **extra_kwargs,
                target=targets,
                losses=losses,
                return_outputs=False,
            )
        else:
            # ä¸­é—´ / æœ€å stage: ä¸éœ€è¦ä¼ å…¥ input
            self.pp_schedule.step(
                **extra_kwargs,
                target=targets,
                losses=losses,
                return_outputs=False,
            )

    # æ±‡æ€» loss
    loss = (
        torch.sum(torch.stack(losses)).to(self.device)
        if self.pp_has_last_stage
        else torch.tensor([-1.0], device=self.device)
    )
```

**å…³é”®ç‚¹**ï¼š
- **pp_schedule.step()**ï¼šæ‰§è¡Œå®Œæ•´çš„ forward + backward
- **åªæœ‰ first_stage ä¼ å…¥ input**ï¼šå…¶ä»– stage ä»ä¸Šä¸€ä¸ª stage æ¥æ”¶
- **åªæœ‰ last_stage è®¡ç®— loss**ï¼šloss åœ¨æœ€åä¸€ä¸ª stage äº§ç”Ÿ
- **losses åˆ—è¡¨**ï¼šæ”¶é›†æ‰€æœ‰ microbatch çš„ loss

---

## 5. æ€§èƒ½åˆ†æ

### 5.1 å®˜æ–¹ Benchmark ç»“æœ

æ¥è‡ª `benchmarks/llama3_h100_202412_torchtitan.md`

**Table 5: Llama 3.1 405B, 512 H100s (FSDP 8, TP 8, PP 8)**

| Schedule | TPS/GPU | Memory (GiB) |
|----------|---------|--------------|
| **1F1B** | 100 | 82.5 |
| **Interleaved 1F1B** | 128 | 72.7 |

**åˆ†æ**ï¼š
- **Interleaved 1F1B å¿« 28%**
- **Interleaved 1F1B å†…å­˜æ›´å°‘**ï¼šå› ä¸º Bubble å°ï¼Œä¸éœ€è¦ä¿å­˜é‚£ä¹ˆå¤šæ¿€æ´»

### 5.2 Bubble æ¯”ä¾‹è®¡ç®—

**å…¬å¼**ï¼š
```
Bubble æ¯”ä¾‹ â‰ˆ (PP - 1) / n_microbatches

1F1B with PP=8, n_microbatches=32:
  Bubble = 7 / 32 = 21.9%

Interleaved 1F1B with PP=8, n_microbatches=32, stages_per_rank=2:
  æœ‰æ•ˆ PP = 8 / 2 = 4
  Bubble = 3 / 32 = 9.4%
```

**Interleaved çš„ä¼˜åŠ¿**ï¼šBubble å‡å°‘äº† 12.5%

### 5.3 é€šä¿¡å¼€é”€

**PP çš„é€šä¿¡ç‰¹ç‚¹**ï¼š
- **ç‚¹å¯¹ç‚¹é€šä¿¡**ï¼šStage ä¹‹é—´ä¼ é€’æ¿€æ´»å€¼
- **é€šä¿¡é‡å°**ï¼šåªä¼  activationsï¼Œä¸ä¼  weights

```python
# æ¯ä¸ª stage ä¹‹é—´çš„é€šä¿¡é‡
activation_size = batch_size * seq_len * hidden_dim * sizeof(dtype)

# Llama3 405B
batch_size = 2, seq_len = 8192, hidden_dim = 16384, dtype = fp16
activation_size = 2 * 8192 * 16384 * 2 = 512 MB

# å¯¹æ¯” FSDP (ä¼ é€’æƒé‡)
weight_size = 405B * 2 = 810 GB

PP é€šä¿¡é‡ << FSDP é€šä¿¡é‡
```

### 5.4 å†…å­˜åˆ†æ

**æ¯ä¸ª stage çš„å†…å­˜**ï¼š
```python
# Llama3 405B, PP = 8

# å‚æ•°å†…å­˜
params_per_stage = 405B / 8 = 50.6B params
params_memory = 50.6B * 2 bytes = 101 GB

# æ¿€æ´»å†…å­˜ (1F1B)
# éœ€è¦ä¿å­˜ PP ä¸ª microbatch çš„æ¿€æ´»
activations_memory = PP * activation_size = 8 * 512 MB = 4 GB

# æ¢¯åº¦å†…å­˜
gradients_memory = params_memory = 101 GB

# ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW)
optimizer_memory = params_memory * 2 = 202 GB

# æ€»å†…å­˜
total = 101 + 4 + 101 + 202 = 408 GB
```

**ä½†å®é™…åªæœ‰ 82.5 GBï¼Ÿ**

å› ä¸ºé…åˆäº†ï¼š
- **FSDP**ï¼šå‚æ•°åˆ†æ•£åˆ° 8 ä¸ª GPU
- **Float8**ï¼šå‡å°‘æ¿€æ´»å’Œæ¢¯åº¦å†…å­˜
- **Activation Checkpointing**ï¼šå‡å°‘æ¿€æ´»å†…å­˜

### 5.5 ä¸å…¶ä»–å¹¶è¡Œçš„ç»„åˆæ•ˆæœ

**3D Parallelism: FSDP + TP + PP**

```
Llama3 405B on 512 H100s

é…ç½®: FSDP 8, TP 8, PP 8
      512 = 8 Ã— 8 Ã— 8 GPUs

æ¯ä¸ª GPU çš„å†…å­˜:
- å‚æ•°: 405B / 8 (FSDP) / 8 (TP) / 8 (PP) = 0.79B params = 1.6 GB
- æ¿€æ´»: è¢« TP åˆ‡åˆ†ï¼Œå†è¢« CP åˆ‡åˆ†
- æ€»è®¡: 72-82 GB

åå: 100-128 TPS/GPU
```

---

## 6. ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ

### 6.1 ä½•æ—¶åº”è¯¥ä½¿ç”¨ Pipeline Parallelï¼Ÿ

**æ¨èä½¿ç”¨çš„åœºæ™¯**ï¼š

âœ… **è¶…å¤§æ¨¡å‹ (> 70B)**
   - å±‚æ•°å¤ªå¤šï¼Œå• GPU æ”¾ä¸ä¸‹æ‰€æœ‰å±‚
   - éœ€è¦è·¨èŠ‚ç‚¹åˆ†å¸ƒæ¨¡å‹

âœ… **ä¸ FSDP + TP ç»„åˆ**
   - 3D æˆ– 4D å¹¶è¡Œ
   - å¤„ç†è¶…å¤§æ¨¡å‹ (405B+)

âœ… **èŠ‚ç‚¹é—´é€šä¿¡å—é™**
   - PP çš„é€šä¿¡é‡æ¯” FSDP å°
   - é€‚åˆ InfiniBand å¸¦å®½æœ‰é™çš„åœºæ™¯

**ä¸æ¨èä½¿ç”¨çš„åœºæ™¯**ï¼š

âŒ **å°æ¨¡å‹ (< 13B)**
   - FSDP è¶³å¤Ÿï¼Œä¸éœ€è¦ PP
   - PP ä¼šå¼•å…¥ Bubble å¼€é”€

âŒ **Batch size å¤ªå°**
   - æ— æ³•åˆ›å»ºè¶³å¤Ÿçš„ microbatch
   - Bubble æ¯”ä¾‹å¤ªé«˜

âŒ **è°ƒè¯•é˜¶æ®µ**
   - PP å¢åŠ è°ƒè¯•éš¾åº¦
   - å…ˆç”¨ FSDP è°ƒé€šï¼Œå†åŠ  PP

### 6.2 é…ç½®æ–¹æ³•

**åŸºæœ¬é…ç½®**ï¼š

```toml
[parallelism]
pipeline_parallel_degree = 4  # PP å¹¶è¡Œåº¦

# Schedule é€‰æ‹©
pipeline_parallel_schedule = "1F1B"  # æˆ– "Interleaved1F1B"

# Microbatch é…ç½®
pipeline_parallel_microbatch_size = 1

[training]
local_batch_size = 8  # å¿…é¡»èƒ½è¢« microbatch_size æ•´é™¤
```

**n_microbatches è®¡ç®—**ï¼š
```python
n_microbatches = local_batch_size / pipeline_parallel_microbatch_size
              = 8 / 1 = 8 microbatches
```

### 6.3 Schedule é€‰æ‹©æŒ‡å—

**1F1B**ï¼ˆé»˜è®¤ï¼‰ï¼š
```toml
pipeline_parallel_schedule = "1F1B"
```
- ç®€å•ç¨³å®š
- é€‚åˆå¤§å¤šæ•°åœºæ™¯
- æ¯ä¸ª rank 1 ä¸ª stage

**Interleaved 1F1B**ï¼ˆæ¨èå¤§æ¨¡å‹ï¼‰ï¼š
```toml
pipeline_parallel_schedule = "Interleaved1F1B"
```
- æ›´å°‘ Bubble
- æ¯ä¸ª rank å¤šä¸ª stage
- éœ€è¦æ›´å¤š microbatch

**ZeroBubble**ï¼ˆæè‡´æ€§èƒ½ï¼‰ï¼š
```toml
pipeline_parallel_schedule = "ZBVZeroBubble"
# æˆ–
pipeline_parallel_schedule = "InterleavedZeroBubble"
```
- ç†è®º 0 Bubble
- å®ç°å¤æ‚
- éœ€è¦ä»”ç»†è°ƒå‚

### 6.4 Microbatch æ•°é‡è°ƒä¼˜

**ç»éªŒæ³•åˆ™**ï¼š

```python
# æœ€å° microbatch æ•°
min_microbatches = PP * 2  # è‡³å°‘ 2 å€ PP æ•°é‡

# æ¨è microbatch æ•°
recommended = PP * 4 ~ PP * 8

# ç¤ºä¾‹
PP = 8:
  min = 16, recommended = 32 ~ 64
```

**trade-off**ï¼š
- **microbatch å¤ªå°‘**ï¼šBubble å¤§ï¼Œæ•ˆç‡ä½
- **microbatch å¤ªå¤š**ï¼šå†…å­˜å¤§ï¼Œå° batch é€šä¿¡å¼€é”€å æ¯”é«˜

### 6.5 å±‚æ•°åˆ†é…è°ƒä¼˜

**è‡ªåŠ¨åˆ†é…** (é»˜è®¤)ï¼š

```toml
[parallelism]
# ä¸æŒ‡å®š layers_per_stageï¼Œè‡ªåŠ¨å‡åŒ€åˆ†é…
```

**æ‰‹åŠ¨åˆ†é…**ï¼š

```toml
[parallelism]
pipeline_parallel_layers_per_stage = 4
# Llama3 8B (32 layers), PP = 4:
# æ¯ä¸ª stage 8 å±‚ï¼Œæ€»å…± 4 stages

# æˆ–æŒ‡å®šå…·ä½“æ¨¡å—
module_fqns_per_model_part = [
    ["tok_embeddings", "layers.0", ..., "layers.9"],   # Stage 0: 10 layers
    ["layers.10", ..., "layers.19"],                    # Stage 1: 10 layers
    ["layers.20", ..., "layers.29"],                    # Stage 2: 10 layers
    ["layers.30", "layers.31", "norm", "output"],       # Stage 3: 2 layers + output
]
```

**è´Ÿè½½å‡è¡¡**ï¼š

```toml
[parallelism]
# ç¬¬ä¸€ä¸ª stage å°‘æ”¾å±‚ï¼ˆå› ä¸ºæœ‰ embeddingï¼‰
pipeline_parallel_first_stage_less_layers = 1

# æœ€åä¸€ä¸ª stage å°‘æ”¾å±‚ï¼ˆå› ä¸ºæœ‰ outputï¼‰
pipeline_parallel_last_stage_less_layers = 1
```

### 6.6 ä¸å…¶ä»–å¹¶è¡Œçš„ç»„åˆ

**æ¨èç»„åˆ**ï¼š

| æ¨¡å‹å¤§å° | GPU æ•° | é…ç½® | è¯´æ˜ |
|---------|--------|------|------|
| 8B | 8 | FSDP 8 | åªç”¨ FSDP |
| 70B | 64 | FSDP 8, TP 8 | 2D |
| 405B | 256 | FSDP 4, TP 8, PP 8 | 3D |
| 405B + é•¿åºåˆ— | 512 | FSDP 8, TP 8, PP 8, CP 1-8 | 4D |

**é…ç½®ç¤ºä¾‹ (Llama3 405B on 512 H100s)**ï¼š

```toml
[model]
name = "llama3"
flavor = "405B"
converters = ["float8"]

[training]
local_batch_size = 8
seq_len = 8192

[parallelism]
# 512 = 8 Ã— 8 Ã— 8
data_parallel_shard_degree = 8   # FSDP
tensor_parallel_degree = 8       # TP
pipeline_parallel_degree = 8     # PP
enable_async_tensor_parallel = true

# PP é…ç½®
pipeline_parallel_schedule = "Interleaved1F1B"
pipeline_parallel_microbatch_size = 1

[activation_checkpoint]
mode = "full"

[compile]
enable = true
components = ["model", "loss"]
```

### 6.7 è°ƒè¯•æŠ€å·§

**1. éªŒè¯åˆ‡åˆ†æ˜¯å¦æ­£ç¡®**ï¼š

```python
# æŸ¥çœ‹æ—¥å¿—
# PP rank 0 is building stage_idx 0 with modules [tok_embeddings, layers.0, ...]
# PP rank 1 is building stage_idx 1 with modules [layers.8, ...]
```

**2. æ£€æŸ¥ Bubble**ï¼š

```python
# æŸ¥çœ‹ warning
# "Number of microbatches (4) is less than stages (8) which may result in a bubble."
```

**3. Profiling**ï¼š

```bash
[profiling]
enable_profiling = true
```

ç”¨ `chrome://tracing` æŸ¥çœ‹ï¼š
- å„ä¸ª stage çš„ forward/backward æ—¶é—´
- é€šä¿¡æ—¶é—´
- Bubble æ—¶é—´

**å¸¸è§é—®é¢˜**ï¼š

â“ **Loss ä¸å¯¹ï¼Ÿ**
- æ£€æŸ¥ last_stage æ˜¯å¦æ­£ç¡®è®¡ç®— loss
- æ£€æŸ¥ loss æ˜¯å¦æ­£ç¡® rescale

â“ **OOMï¼Ÿ**
- å‡å°‘ microbatch æ•°é‡
- å¢åŠ  PP å¹¶è¡Œåº¦
- å¯ç”¨ activation checkpointing

â“ **é€Ÿåº¦å¾ˆæ…¢ï¼Ÿ**
- æ£€æŸ¥ microbatch æ•°é‡æ˜¯å¦å¤ªå°‘
- è€ƒè™‘ç”¨ Interleaved schedule
- æ£€æŸ¥é€šä¿¡æ˜¯å¦æˆä¸ºç“¶é¢ˆ

---

## 7. æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹

ç”¨**å·¥å‚æµæ°´çº¿**æ€»ç»“ Pipeline Parallelï¼š

```
ä¼ ç»Ÿæ–¹å¼ = ä¸€ä¸ªå·¥äººå®Œæˆæ‰€æœ‰å·¥åº
    å·¥åºå¤ªå¤šï¼Œè®°ä¸ä½ï¼Œæ•ˆç‡ä½

Pipeline Parallel = æµæ°´çº¿ä½œä¸š
    æ¯äººè´Ÿè´£ä¸€é“å·¥åº
    å·¥ä»¶åœ¨æµæ°´çº¿ä¸Šä¾æ¬¡ä¼ é€’
    å¹¶è¡Œå¤„ç†å¤šä¸ªå·¥ä»¶
```

**ä¸‰å¤§æ ¸å¿ƒæ¦‚å¿µ**ï¼š

1. **Stage**ï¼šæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆè‹¥å¹²å±‚ï¼‰
2. **Microbatch**ï¼šæ•°æ®çš„ä¸€éƒ¨åˆ†
3. **Schedule**ï¼šforward/backward çš„æ‰§è¡Œé¡ºåº

### 7.2 æ€§èƒ½ç‰¹ç‚¹

**ä¼˜ç‚¹**ï¼š
- âœ… **é€šä¿¡é‡å°**ï¼šåªä¼  activations
- âœ… **å®ç°ç®€å•**ï¼šæŒ‰å±‚åˆ‡åˆ†
- âœ… **ä¸å…¶ä»–å¹¶è¡Œç»„åˆå¥½**ï¼š3D/4D å¹¶è¡Œ

**ç¼ºç‚¹**ï¼š
- âŒ **æœ‰ Bubble**ï¼šGPU ç©ºé—²æ—¶é—´
- âŒ **éœ€è¦è¶³å¤Ÿçš„ microbatch**
- âŒ **è°ƒè¯•å¤æ‚**

### 7.3 Schedule é€‰æ‹©

| Schedule | Bubble | å†…å­˜ | æ¨èåœºæ™¯ |
|----------|--------|------|---------|
| **1F1B** | ä¸­ | ä½ | é»˜è®¤é€‰æ‹© |
| **Interleaved 1F1B** | ä½ | ä¸­ | å¤§æ¨¡å‹ |
| **ZeroBubble** | ~0 | ä¸­ | æè‡´æ€§èƒ½ |

**å®æµ‹æ€§èƒ½ (405B)**ï¼š
- 1F1B: 100 TPS/GPU
- Interleaved 1F1B: 128 TPS/GPU (**+28%**)

### 7.4 ä½¿ç”¨å»ºè®®

**æ¨èä½¿ç”¨**ï¼š
- âœ… è¶…å¤§æ¨¡å‹ (> 70B)
- âœ… ä¸ FSDP + TP ç»„åˆ
- âœ… èŠ‚ç‚¹é—´é€šä¿¡å—é™

**ä¸æ¨èä½¿ç”¨**ï¼š
- âŒ å°æ¨¡å‹ (< 13B)
- âŒ Batch size å¤ªå°
- âŒ è°ƒè¯•é˜¶æ®µ

**é…ç½®è¦ç‚¹**ï¼š
```toml
[parallelism]
pipeline_parallel_degree = 8
pipeline_parallel_schedule = "Interleaved1F1B"
pipeline_parallel_microbatch_size = 1

[training]
local_batch_size = 32  # = 32 microbatchesï¼ŒBubble â‰ˆ 22%
```

### 7.5 ä¸å…¶ä»–å¹¶è¡Œçš„å¯¹æ¯”

| ç‰¹æ€§ | FSDP | TP | CP | **PP** |
|------|------|----|----|--------|
| **åˆ‡åˆ†å¯¹è±¡** | å‚æ•° | å•å±‚æƒé‡ | åºåˆ— | **å±‚** |
| **é€šä¿¡é‡** | å¤§ | ä¸­ | å° | **å°** |
| **å†…å­˜èŠ‚çœ** | å‚æ•° | å‚æ•°+æ¿€æ´» | æ¿€æ´» | **å‚æ•°** |
| **é¢å¤–å¼€é”€** | æ—  | å°‘ | å°‘ | **Bubble** |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨ | å•å±‚å¤§ | åºåˆ—é•¿ | **å±‚æ•°å¤š** |

---

## 8. å‚è€ƒèµ„æ–™

**æºç æ–‡ä»¶**ï¼š
- `torchtitan/distributed/pipeline_parallel.py` - PP æ ¸å¿ƒå®ç°
- `torchtitan/train.py` - è®­ç»ƒå¾ªç¯ä¸­çš„ä½¿ç”¨
- `torchtitan/config/job_config.py` - é…ç½®é€‰é¡¹

**PyTorch å®˜æ–¹èµ„æº**ï¼š
- [Pipeline Parallelism](https://pytorch.org/docs/stable/distributed.pipelining.html)
- [Schedule å®ç°](https://github.com/pytorch/pytorch/blob/main/torch/distributed/pipelining/schedules.py)

**ç›¸å…³è®ºæ–‡**ï¼š
- GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism
- PipeDream: Generalized Pipeline Parallelism for DNN Training
- Zero Bubble Pipeline Parallelism

**ç›¸å…³æ–‡æ¡£**ï¼š
- `docs/analysis/02_tensor_parallel_implementation.md` - TP è¯¦è§£
- `docs/analysis/03_async_tensor_parallel.md` - Async TP è¯¦è§£
- `docs/analysis/04_context_parallel.md` - CP è¯¦è§£
- `benchmarks/llama3_h100_202412_torchtitan.md` - æ€§èƒ½ Benchmark
- `docs/converging.md` - æ”¶æ•›æ€§éªŒè¯

---

## é™„å½•ï¼šé«˜çº§è¯é¢˜

### A.1 Custom Schedule

TorchTitan æ”¯æŒä» CSV æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰ scheduleï¼š

```toml
[parallelism]
pipeline_parallel_schedule_csv = "/path/to/schedule.csv"
```

CSV æ ¼å¼å®šä¹‰äº†æ¯ä¸ªæ—¶é—´æ­¥æ¯ä¸ª rank æ‰§è¡Œä»€ä¹ˆæ“ä½œã€‚

### A.2 Virtual Stage çš„å†…å­˜ trade-off

**Interleaved 1F1B çš„å†…å­˜è®¡ç®—**ï¼š

```python
# PP = 4, stages_per_rank = 2

# æ¯ä¸ª rank æŒæœ‰ 2 ä¸ª stage
# éœ€è¦ä¿å­˜ 2 ç»„ forward æ¿€æ´»

# 1F1B:
# å†…å­˜ = PP ä¸ªæ¿€æ´» = 4 ä¸ªæ¿€æ´»

# Interleaved 1F1B:
# å†…å­˜ = PP / stages_per_rank * stages_per_rank = PP ä¸ªæ¿€æ´»
# ä½†å®é™…ä¸Šå› ä¸º warm-up é˜¶æ®µï¼Œå¯èƒ½éœ€è¦æ›´å¤š

# å®æµ‹ï¼šInterleaved å†…å­˜åè€Œæ›´å°
# å› ä¸º Bubble å°ï¼Œä¸éœ€è¦é‚£ä¹ˆé•¿çš„ warm-up é˜¶æ®µ
```

### A.3 PP ä¸ Gradient Accumulation çš„äº¤äº’

```python
# é…ç½®
local_batch_size = 8
microbatch_size = 1
gradient_accumulation_steps = 4

# å®é™…æ‰§è¡Œ
# æ¯ä¸ª training step:
#   1. æ‰§è¡Œ 4 æ¬¡ forward-backward (gradient_accumulation_steps)
#   2. æ¯æ¬¡æœ‰ 8 ä¸ª microbatch (n_microbatches)
#   3. æ€»å…±å¤„ç† 4 Ã— 8 = 32 ä¸ªæ ·æœ¬
#   4. ç„¶ååšä¸€æ¬¡ optimizer.step()
```

### A.4 PP çš„é€šä¿¡æ¨¡å¼

```
Stage 0          Stage 1          Stage 2          Stage 3

Forward:
[F0] â”€â”€sendâ”€â”€â†’ [F0] â”€â”€sendâ”€â”€â†’ [F0] â”€â”€sendâ”€â”€â†’ [F0]
     activation     activation     activation

Backward:
[B0] â†â”€â”€sendâ”€â”€ [B0] â†â”€â”€sendâ”€â”€ [B0] â†â”€â”€sendâ”€â”€ [B0]
     gradient      gradient      gradient
```

**é€šä¿¡åŸè¯­**ï¼š
- **P2P Send/Recv**ï¼šç›¸é‚» stage ä¹‹é—´
- **å¼‚æ­¥é€šä¿¡**ï¼šå¯ä»¥ä¸è®¡ç®—é‡å 

### A.5 Model-aware åˆ‡åˆ†

å¯¹äºæŸäº›æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šçš„åˆ‡åˆ†ç­–ç•¥ï¼š

```python
# ä¾‹å¦‚ï¼šMixture of Experts æ¨¡å‹
# Expert å±‚å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦å•ç‹¬æ”¾åœ¨ä¸€ä¸ª stage

module_fqns_per_model_part = [
    ["tok_embeddings", "layers.0", ..., "layers.7"],   # æ™®é€šå±‚
    ["layers.8"],                                      # Expert å±‚ (å•ç‹¬ä¸€ä¸ª stage)
    ["layers.9", ..., "layers.15"],                    # æ™®é€šå±‚
    ["layers.16", ..., "layers.23", "norm", "output"], # æ™®é€šå±‚
]
```

TorchTitan çš„ `module_fqns_per_model_part` é…ç½®æ”¯æŒè¿™ç§çµæ´»çš„åˆ‡åˆ†ã€‚
