# TorchTitan Float8 Training å®ç°è¯¦è§£

## ç›®å½•
1. [ä»€ä¹ˆæ˜¯ Float8 Training](#1-ä»€ä¹ˆæ˜¯-float8-training)
2. [æ¬æ¡Œå­æ¯”å–»ï¼šå‹ç¼©æ¬è¿](#2-æ¬æ¡Œå­æ¯”å–»å‹ç¼©æ¬è¿)
3. [Float8 vs BFloat16/Float32](#3-float8-vs-bfloat16float32)
4. [ä¸¤ç§ Scaling ç­–ç•¥](#4-ä¸¤ç§-scaling-ç­–ç•¥)
5. [Float8 ä¸ FSDP çš„ç»“åˆ](#5-float8-ä¸-fsdp-çš„ç»“åˆ)
6. [Float8 ä¸ TP çš„ç»“åˆ](#6-float8-ä¸-tp-çš„ç»“åˆ)
7. [æºç å®ç°è¯¦è§£](#7-æºç å®ç°è¯¦è§£)
8. [é…ç½®å’Œä½¿ç”¨](#8-é…ç½®å’Œä½¿ç”¨)
9. [æ€§èƒ½æ•°æ®](#9-æ€§èƒ½æ•°æ®)
10. [æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)
11. [æ€»ç»“](#11-æ€»ç»“)
12. [å‚è€ƒèµ„æ–™](#12-å‚è€ƒèµ„æ–™)

---

## 1. ä»€ä¹ˆæ˜¯ Float8 Training

### æ ¸å¿ƒæ€æƒ³

**Float8 Training** æ˜¯ä¸€ç§**ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯**ï¼Œé€šè¿‡ä½¿ç”¨ 8 ä½æµ®ç‚¹æ•°ï¼ˆFloat8ï¼‰ä»£æ›¿ä¼ ç»Ÿçš„ 16 ä½æµ®ç‚¹æ•°ï¼ˆBFloat16ï¼‰æˆ– 32 ä½æµ®ç‚¹æ•°ï¼ˆFloat32ï¼‰ï¼Œåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶ï¼š

1. **åŠ é€Ÿè®¡ç®—**ï¼šåˆ©ç”¨ GPU çš„ FP8 Tensor Coreï¼Œæ¯” BF16 Tensor Core æ›´å¿«
2. **èŠ‚çœå¸¦å®½**ï¼šé€šä¿¡å’Œå†…å­˜è®¿é—®çš„æ•°æ®é‡å‡åŠï¼ˆ8 bit vs 16 bitï¼‰
3. **é™ä½å†…å­˜**ï¼šæ¨¡å‹å‚æ•°å’Œæ¿€æ´»å€¼å ç”¨æ›´å°‘æ˜¾å­˜

### Float8 æ ¼å¼

Float8 æœ‰ä¸¤ç§å¸¸è§æ ¼å¼ï¼ŒPyTorch ä¸»è¦ä½¿ç”¨ **E4M3**ï¼ˆ4 bits æŒ‡æ•°ï¼Œ3 bits å°¾æ•°ï¼‰ï¼š

```
Float8 E4M3: [S][EEEE][MMM]
  - 1 bit ç¬¦å·ä½ (Sign)
  - 4 bits æŒ‡æ•° (Exponent)
  - 3 bits å°¾æ•° (Mantissa)

å¯¹æ¯”ï¼š
  - BFloat16: [S][EEEEEEEE][MMMMMMM]  (8 bits æŒ‡æ•°, 7 bits å°¾æ•°)
  - Float32:  [S][EEEEEEEE][MMMMMMMMMMMMMMMMMMMMMMM]  (8 bits æŒ‡æ•°, 23 bits å°¾æ•°)
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- âœ… **æŒ‡æ•°èŒƒå›´é€‚ä¸­**ï¼š4 bits æŒ‡æ•°å¯ä»¥è¡¨ç¤ºè¾ƒå¤§èŒƒå›´çš„æ•°å€¼
- âš ï¸ **ç²¾åº¦æœ‰é™**ï¼šåªæœ‰ 3 bits å°¾æ•°ï¼Œéœ€è¦é€šè¿‡ **scalesï¼ˆç¼©æ”¾å› å­ï¼‰** æ¥ä¿æŒç²¾åº¦
- ğŸš€ **ç¡¬ä»¶åŠ é€Ÿ**ï¼šH100 GPU çš„ FP8 Tensor Core å³°å€¼æ€§èƒ½æ˜¯ BF16 çš„ 2 å€

---

## 2. æ¬æ¡Œå­æ¯”å–»ï¼šå‹ç¼©æ¬è¿

å»¶ç»­æˆ‘ä»¬çš„"æ¬æ¡Œå­"æ¯”å–»ç³»åˆ—ï¼ŒFloat8 Training å°±åƒ**å‹ç¼©æ¬è¿**ã€‚

### åœºæ™¯ï¼šä»ä»“åº“æ¬æ¡Œå­åˆ°å·¥åœ°

**ä¼ ç»Ÿæ–¹å¼ï¼ˆBFloat16ï¼‰**ï¼š
- æ¯å¼ æ¡Œå­ç”¨**æ ‡å‡†è´§è½¦**è¿è¾“
- æ¯è¾†è´§è½¦èƒ½è£… **2 å¼ æ¡Œå­**
- ç²¾ç¡®è®°å½•æ¯å¼ æ¡Œå­çš„é‡é‡ï¼ˆkgï¼‰

```
è´§è½¦ 1: [æ¡Œå­A: 45.3kg] [æ¡Œå­B: 52.7kg]
è´§è½¦ 2: [æ¡Œå­C: 38.9kg] [æ¡Œå­D: 61.2kg]
```

**Float8 æ–¹å¼ï¼ˆå‹ç¼©æ¬è¿ï¼‰**ï¼š
- æ¯å¼ æ¡Œå­ç”¨**å°å‹è´§è½¦**è¿è¾“ï¼ˆçœæ²¹ã€æ›´å¿«ï¼‰
- æ¯è¾†è´§è½¦ä»èƒ½è£… **2 å¼ æ¡Œå­**ï¼ˆä½“ç§¯å‡åŠï¼‰
- ä½†æ˜¯ï¼é‡é‡è®°å½•ç²¾åº¦é™ä½ï¼Œåªèƒ½è®°å½•åˆ° **æ•´æ•° kg**
- ä¸ºäº†ä¿æŒç²¾åº¦ï¼Œæˆ‘ä»¬è®°å½•ä¸€ä¸ª **ç¼©æ”¾æ¯”ä¾‹ (scale)**

```
å°è´§è½¦ 1: [æ¡Œå­A: 45kg] [æ¡Œå­B: 53kg]  ç¼©æ”¾æ¯”ä¾‹: 1.0x
å°è´§è½¦ 2: [æ¡Œå­C: 39kg] [æ¡Œå­D: 61kg]  ç¼©æ”¾æ¯”ä¾‹: 1.0x

å®é™…é‡é‡ = è®°å½•é‡é‡ Ã— ç¼©æ”¾æ¯”ä¾‹
```

### ä¸ºä»€ä¹ˆéœ€è¦ Scaleï¼ˆç¼©æ”¾æ¯”ä¾‹ï¼‰ï¼Ÿ

å‡è®¾æ¡Œå­é‡é‡èŒƒå›´æ˜¯ 0-100kgï¼Œä½†æˆ‘ä»¬åªèƒ½ç”¨ **8 ä½æ•´æ•°ï¼ˆ0-255ï¼‰** è¡¨ç¤ºï¼š

**ä¸ç”¨ Scale çš„é—®é¢˜**ï¼š
```
æ¡Œå­A: 45.3kg â†’ å­˜å‚¨ä¸º 45 (æŸå¤± 0.3kg)
æ¡Œå­B: 0.5kg  â†’ å­˜å‚¨ä¸º 0  (âŒ å®Œå…¨ä¸¢å¤±ï¼)
```

**ä½¿ç”¨ Scale çš„è§£å†³æ–¹æ¡ˆ**ï¼š
```
æ‰¾åˆ°è¿™æ‰¹æ¡Œå­çš„æœ€å¤§ç»å¯¹å€¼ï¼šmax = 100kg
è®¡ç®— scale = 255 / 100 = 2.55

æ¡Œå­A: 45.3kg Ã— 2.55 = 115.5 â†’ å­˜å‚¨ä¸º 116 â†’ æ¢å¤ä¸º 116 / 2.55 = 45.5kg âœ“
æ¡Œå­B:  0.5kg Ã— 2.55 = 1.3   â†’ å­˜å‚¨ä¸º 1   â†’ æ¢å¤ä¸º 1 / 2.55 = 0.39kg âœ“
```

é€šè¿‡ **åŠ¨æ€è°ƒæ•´ scale**ï¼Œæˆ‘ä»¬å¯ä»¥å……åˆ†åˆ©ç”¨ Float8 çš„è¡¨ç¤ºèŒƒå›´ï¼

### Tensorwise vs Rowwise Scaling

ç»§ç»­æˆ‘ä»¬çš„æ¯”å–»ï¼š

**Tensorwise Scalingï¼ˆæ•´è½¦ä¸€ä¸ªæ¯”ä¾‹ï¼‰**ï¼š
```
è´§è½¦ 1: [æ¡Œå­A: 45kg] [æ¡Œå­B: 53kg]  ç»Ÿä¸€ç¼©æ”¾æ¯”ä¾‹: 1.0x
è´§è½¦ 2: [æ¡Œå­C: 39kg] [æ¡Œå­D: 61kg]  ç»Ÿä¸€ç¼©æ”¾æ¯”ä¾‹: 1.0x

ä¼˜ç‚¹ï¼šç®€å•å¿«é€Ÿï¼Œåªéœ€è®°å½•ä¸€ä¸ªæ¯”ä¾‹
ç¼ºç‚¹ï¼šå¦‚æœæŸå¼ æ¡Œå­ç‰¹åˆ«é‡ï¼ˆæ¯”å¦‚ 200kgï¼‰ï¼Œå…¶ä»–è½»æ¡Œå­çš„ç²¾åº¦ä¼šå—å½±å“
```

**Rowwise Scalingï¼ˆæ¯å¼ æ¡Œå­ä¸€ä¸ªæ¯”ä¾‹ï¼‰**ï¼š
```
æ¡Œå­A: 45kg  ç¼©æ”¾æ¯”ä¾‹: 1.1x
æ¡Œå­B: 53kg  ç¼©æ”¾æ¯”ä¾‹: 0.9x
æ¡Œå­C: 39kg  ç¼©æ”¾æ¯”ä¾‹: 1.2x
æ¡Œå­D: 61kg  ç¼©æ”¾æ¯”ä¾‹: 0.8x

ä¼˜ç‚¹ï¼šæ¯å¼ æ¡Œå­éƒ½æœ‰æœ€ä¼˜ç²¾åº¦
ç¼ºç‚¹ï¼šéœ€è¦è®°å½•æ›´å¤šæ¯”ä¾‹ï¼Œè®¡ç®—å¼€é”€ç¨å¤§
```

---

## 3. Float8 vs BFloat16/Float32

### æ•°å€¼è¡¨ç¤ºèƒ½åŠ›å¯¹æ¯”

| æ•°æ®ç±»å‹ | ä½æ•° | æŒ‡æ•°ä½ | å°¾æ•°ä½ | èŒƒå›´ | ç²¾åº¦ |
|---------|-----|-------|-------|------|------|
| **Float32** | 32 | 8 | 23 | Â±3.4e38 | ~7 ä½åè¿›åˆ¶ |
| **BFloat16** | 16 | 8 | 7 | Â±3.4e38 | ~3 ä½åè¿›åˆ¶ |
| **Float8 E4M3** | 8 | 4 | 3 | Â±240 | ~1 ä½åè¿›åˆ¶ |

### ä¸ºä»€ä¹ˆ Float8 èƒ½è®­ç»ƒæ·±åº¦æ¨¡å‹ï¼Ÿ

è™½ç„¶ Float8 ç²¾åº¦å¾ˆä½ï¼Œä½†åœ¨è®­ç»ƒä¸­ï¼š

1. **æ¢¯åº¦æ›´æ–°æ˜¯ç´¯ç§¯çš„**ï¼šå•æ¬¡è®¡ç®—ç²¾åº¦ä½ï¼Œä½†å¤šæ¬¡ç´¯ç§¯åç²¾åº¦è¶³å¤Ÿ
2. **Scale åŠ¨æ€è°ƒæ•´**ï¼šé€šè¿‡ `max(abs(tensor))` åŠ¨æ€è®¡ç®— scaleï¼Œå……åˆ†åˆ©ç”¨è¡¨ç¤ºèŒƒå›´
3. **å…³é”®æ“ä½œä¿æŒé«˜ç²¾åº¦**ï¼šä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦ç´¯ç§¯ä»ç”¨ Float32

### è®¡ç®—ç¤ºä¾‹ï¼šFloat8 çŸ©é˜µä¹˜æ³•

**ä¼ ç»Ÿ BFloat16 çŸ©é˜µä¹˜æ³•**ï¼š
```python
# ä¸éœ€è¦ scale
output = torch.mm(input_bf16, weight_bf16)
```

**Float8 çŸ©é˜µä¹˜æ³•**ï¼š
```python
# éœ€è¦ scale æ¥æ¢å¤æ­£ç¡®çš„æ•°å€¼èŒƒå›´
output = torch._scaled_mm(
    input_fp8,              # Float8 è¾“å…¥
    weight_fp8,             # Float8 æƒé‡
    scale_a=scale_input,    # è¾“å…¥çš„ scale
    scale_b=scale_weight,   # æƒé‡çš„ scale
)
```

**Scale çš„è®¡ç®—**ï¼š
```python
# è®¡ç®—è¾“å…¥çš„ scale
amax_input = torch.max(torch.abs(input_bf16))
scale_input = 255.0 / amax_input  # Float8 E4M3 çš„æœ€å¤§å€¼æ˜¯ 240ï¼Œè¿™é‡Œç®€åŒ–ä¸º 255

# é‡åŒ–åˆ° Float8
input_fp8 = (input_bf16 * scale_input).to(torch.float8_e4m3fn)

# åŒæ ·è®¡ç®—æƒé‡çš„ scale
amax_weight = torch.max(torch.abs(weight_bf16))
scale_weight = 255.0 / amax_weight
weight_fp8 = (weight_bf16 * scale_weight).to(torch.float8_e4m3fn)

# Float8 çŸ©é˜µä¹˜æ³•
output_scaled = torch._scaled_mm(input_fp8, weight_fp8, scale_a=scale_input, scale_b=scale_weight)

# è¾“å‡ºå·²ç»è‡ªåŠ¨ descale å›æ­£ç¡®çš„æ•°å€¼èŒƒå›´
```

---

## 4. ä¸¤ç§ Scaling ç­–ç•¥

TorchTitan æ”¯æŒä¸¤ç§ Float8 scaling ç­–ç•¥ï¼Œå¯¹åº” TorchAO çš„ä¸¤ç§ recipeã€‚

### 4.1 Tensorwise Scalingï¼ˆå¼ é‡çº§ç¼©æ”¾ï¼‰

**å®šä¹‰**ï¼šæ•´ä¸ª tensor ä½¿ç”¨ä¸€ä¸ª scaleã€‚

```python
# Tensorwise scaling
amax = torch.max(torch.abs(tensor))  # æ•´ä¸ª tensor çš„æœ€å¤§ç»å¯¹å€¼
scale = 255.0 / amax
tensor_fp8 = (tensor * scale).to(torch.float8_e4m3fn)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **è®¡ç®—ç®€å•**ï¼šåªéœ€è®¡ç®—ä¸€ä¸ª amax
- âœ… **é€šä¿¡é«˜æ•ˆ**ï¼šFSDP all-gather æ—¶ï¼Œæ¯ä¸ªå‚æ•°åªéœ€é€šä¿¡ä¸€ä¸ª scale
- âœ… **é€Ÿåº¦å¿«**ï¼šå¼€é”€å°ï¼Œé€‚åˆå¤§è§„æ¨¡è®­ç»ƒ

**ç¼ºç‚¹**ï¼š
- âš ï¸ **ç²¾åº¦å—é™**ï¼šå¦‚æœ tensor ä¸­æœ‰æç«¯å€¼ï¼Œå…¶ä»–å€¼çš„ç²¾åº¦ä¼šå—å½±å“
- âš ï¸ **ä¸é€‚åˆ outliers**ï¼šå½“ tensor ä¸­æœ‰å°‘æ•°å¼‚å¸¸å¤§/å°çš„å€¼æ—¶ï¼Œç²¾åº¦æŸå¤±æ˜æ˜¾

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆFSDP + TPï¼‰
- è¿½æ±‚æœ€å¤§ååé‡
- æ¨¡å‹æƒé‡åˆ†å¸ƒç›¸å¯¹å‡åŒ€

### 4.2 Rowwise Scalingï¼ˆè¡Œçº§ç¼©æ”¾ï¼‰

**å®šä¹‰**ï¼šå¯¹äºçŸ©é˜µçš„æ¯ä¸€è¡Œï¼Œä½¿ç”¨ç‹¬ç«‹çš„ scaleã€‚

```python
# Rowwise scalingï¼ˆå‡è®¾ tensor æ˜¯ 2Dï¼‰
amax_per_row = torch.max(torch.abs(tensor), dim=1, keepdim=True)[0]  # æ¯è¡Œçš„æœ€å¤§ç»å¯¹å€¼
scale_per_row = 255.0 / amax_per_row
tensor_fp8 = (tensor * scale_per_row).to(torch.float8_e4m3fn)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **ç²¾åº¦æ›´é«˜**ï¼šæ¯è¡Œç‹¬ç«‹ç¼©æ”¾ï¼Œä¸å—å…¶ä»–è¡Œå½±å“
- âœ… **é²æ£’æ€§å¼º**ï¼šå¯¹ outliers ä¸æ•æ„Ÿ
- âœ… **æ”¶æ•›æ›´å¥½**ï¼šåœ¨ä¸€äº›ä»»åŠ¡ä¸Šæ”¶æ•›æ›²çº¿æ›´æ¥è¿‘ BF16

**ç¼ºç‚¹**ï¼š
- âš ï¸ **è®¡ç®—å¼€é”€å¤§**ï¼šéœ€è¦è®¡ç®—æ¯è¡Œçš„ amax
- âš ï¸ **é€šä¿¡å¼€é”€å¤§**ï¼šFSDP/TP é€šä¿¡æ—¶ï¼Œéœ€è¦ä¼ è¾“æ›´å¤š scales
- âš ï¸ **ç¼–è¯‘å‹å¥½æ€§**ï¼šéœ€è¦ `torch.compile` æ¥ä¼˜åŒ–æ€§èƒ½

**é€‚ç”¨åœºæ™¯**ï¼š
- å¯¹ç²¾åº¦è¦æ±‚é«˜çš„ä»»åŠ¡
- æƒé‡åˆ†å¸ƒä¸å‡åŒ€ï¼ˆå­˜åœ¨ outliersï¼‰
- è¿½æ±‚æ”¶æ•›è´¨é‡è€Œéæè‡´é€Ÿåº¦

### ä¸¤ç§ç­–ç•¥çš„æ€§èƒ½å¯¹æ¯”

**Llama3 70B (256 H100s, FSDP=32, TP=8)**ï¼š

| é…ç½® | TPS/GPU | ç›¸å¯¹ BF16 åŠ é€Ÿ |
|-----|---------|--------------|
| BFloat16 baseline | 597 | 1.00x |
| Float8 tensorwise | 810 | **1.36x** |
| Float8 rowwise | 600 | 1.00x |

**è§‚å¯Ÿ**ï¼š
- Tensorwise åœ¨è¿™ä¸ªè§„æ¨¡ä¸‹æœ‰æ˜¾è‘—åŠ é€Ÿï¼ˆ1.36xï¼‰
- Rowwise é€Ÿåº¦æ¥è¿‘ BF16ï¼ˆå› ä¸ºè®¡ç®—å’Œé€šä¿¡å¼€é”€æŠµæ¶ˆäº† FP8 Tensor Core çš„ä¼˜åŠ¿ï¼‰
- å¦‚æœé…åˆ AsyncTPï¼Œtensorwise å¯ä»¥è¾¾åˆ° 1.16x åŠ é€Ÿï¼ˆç›¸å¯¹ BF16 + AsyncTPï¼‰

---

## 5. Float8 ä¸ FSDP çš„ç»“åˆ

### 5.1 ä¼ ç»Ÿ FSDP çš„é—®é¢˜

å›é¡¾ FSDP çš„å·¥ä½œæµç¨‹ï¼š

```
Forward:
  1. All-Gather æƒé‡åˆ†ç‰‡ï¼ˆBFloat16ï¼‰  â† é€šä¿¡ç“¶é¢ˆ
  2. è®¡ç®—ï¼ˆBFloat16ï¼‰
  3. Reshard é‡Šæ”¾å†…å­˜

Backward:
  1. All-Gather æƒé‡åˆ†ç‰‡ï¼ˆBFloat16ï¼‰  â† é€šä¿¡ç“¶é¢ˆ
  2. è®¡ç®—æ¢¯åº¦ï¼ˆBFloat16ï¼‰
  3. Reduce-Scatter æ¢¯åº¦ï¼ˆBFloat16ï¼‰ â† é€šä¿¡ç“¶é¢ˆ
  4. Reshard
```

**é€šä¿¡å¼€é”€å·¨å¤§**ï¼šAll-Gather å’Œ Reduce-Scatter æ˜¯é€šä¿¡å¯†é›†å‹æ“ä½œã€‚

### 5.2 Float8 All-Gather ä¼˜åŒ–

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨ All-Gather ä¹‹å‰ï¼Œå°†æƒé‡ä» BFloat16 è½¬æ¢ä¸º Float8ï¼Œé€šä¿¡é‡å‡åŠï¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FSDP Float8 Forward                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Rank 0: [param_shard_0] (BFloat16, å­˜å‚¨)                  â”‚
â”‚  Rank 1: [param_shard_1] (BFloat16, å­˜å‚¨)                  â”‚
â”‚  Rank 2: [param_shard_2] (BFloat16, å­˜å‚¨)                  â”‚
â”‚  Rank 3: [param_shard_3] (BFloat16, å­˜å‚¨)                  â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 1: æœ¬åœ°é‡åŒ–ï¼ˆCast to Float8ï¼‰                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  Rank 0: [param_shard_0_fp8] + scale_0                     â”‚
â”‚  Rank 1: [param_shard_1_fp8] + scale_1                     â”‚
â”‚  Rank 2: [param_shard_2_fp8] + scale_2                     â”‚
â”‚  Rank 3: [param_shard_3_fp8] + scale_3                     â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 2: All-Gather Float8 æƒé‡ï¼ˆé€šä¿¡é‡å‡åŠï¼ï¼‰              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  æ¯ä¸ª Rank éƒ½æœ‰: [param_fp8_full] = concat([shard_0_fp8,   â”‚
â”‚                                              shard_1_fp8,   â”‚
â”‚                                              shard_2_fp8,   â”‚
â”‚                                              shard_3_fp8])  â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 3: è®¡ç®—å…¨å±€ scaleï¼ˆAll-Reduce scalesï¼‰                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  global_scale = max(scale_0, scale_1, scale_2, scale_3)    â”‚
â”‚  â†’ é€šè¿‡ All-Reduce é€šä¿¡                                     â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 4: Float8 çŸ©é˜µä¹˜æ³•                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  output = torch._scaled_mm(input_fp8, param_fp8_full,      â”‚
â”‚                            scale_a=scale_input,             â”‚
â”‚                            scale_b=global_scale)            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Precompute Scale for FSDP

**é—®é¢˜**ï¼šæ¯ä¸ªå‚æ•°å•ç‹¬ All-Reduce scaleï¼Œé€šä¿¡æ¬¡æ•°å¤ªå¤šï¼

**è§£å†³æ–¹æ¡ˆ**ï¼š`precompute_float8_dynamic_scale_for_fsdp`

```python
# åŸå§‹æ–¹å¼ï¼šæ¯ä¸ªå‚æ•°å•ç‹¬ All-Reduce scale
for param in model.parameters():
    local_amax = torch.max(torch.abs(param))
    global_amax = torch.distributed.all_reduce(local_amax, op=ReduceOp.MAX)  # â† N æ¬¡é€šä¿¡ï¼
    scale = 255.0 / global_amax
```

**ä¼˜åŒ–æ–¹å¼**ï¼šå°†æ‰€æœ‰ scales åˆå¹¶æˆä¸€ä¸ª All-Reduce

```python
# TorchAO çš„ä¼˜åŒ–ï¼šä¸€æ¬¡ All-Reduce é€šä¿¡æ‰€æœ‰ scales
from torchao.float8 import precompute_float8_dynamic_scale_for_fsdp

# åœ¨ optimizer step ä¹‹åè°ƒç”¨
precompute_float8_dynamic_scale_for_fsdp(model)

# åŸç†ï¼š
# 1. æ”¶é›†æ‰€æœ‰å‚æ•°çš„ local amax
# 2. æ‰“åŒ…æˆä¸€ä¸ª tensor: [amax_0, amax_1, ..., amax_N]
# 3. ä¸€æ¬¡ All-Reduce é€šä¿¡
# 4. ä¸ºæ¯ä¸ªå‚æ•°è®¡ç®— global scale
```

**æ€§èƒ½æå‡**ï¼š
- âŒ ä¸ä¼˜åŒ–ï¼šN ä¸ªå‚æ•° = N æ¬¡å°çš„ All-Reduceï¼ˆlatency é«˜ï¼‰
- âœ… ä¼˜åŒ–åï¼š1 æ¬¡å¤§çš„ All-Reduceï¼ˆlatency ä½ï¼Œbandwidth åˆ©ç”¨ç‡é«˜ï¼‰

### 5.4 é…ç½®ç¤ºä¾‹

```toml
[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = true   # å¯ç”¨ Float8 all-gather
precompute_float8_dynamic_scale_for_fsdp = true  # ä¼˜åŒ– scale é€šä¿¡
```

---

## 6. Float8 ä¸ TP çš„ç»“åˆ

### 6.1 TP ä¸­çš„é€šä¿¡æ¨¡å¼

å›é¡¾ TP çš„é€šä¿¡æ¨¡å¼ï¼ˆä»¥ Colwise Parallel ä¸ºä¾‹ï¼‰ï¼š

```
Input: [batch, seq_len, hidden]  (Replicate)
Weight: [hidden, ffn_dim]  (Shard on dim=1, åˆ—åˆ‡åˆ†)

Forward:
  1. è¾“å…¥åœ¨æ‰€æœ‰ TP ranks ä¸Šæ˜¯ç›¸åŒçš„ï¼ˆReplicateï¼‰
  2. æ¯ä¸ª rank è®¡ç®— matmul(input, weight_shard)
  3. è¾“å‡º: [batch, seq_len, ffn_dim] (Shard on dim=-1)
```

**å…³é”®é—®é¢˜**ï¼šè¾“å…¥æ˜¯ Replicate çš„ï¼Œä½†åœ¨ TP åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—**å…¨å±€çš„ scale**ã€‚

### 6.2 Float8 TP çš„å®ç°

**Tensorwise Float8 TP**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Float8 TP Colwise Forward                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Input (Replicate):  [batch, seq, hidden]                  â”‚
â”‚                                                             â”‚
â”‚  Rank 0: weight_shard_0 [hidden, ffn_dim/4]                â”‚
â”‚  Rank 1: weight_shard_1 [hidden, ffn_dim/4]                â”‚
â”‚  Rank 2: weight_shard_2 [hidden, ffn_dim/4]                â”‚
â”‚  Rank 3: weight_shard_3 [hidden, ffn_dim/4]                â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 1: è®¡ç®— Input çš„å…¨å±€ scaleï¼ˆéœ€è¦åœ¨ TP group é€šä¿¡ï¼‰    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  local_amax_input = max(abs(input))  # æ¯ä¸ª rank ç›¸åŒ      â”‚
â”‚  global_amax_input = local_amax_input  # TP ä¸­ input æ˜¯ replicate çš„ â”‚
â”‚  scale_input = 255.0 / global_amax_input                   â”‚
â”‚  input_fp8 = cast_to_fp8(input, scale_input)               â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 2: è®¡ç®— Weight çš„å…¨å±€ scaleï¼ˆéœ€è¦åœ¨ TP group é€šä¿¡ï¼‰   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  Rank 0: local_amax_0 = max(abs(weight_shard_0))           â”‚
â”‚  Rank 1: local_amax_1 = max(abs(weight_shard_1))           â”‚
â”‚  Rank 2: local_amax_2 = max(abs(weight_shard_2))           â”‚
â”‚  Rank 3: local_amax_3 = max(abs(weight_shard_3))           â”‚
â”‚                                                             â”‚
â”‚  global_amax_weight = All-Reduce(local_amax, op=MAX)       â”‚
â”‚  â†’ åœ¨ TP group å†…é€šä¿¡                                       â”‚
â”‚                                                             â”‚
â”‚  scale_weight = 255.0 / global_amax_weight                 â”‚
â”‚  weight_fp8 = cast_to_fp8(weight_shard, scale_weight)      â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 3: Float8 çŸ©é˜µä¹˜æ³•                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  output_fp8 = torch._scaled_mm(input_fp8, weight_fp8,      â”‚
â”‚                                scale_a=scale_input,         â”‚
â”‚                                scale_b=scale_weight)        â”‚
â”‚                                                             â”‚
â”‚  Output: [batch, seq, ffn_dim/4] (Shard on dim=-1)        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Rowwise Float8 TP**ï¼š

å¯¹äº Rowwise scalingï¼Œé€šä¿¡å¼€é”€æ›´å¤§ï¼Œå› ä¸ºæ¯ä¸€è¡Œéƒ½éœ€è¦é€šä¿¡ scaleï¼š

```python
# Rowwise scaling in TP
amax_per_row_local = max(abs(weight_shard), dim=1)  # æ¯è¡Œçš„ local amax
# éœ€è¦ All-Reduce æ¯ä¸€è¡Œçš„ amaxï¼ˆé€šä¿¡é‡å¤§ï¼ï¼‰
amax_per_row_global = torch.distributed.all_reduce(amax_per_row_local, op=ReduceOp.MAX)
```

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Rowwise Float8 åœ¨ TP ä¸­æ€§èƒ½æå‡ä¸æ˜æ˜¾**ï¼šé€šä¿¡å¼€é”€æŠµæ¶ˆäº† FP8 Tensor Core çš„ä¼˜åŠ¿ã€‚

### 6.3 Float8 All-Gather for TP

åœ¨æŸäº› TP æ¨¡å¼ä¸‹ï¼ˆä¾‹å¦‚ Sequence Parallelï¼‰ï¼Œè¾“å…¥ä¹Ÿæ˜¯ Shard çš„ï¼Œéœ€è¦ All-Gatherï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Float8 TP with Sequence Parallel (All-Gather)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Input (Shard on seq_len):                                 â”‚
â”‚    Rank 0: [batch, seq_len/4, hidden]                      â”‚
â”‚    Rank 1: [batch, seq_len/4, hidden]                      â”‚
â”‚    Rank 2: [batch, seq_len/4, hidden]                      â”‚
â”‚    Rank 3: [batch, seq_len/4, hidden]                      â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 1: Cast input to Float8 + compute scale              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  local_amax = max(abs(input_shard))                        â”‚
â”‚  global_amax = All-Reduce(local_amax, op=MAX)  â† é€šä¿¡ scaleâ”‚
â”‚  scale_input = 255.0 / global_amax                         â”‚
â”‚  input_fp8_shard = cast_to_fp8(input_shard, scale_input)   â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 2: Float8 All-Gatherï¼ˆé€šä¿¡é‡å‡åŠï¼ï¼‰                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  input_fp8_full = All-Gather(input_fp8_shard)              â”‚
â”‚  â†’ æ¯ä¸ª rank: [batch, seq_len, hidden] (Float8)           â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Step 3: Float8 matmul                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  output = torch._scaled_mm(input_fp8_full, weight_fp8, ...)â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. æºç å®ç°è¯¦è§£

### 7.1 Float8LinearConverter ç±»

æ–‡ä»¶ï¼š`torchtitan/components/quantization/float8.py`

è¿™æ˜¯ TorchTitan ä¸­è´Ÿè´£å°†æ¨¡å‹è½¬æ¢ä¸º Float8 çš„æ ¸å¿ƒç±»ã€‚

```python
class Float8LinearConverter(QuantizationConverter):
    def __init__(self, job_config: JobConfig, parallel_dims: ParallelDims):
        super().__init__(job_config, parallel_dims)
        float8_config: Float8Linear = job_config.quantize.linear.float8

        # 1. æ£€æŸ¥ç¡¬ä»¶æ”¯æŒï¼ˆéœ€è¦ SM89 æˆ–æ›´é«˜ï¼Œå³ H100+ï¼‰
        if has_cuda_capability(8, 9) or (
            float8_config.emulate and not model_compile_enabled
        ):
            pass
        else:
            raise ValueError(
                "Float8 is only supported on SM89 or later (H100+)."
            )

        # 2. å¯¼å…¥ TorchAO çš„ Float8LinearConfig
        from torchao.float8 import Float8LinearConfig as TorchAOFloat8LinearConfig

        # 3. æ ¹æ® recipe_name æˆ–æ‰‹åŠ¨é…ç½®åˆ›å»º config
        if float8_config.recipe_name is not None:
            # ä½¿ç”¨é¢„å®šä¹‰çš„ recipeï¼ˆtensorwise, rowwise, rowwise_with_gw_hpï¼‰
            self.config = TorchAOFloat8LinearConfig.from_recipe_name(
                float8_config.recipe_name
            )
            self.precompute_scale = False
        else:
            # æ‰‹åŠ¨é…ç½® tensorwise scaling
            enable_fsdp_float8_all_gather = (
                parallel_dims.dp_shard_enabled
                and float8_config.enable_fsdp_float8_all_gather
            )
            self.config = TorchAOFloat8LinearConfig(
                enable_fsdp_float8_all_gather=enable_fsdp_float8_all_gather,
                emulate=float8_config.emulate,
            )
            # æ˜¯å¦å¯ç”¨ precompute_scale ä¼˜åŒ–
            self.precompute_scale = (
                enable_fsdp_float8_all_gather
                and float8_config.precompute_float8_dynamic_scale_for_fsdp
            )

        # 4. åˆå§‹åŒ–è¿‡æ»¤å‡½æ•°ï¼ˆå“ªäº›å±‚ä¸è½¬æ¢ä¸º Float8ï¼‰
        self.filter_fn = self._init_filter_fn(float8_config)

        self.enabled = True
```

**å…³é”®ç‚¹**ï¼š
1. **ç¡¬ä»¶æ£€æŸ¥**ï¼šFloat8 éœ€è¦ H100+ GPUï¼ˆSM89ï¼‰ï¼Œå¦åˆ™åªèƒ½ç”¨ `emulate=True` æ¨¡æ‹Ÿï¼ˆæ€§èƒ½å·®ï¼‰
2. **Recipe é€‰æ‹©**ï¼šå¯ä»¥ç”¨é¢„å®šä¹‰ recipeï¼ˆtensorwise, rowwiseï¼‰æˆ–æ‰‹åŠ¨é…ç½®
3. **FSDP ä¼˜åŒ–**ï¼šé€šè¿‡ `enable_fsdp_float8_all_gather` å¯ç”¨ Float8 all-gather
4. **Precompute Scale**ï¼šé€šè¿‡ `precompute_float8_dynamic_scale_for_fsdp` å‡å°‘é€šä¿¡æ¬¡æ•°

### 7.2 æ¨¡å‹è½¬æ¢ï¼šconvert æ–¹æ³•

```python
def convert(self, model: nn.Module):
    """
    å°†æ¨¡å‹çš„ nn.Linear å±‚è½¬æ¢ä¸º Float8Linearã€‚
    """
    if not self.enabled:
        return

    from torchao.float8 import convert_to_float8_training

    # è°ƒç”¨ TorchAO çš„è½¬æ¢å‡½æ•°
    convert_to_float8_training(
        model,
        config=self.config,
        module_filter_fn=self.filter_fn,  # è¿‡æ»¤ä¸éœ€è¦è½¬æ¢çš„å±‚
    )
    logger.info(
        f"Swapped to Float8Linear layers with enable_fsdp_float8_all_gather="
        f"{self.config.enable_fsdp_float8_all_gather}"
    )
```

**convert_to_float8_training åšäº†ä»€ä¹ˆï¼Ÿ**

1. éå†æ¨¡å‹çš„æ‰€æœ‰ `nn.Linear` å±‚
2. æ ¹æ® `module_filter_fn` å†³å®šæ˜¯å¦è½¬æ¢ï¼ˆä¾‹å¦‚è·³è¿‡ `output` å±‚ï¼‰
3. å°† `nn.Linear` æ›¿æ¢ä¸º `Float8Linear`
4. `Float8Linear` çš„ forward ä¼šè‡ªåŠ¨å¤„ç† Float8 é‡åŒ–å’Œ scaled_mm

### 7.3 Precompute Scale ä¼˜åŒ–

```python
def post_optimizer_hook(self, model: nn.Module | list[nn.Module]):
    """
    åœ¨ optimizer step ä¹‹åè°ƒç”¨ï¼Œé¢„è®¡ç®—æ‰€æœ‰å‚æ•°çš„ Float8 scalesã€‚
    """
    if not self.enabled:
        return

    if not self.precompute_scale:
        return

    from torchao.float8 import precompute_float8_dynamic_scale_for_fsdp

    models = [model] if isinstance(model, nn.Module) else model
    for m in models:
        precompute_float8_dynamic_scale_for_fsdp(m)
```

**precompute_float8_dynamic_scale_for_fsdp çš„å®ç°åŸç†**ï¼š

```python
# ä¼ªä»£ç ï¼šTorchAO ä¸­çš„å®ç°
def precompute_float8_dynamic_scale_for_fsdp(model):
    # 1. æ”¶é›†æ‰€æœ‰ Float8Linear å±‚çš„å‚æ•°
    params = []
    for module in model.modules():
        if isinstance(module, Float8Linear):
            params.append(module.weight)

    # 2. è®¡ç®—æ¯ä¸ªå‚æ•°çš„ local amax
    local_amaxs = []
    for param in params:
        local_amax = torch.max(torch.abs(param))
        local_amaxs.append(local_amax)

    # 3. æ‰“åŒ…æˆä¸€ä¸ª tensorï¼Œä¸€æ¬¡æ€§ All-Reduce
    local_amaxs_tensor = torch.stack(local_amaxs)
    global_amaxs_tensor = torch.distributed.all_reduce(
        local_amaxs_tensor,
        op=ReduceOp.MAX
    )

    # 4. ä¸ºæ¯ä¸ªå‚æ•°ç¼“å­˜ scale
    for i, param in enumerate(params):
        global_amax = global_amaxs_tensor[i]
        scale = 255.0 / global_amax
        param._float8_scale = scale  # ç¼“å­˜ scale
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| æ–¹æ³• | é€šä¿¡æ¬¡æ•° | Latency |
|-----|---------|---------|
| ä¸ä¼˜åŒ– | N æ¬¡ All-Reduceï¼ˆN = å‚æ•°æ•°é‡ï¼‰ | é«˜ |
| Precompute | 1 æ¬¡ All-Reduce | ä½ |

å¯¹äº Llama3 70Bï¼ˆ~80 ä¸ª Linear å±‚ï¼‰ï¼Œä» 80 æ¬¡é€šä¿¡é™åˆ° 1 æ¬¡ï¼

### 7.4 Filter FQNsï¼šé€‰æ‹©æ€§è½¬æ¢

**ä¸ºä»€ä¹ˆéœ€è¦è¿‡æ»¤ï¼Ÿ**

å¹¶éæ‰€æœ‰ Linear å±‚éƒ½é€‚åˆ Float8ï¼š
1. **å°çŸ©é˜µ**ï¼šçŸ©é˜µå¤ªå°æ—¶ï¼Œé‡åŒ–å¼€é”€ > FP8 Tensor Core æ”¶ç›Š
2. **ç²¾åº¦æ•æ„Ÿå±‚**ï¼šæŸäº›å±‚ï¼ˆå¦‚ output projectionï¼‰å¯¹ç²¾åº¦è¦æ±‚é«˜

**é…ç½®ç¤ºä¾‹**ï¼š

```toml
[quantize.linear.float8]
filter_fqns = ["output", "attention.wk"]  # ä¸è½¬æ¢è¿™äº›å±‚
```

**Auto Filter**ï¼š

TorchTitan æ”¯æŒè‡ªåŠ¨è¿‡æ»¤å°çŸ©é˜µï¼š

```toml
[quantize.linear.float8]
filter_fqns = ["auto_filter_small_kn"]  # è‡ªåŠ¨è¿‡æ»¤ K,N ç»´åº¦è¿‡å°çš„å±‚
```

**å®ç°åŸç†**ï¼š

```python
def _init_filter_fn(self, float8_config: Float8Linear):
    use_auto_filter = "auto_filter_small_kn" in float8_config.filter_fqns
    if use_auto_filter:
        from torchao.float8 import _auto_filter_for_recipe

        # æ ¹æ® recipe è‡ªåŠ¨å†³å®šé˜ˆå€¼
        return _auto_filter_for_recipe(
            recipe_name,
            filter_fqns=float8_config.filter_fqns,
        )

    # æ‰‹åŠ¨è¿‡æ»¤
    return partial(module_filter_fn, filter_fqns=float8_config.filter_fqns)
```

**Auto filter çš„é˜ˆå€¼**ï¼ˆåŸºäº H100 microbenchmarkï¼‰ï¼š

| Recipe | K é˜ˆå€¼ | N é˜ˆå€¼ |
|--------|-------|-------|
| tensorwise | K â‰¥ 2048 | N â‰¥ 2048 |
| rowwise | K â‰¥ 4096 | N â‰¥ 4096 |

åªæœ‰å½“çŸ©é˜µçš„ K å’Œ N éƒ½è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œæ‰è½¬æ¢ä¸º Float8ã€‚

### 7.5 ä¸å¹¶è¡Œç­–ç•¥çš„é›†æˆ

åœ¨ `torchtitan/models/llama3/infra/parallelize.py` ä¸­ï¼š

```python
def parallelize_llama(
    model: nn.Module,
    parallel_dims: ParallelDims,
    job_config: JobConfig,
):
    # 1. åº”ç”¨ TP
    if parallel_dims.tp_enabled:
        enable_float8_linear = "float8" in job_config.model.converters
        float8_is_rowwise = job_config.quantize.linear.float8.recipe_name in (
            "rowwise",
            "rowwise_with_gw_hp",
        )

        # Tensorwise Float8 æ”¯æŒ Float8 all-gather in TP
        # Rowwise Float8 ä½¿ç”¨é«˜ç²¾åº¦é€šä¿¡
        enable_float8_tensorwise_tp = enable_float8_linear and not float8_is_rowwise

        apply_tp(
            model,
            world_mesh["tp"],
            loss_parallel=not job_config.parallelism.disable_loss_parallel,
            enable_float8_tensorwise_tp=enable_float8_tensorwise_tp,  # â† ä¼ é€’ç»™ TP
        )

    # 2. åº”ç”¨ AC
    if job_config.activation_checkpoint.mode != "none":
        apply_ac(model, job_config.activation_checkpoint, ...)

    # 3. åº”ç”¨ torch.compile
    if model_compile_enabled:
        apply_compile(model, job_config.compile)

    # 4. åº”ç”¨ FSDP
    if parallel_dims.fsdp_enabled:
        apply_fsdp(model, ...)
```

**å…³é”®é¡ºåº**ï¼š
1. **å…ˆ TPï¼Œå FSDP**ï¼šè¿™æ · Float8 é‡åŒ–å‘ç”Ÿåœ¨ TP é€šä¿¡æ—¶
2. **å…ˆ ACï¼Œå Compile**ï¼šç¡®ä¿ checkpoint wrapper èƒ½è¢«ç¼–è¯‘
3. **Float8 è½¬æ¢åœ¨æ‰€æœ‰å¹¶è¡Œç­–ç•¥ä¹‹å‰**ï¼šé€šè¿‡ `model_converter` æœºåˆ¶

---

## 8. é…ç½®å’Œä½¿ç”¨

### 8.1 Tensorwise Float8 é…ç½®

**æœ€å¸¸ç”¨çš„é…ç½®**ï¼ˆæ¨èç”¨äºå¤§è§„æ¨¡è®­ç»ƒï¼‰ï¼š

```toml
[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = true
precompute_float8_dynamic_scale_for_fsdp = true
filter_fqns = ["auto_filter_small_kn"]  # è‡ªåŠ¨è¿‡æ»¤å°çŸ©é˜µ

[compile]
enable = true
components = ["model", "loss"]  # Float8 éœ€è¦ compile æ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½
```

**å‘½ä»¤è¡Œå¯åŠ¨**ï¼š

```bash
CONFIG_FILE="./torchtitan/models/llama3/train_configs/llama3_8b.toml" ./run_train.sh \
  --model.converters="quantize.linear.float8" \
  --quantize.linear.float8.enable_fsdp_float8_all_gather \
  --quantize.linear.float8.precompute_float8_dynamic_scale_for_fsdp \
  --compile.enable
```

### 8.2 Rowwise Float8 é…ç½®

**è¿½æ±‚ç²¾åº¦çš„é…ç½®**ï¼ˆé€‚åˆå°è§„æ¨¡æˆ–ç²¾åº¦æ•æ„Ÿä»»åŠ¡ï¼‰ï¼š

```toml
[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
recipe_name = "rowwise"  # ä½¿ç”¨ rowwise scaling
# ä¸å¯ç”¨ enable_fsdp_float8_all_gatherï¼ˆrowwise é€šä¿¡å¼€é”€å¤§ï¼‰

[compile]
enable = true
components = ["model", "loss"]  # Rowwise æ›´ä¾èµ– compile ä¼˜åŒ–
```

**å‘½ä»¤è¡Œå¯åŠ¨**ï¼š

```bash
CONFIG_FILE="./torchtitan/models/llama3/train_configs/llama3_8b.toml" ./run_train.sh \
  --model.converters="quantize.linear.float8" \
  --quantize.linear.float8.recipe_name=rowwise \
  --compile.enable
```

### 8.3 æ‰‹åŠ¨è¿‡æ»¤ç‰¹å®šå±‚

**è·³è¿‡ç²¾åº¦æ•æ„Ÿå±‚**ï¼š

```toml
[quantize.linear.float8]
filter_fqns = ["output", "attention.wk", "attention.wv"]
```

**å¦‚ä½•ç¡®å®šå“ªäº›å±‚éœ€è¦è¿‡æ»¤ï¼Ÿ**

1. **æŸ¥çœ‹ TorchAO çš„ microbenchmark**ï¼š[torchao/float8 performance](https://github.com/pytorch/ao/tree/main/torchao/float8#performance)
2. **å®éªŒéªŒè¯**ï¼šè®­ç»ƒæ—¶ç›‘æ§ loss æ›²çº¿ï¼Œå¦‚æœ Float8 æ”¶æ•›æ˜æ˜¾å˜å·®ï¼Œå°è¯•è¿‡æ»¤æ›´å¤šå±‚
3. **ç»éªŒè§„åˆ™**ï¼š
   - `output` projection é€šå¸¸éœ€è¦è¿‡æ»¤ï¼ˆå½±å“æœ€ç»ˆ logitsï¼‰
   - å°äº 2048x2048 çš„çŸ©é˜µå»ºè®®è¿‡æ»¤
   - MoE çš„ gate å±‚é€šå¸¸éœ€è¦é«˜ç²¾åº¦

### 8.4 Llama3 å„æ¨¡å‹é…ç½®

**Llama3 8B (8 GPUs)**ï¼š

```toml
[parallelism]
data_parallel_shard_degree = 8
tensor_parallel_degree = 1

[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = true
precompute_float8_dynamic_scale_for_fsdp = true
filter_fqns = ["auto_filter_small_kn"]

[compile]
enable = true

[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"
```

**Llama3 70B (256 GPUs)**ï¼š

```toml
[parallelism]
data_parallel_shard_degree = 32
tensor_parallel_degree = 8

[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = true
precompute_float8_dynamic_scale_for_fsdp = true
filter_fqns = ["output"]

[compile]
enable = true

[activation_checkpoint]
mode = "full"
```

**Llama3 405B (512 GPUs)**ï¼š

```toml
[parallelism]
data_parallel_shard_degree = 8
tensor_parallel_degree = 8
pipeline_parallel_degree = 8
enable_async_tensor_parallel = true

[model]
converters = ["quantize.linear.float8"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = true
precompute_float8_dynamic_scale_for_fsdp = true
filter_fqns = ["output"]

[compile]
enable = true

[activation_checkpoint]
mode = "full"
```

---

## 9. æ€§èƒ½æ•°æ®

### 9.1 Llama3 8B (8 H100s)

| é…ç½® | TPS/GPU | æ˜¾å­˜ (GiB) | ç›¸å¯¹ Baseline åŠ é€Ÿ |
|-----|---------|-----------|-----------------|
| FSDP (baseline) | 5,762 | 68.2 | 1.00x |
| FSDP + compile | 6,667 | 77.0 | 1.16x |
| FSDP + compile + Float8 | **8,532** | 76.8 | **1.48x** |

**è§‚å¯Ÿ**ï¼š
- Float8 åœ¨å°è§„æ¨¡ï¼ˆå•æœº 8 å¡ï¼‰ä¹Ÿæœ‰æ˜¾è‘—åŠ é€Ÿï¼ˆ1.48xï¼‰
- æ˜¾å­˜å ç”¨å‡ ä¹ä¸å˜ï¼ˆå› ä¸ºæ¿€æ´»å€¼ä»æ˜¯ BF16ï¼Œåªæœ‰æƒé‡é€šä¿¡ç”¨ Float8ï¼‰

### 9.2 Llama3 70B (256 H100s)

**é…ç½®**ï¼šFSDP=32, TP=8, local batch size=16, Full AC

| é…ç½® | TPS/GPU | æ˜¾å­˜ (GiB) | ç›¸å¯¹ Baseline åŠ é€Ÿ |
|-----|---------|-----------|-----------------|
| FSDP + TP + compile (baseline) | 597 | 65.5 | 1.00x |
| + Float8 tensorwise | **810** | 64.8 | **1.36x** |
| + Float8 tensorwise + AsyncTP | **942** | 64.8 | **1.58x** |

**è§‚å¯Ÿ**ï¼š
- Float8 åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æ”¶ç›Šæ›´å¤§ï¼ˆé€šä¿¡ç“¶é¢ˆæ˜æ˜¾ï¼‰
- é…åˆ AsyncTPï¼Œå¯ä»¥è¾¾åˆ° 1.58x åŠ é€Ÿï¼
- æ˜¾å­˜å ç”¨ç•¥å¾®é™ä½ï¼ˆå› ä¸º FSDP all-gather çš„ä¸´æ—¶ç¼“å†²åŒºå‡å°ï¼‰

### 9.3 Llama3 405B (512 H100s)

**é…ç½®**ï¼šFSDP=8, TP=8, PP=8, AsyncTP, local batch size=32, Full AC, Interleaved 1F1B

| é…ç½® | TPS/GPU | æ˜¾å­˜ (GiB) |
|-----|---------|-----------|
| FSDP + TP + PP + compile + Float8 + AsyncTP | **128** | 77.2 |

**è¯´æ˜**ï¼š
- 405B å¿…é¡»ä½¿ç”¨ Float8 æ‰èƒ½åœ¨ 512 å¡ä¸Šé«˜æ•ˆè®­ç»ƒ
- Float8 èŠ‚çœçš„é€šä¿¡å¸¦å®½ä½¿å¾— 3D å¹¶è¡Œæ›´é«˜æ•ˆ

### 9.4 Float8 Tensorwise vs Rowwise

**Llama3 70B (256 H100s, FSDP=32, TP=8)**

| Scaling ç­–ç•¥ | TPS/GPU | ç›¸å¯¹ BF16 åŠ é€Ÿ | æ”¶æ•›æ€§ |
|------------|---------|--------------|-------|
| BFloat16 baseline | 597 | 1.00x | âœ“ |
| Float8 tensorwise | 810 | 1.36x | âœ“ (ä¸ BF16 åŸºæœ¬ä¸€è‡´) |
| Float8 rowwise | 600 | 1.00x | âœ“âœ“ (ç•¥å¥½äº BF16) |

**è§‚å¯Ÿ**ï¼š
- Tensorwise é€Ÿåº¦å¿«ï¼Œæ”¶æ•›æ€§å¥½ï¼ˆæ¨èï¼‰
- Rowwise é€Ÿåº¦ä¸ BF16 æ¥è¿‘ï¼ˆé€šä¿¡å¼€é”€æŠµæ¶ˆäº†æ”¶ç›Šï¼‰ï¼Œä½†æ”¶æ•›æ€§ç¨å¥½

### 9.5 Float8 + AsyncTP çš„å åŠ æ•ˆæœ

**Llama3 70B (256 H100s)**

| é…ç½® | TPS/GPU | ç›¸å¯¹ Vanilla TP åŠ é€Ÿ |
|-----|---------|-------------------|
| Vanilla TP (BF16) | 597 | 1.00x |
| Vanilla TP + Float8 tensorwise | 810 | 1.36x |
| AsyncTP (BF16) | 652 | 1.09x |
| AsyncTP + Float8 tensorwise | **942** | **1.58x** |

**è§‚å¯Ÿ**ï¼š
- Float8 å’Œ AsyncTP çš„åŠ é€Ÿæ•ˆæœå¯ä»¥**å åŠ **ï¼
- Float8 é™ä½é€šä¿¡é‡ï¼ŒAsyncTP éšè—é€šä¿¡å»¶è¿Ÿï¼Œä¸¤è€…äº’è¡¥

---

## 10. æœ€ä½³å®è·µ

### 10.1 ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Float8ï¼Ÿ

âœ… **æ¨èä½¿ç”¨**ï¼š
1. **å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä¸–ç•Œå¤§å° â‰¥ 64 GPUsï¼Œé€šä¿¡ç“¶é¢ˆæ˜æ˜¾
2. **TP å¹¶è¡Œåº¦é«˜**ï¼šTP â‰¥ 8ï¼Œé€šä¿¡é‡å¤§
3. **å¤§çŸ©é˜µä¸ºä¸»**ï¼šæ¨¡å‹ä¸­å¤§éƒ¨åˆ† Linear å±‚çš„ K, N â‰¥ 2048
4. **H100+ GPU**ï¼šæœ‰ç¡¬ä»¶ FP8 Tensor Core æ”¯æŒ

âŒ **ä¸æ¨èä½¿ç”¨**ï¼š
1. **å°è§„æ¨¡è®­ç»ƒ**ï¼šå•æœº â‰¤ 8 GPUsï¼Œé€šä¿¡ä¸æ˜¯ç“¶é¢ˆ
2. **å°æ¨¡å‹**ï¼šæ¨¡å‹ < 1B å‚æ•°ï¼ŒçŸ©é˜µå¤ªå°
3. **ç²¾åº¦è¦æ±‚æé«˜**ï¼šç§‘å­¦è®¡ç®—ã€é‡‘èæ¨¡å‹ç­‰
4. **è€ç¡¬ä»¶**ï¼šA100 æˆ–æ›´æ—©çš„ GPUï¼ˆå¯ä»¥ç”¨ `emulate=True` æµ‹è¯•ï¼Œä½†æ²¡æœ‰åŠ é€Ÿï¼‰

### 10.2 Tensorwise vs Rowwise å¦‚ä½•é€‰æ‹©ï¼Ÿ

| åœºæ™¯ | æ¨èç­–ç•¥ | ç†ç”± |
|-----|---------|------|
| å¤§è§„æ¨¡è®­ç»ƒï¼ˆâ‰¥256 GPUsï¼‰ | **Tensorwise** | é€šä¿¡é«˜æ•ˆï¼Œé€Ÿåº¦å¿« |
| ä¸­å°è§„æ¨¡è®­ç»ƒï¼ˆ<256 GPUsï¼‰ | Rowwise æˆ– ä¸ç”¨ Float8 | Rowwise é€šä¿¡å¼€é”€å¤§ï¼Œæ”¶ç›Šä¸æ˜æ˜¾ |
| è¿½æ±‚æè‡´ååé‡ | **Tensorwise** | æœ€å¿« |
| è¿½æ±‚æ”¶æ•›è´¨é‡ | **Rowwise** | ç²¾åº¦é«˜ï¼Œé²æ£’æ€§å¼º |
| æ¨¡å‹æœ‰ outliers | **Rowwise** | æ¯è¡Œç‹¬ç«‹ç¼©æ”¾ï¼Œä¸å—æç«¯å€¼å½±å“ |
| é…åˆ AsyncTP | **Tensorwise** | ä¸¤è€…å åŠ æ•ˆæœæœ€å¥½ |

### 10.3 è°ƒä¼˜ Checklist

1. **å¯ç”¨ torch.compile**ï¼šFloat8 éœ€è¦ compile æ¥èåˆé‡åŒ–/åé‡åŒ– kernel
   ```toml
   [compile]
   enable = true
   components = ["model", "loss"]
   ```

2. **ä½¿ç”¨ Auto Filter**ï¼šè‡ªåŠ¨è·³è¿‡å°çŸ©é˜µ
   ```toml
   [quantize.linear.float8]
   filter_fqns = ["auto_filter_small_kn"]
   ```

3. **å¯ç”¨ Precompute Scale**ï¼ˆTensorwiseï¼‰ï¼šå‡å°‘é€šä¿¡æ¬¡æ•°
   ```toml
   [quantize.linear.float8]
   precompute_float8_dynamic_scale_for_fsdp = true
   ```

4. **è¿‡æ»¤ Output Layer**ï¼šä¿æŒæœ€ç»ˆè¾“å‡ºçš„é«˜ç²¾åº¦
   ```toml
   [quantize.linear.float8]
   filter_fqns = ["output", "auto_filter_small_kn"]
   ```

5. **é…åˆ AsyncTP**ï¼šå åŠ åŠ é€Ÿæ•ˆæœ
   ```toml
   [parallelism]
   enable_async_tensor_parallel = true
   ```

6. **ç›‘æ§æ”¶æ•›æ€§**ï¼šå¯¹æ¯” BF16 baseline çš„ loss æ›²çº¿
   - å¦‚æœ Float8 loss æ˜æ˜¾åé«˜ï¼Œå°è¯•ï¼š
     - è¿‡æ»¤æ›´å¤šå±‚ï¼ˆ`filter_fqns`ï¼‰
     - åˆ‡æ¢åˆ° Rowwise scaling
     - é™ä½å­¦ä¹ ç‡

### 10.4 å¸¸è§é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ Float8 æ²¡æœ‰åŠ é€Ÿï¼Ÿ**

A: å¯èƒ½çš„åŸå› ï¼š
1. çŸ©é˜µå¤ªå°ï¼šå¤§éƒ¨åˆ† Linear å±‚è¢« auto_filter è¿‡æ»¤äº†
2. æ²¡æœ‰å¯ç”¨ compileï¼šFloat8 kernel æ²¡æœ‰èåˆ
3. é€šä¿¡ä¸æ˜¯ç“¶é¢ˆï¼šå°è§„æ¨¡è®­ç»ƒï¼ˆ<64 GPUsï¼‰
4. ä½¿ç”¨ Rowwiseï¼šåœ¨ä¸­å°è§„æ¨¡ä¸‹ï¼ŒRowwise é€šä¿¡å¼€é”€å¤§

**Q2: Float8 ä¼šå½±å“æ”¶æ•›å—ï¼Ÿ**

A: ä¸€èˆ¬ä¸ä¼šã€‚åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šï¼š
- Tensorwise Float8ï¼šæ”¶æ•›æ›²çº¿ä¸ BF16 åŸºæœ¬ä¸€è‡´
- Rowwise Float8ï¼šæ”¶æ•›æ›²çº¿ç•¥å¥½äº BF16ï¼ˆç²¾åº¦æ›´é«˜ï¼‰

ä½†åœ¨æŸäº›ç²¾åº¦æ•æ„Ÿä»»åŠ¡ï¼ˆä¾‹å¦‚é•¿åºåˆ—ã€å° batch sizeï¼‰ï¼Œå¯èƒ½éœ€è¦ï¼š
- è¿‡æ»¤ç²¾åº¦æ•æ„Ÿå±‚ï¼ˆå¦‚ outputï¼‰
- ä½¿ç”¨ Rowwise scaling
- å¾®è°ƒå­¦ä¹ ç‡

**Q3: Float8 æ”¯æŒå“ªäº›å¹¶è¡Œç­–ç•¥ï¼Ÿ**

A: Float8 ä¸ TorchTitan çš„æ‰€æœ‰å¹¶è¡Œç­–ç•¥å…¼å®¹ï¼š
- âœ… FSDPï¼šæ”¯æŒ Float8 all-gather
- âœ… TPï¼šTensorwise æ”¯æŒ Float8 é€šä¿¡ï¼ŒRowwise ä½¿ç”¨é«˜ç²¾åº¦é€šä¿¡
- âœ… PPï¼šæ”¯æŒï¼ˆä½† Float8 ä¸»è¦ä¼˜åŒ–é€šä¿¡ï¼Œå¯¹ PP æ”¶ç›Šæœ‰é™ï¼‰
- âœ… CPï¼šæ”¯æŒ
- âœ… AsyncTPï¼šå®Œç¾é…åˆï¼Œå åŠ åŠ é€Ÿ

**Q4: å¦‚ä½•è°ƒè¯• Float8ï¼Ÿ**

1. **å¯¹æ¯” BF16 baseline**ï¼š
   ```bash
   # å…ˆè·‘ BF16 baseline
   ./run_train.sh  # ä¸åŠ  --model.converters

   # å†è·‘ Float8
   ./run_train.sh --model.converters="quantize.linear.float8" ...
   ```

2. **æ£€æŸ¥å“ªäº›å±‚è¢«è½¬æ¢**ï¼š
   ```python
   # åœ¨è®­ç»ƒè„šæœ¬ä¸­æ‰“å°æ¨¡å‹ç»“æ„
   print(model)
   # Float8Linear ä¼šæ˜¾ç¤º Float8Linear è€Œä¸æ˜¯ nn.Linear
   ```

3. **ç›‘æ§é€šä¿¡é‡**ï¼š
   ```bash
   # ä½¿ç”¨ NCCL è°ƒè¯•
   export NCCL_DEBUG=INFO
   # æŸ¥çœ‹ all-gather/reduce-scatter çš„ sizeï¼ˆFloat8 åº”è¯¥æ˜¯ BF16 çš„ä¸€åŠï¼‰
   ```

4. **Profiling**ï¼š
   ```toml
   [profiling]
   enable_profiling = true
   # æ£€æŸ¥ Float8 kernel çš„æ—¶é—´å æ¯”
   ```

---

## 11. æ€»ç»“

### Float8 Training çš„æ ¸å¿ƒè¦ç‚¹

1. **æœ¬è´¨**ï¼šç”¨ 8 ä½æµ®ç‚¹æ•°ä»£æ›¿ 16 ä½ï¼Œé€šè¿‡**åŠ¨æ€ scale** ä¿æŒç²¾åº¦
   - Float8 E4M3: 4 bits æŒ‡æ•°ï¼Œ3 bits å°¾æ•°
   - Scale = 255 / max(abs(tensor))

2. **ä¸¤ç§ Scaling ç­–ç•¥**ï¼š
   - **Tensorwise**ï¼šæ•´ä¸ª tensor ä¸€ä¸ª scaleï¼ˆå¿«ï¼Œé€‚åˆå¤§è§„æ¨¡ï¼‰
   - **Rowwise**ï¼šæ¯è¡Œä¸€ä¸ª scaleï¼ˆç²¾åº¦é«˜ï¼Œé€šä¿¡å¼€é”€å¤§ï¼‰

3. **ä¸åˆ†å¸ƒå¼è®­ç»ƒç»“åˆ**ï¼š
   - **FSDP Float8 all-gather**ï¼šé€šä¿¡é‡å‡åŠ
   - **TP Float8**ï¼šæƒé‡å’Œæ¿€æ´»éƒ½ç”¨ Float8ï¼ˆTensorwiseï¼‰
   - **Precompute Scale**ï¼šå°† N æ¬¡ All-Reduce åˆå¹¶ä¸º 1 æ¬¡

4. **æ€§èƒ½æå‡**ï¼š
   - Llama3 8B (8 GPUs): **1.48x** åŠ é€Ÿ
   - Llama3 70B (256 GPUs): **1.36x** åŠ é€Ÿï¼ˆFloat8ï¼‰â†’ **1.58x**ï¼ˆFloat8 + AsyncTPï¼‰
   - Llama3 405B (512 GPUs): å¿…é¡»ä½¿ç”¨ Float8 æ‰èƒ½é«˜æ•ˆè®­ç»ƒ

5. **æœ€ä½³å®è·µ**ï¼š
   - âœ… å¤§è§„æ¨¡è®­ç»ƒï¼ˆâ‰¥64 GPUsï¼‰
   - âœ… å¯ç”¨ torch.compile
   - âœ… ä½¿ç”¨ auto_filter è·³è¿‡å°çŸ©é˜µ
   - âœ… Tensorwise + Precompute Scale + AsyncTP ç»„åˆ

### æ¬æ¡Œå­æ¯”å–»æ€»ç»“

Float8 Training å°±åƒ**å‹ç¼©æ¬è¿**ï¼š

```
ä¼ ç»Ÿ BF16: æ ‡å‡†è´§è½¦è¿è¾“ï¼Œç²¾ç¡®è®°å½•é‡é‡ï¼ˆkgï¼Œå°æ•°ç‚¹å 1 ä½ï¼‰
Float8:    å°å‹è´§è½¦è¿è¾“ï¼Œè®°å½•é‡é‡ + ç¼©æ”¾æ¯”ä¾‹
           â†’ è´§è½¦æ›´å¿«ã€æ›´çœæ²¹
           â†’ é€šè¿‡ç¼©æ”¾æ¯”ä¾‹æ¢å¤ç²¾åº¦

Tensorwise: æ•´è½¦ä¸€ä¸ªæ¯”ä¾‹ï¼ˆç®€å•å¿«é€Ÿï¼‰
Rowwise:    æ¯å¼ æ¡Œå­ä¸€ä¸ªæ¯”ä¾‹ï¼ˆç²¾åº¦æ›´é«˜ï¼Œä½†éœ€è¦è®°å½•æ›´å¤šæ¯”ä¾‹ï¼‰
```

### æŠ€æœ¯æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Float8 Training Stack                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  TorchTitan (Integration Layer)                            â”‚
â”‚  â”œâ”€ Float8LinearConverter: æ¨¡å‹è½¬æ¢                         â”‚
â”‚  â”œâ”€ Precompute Scale: é€šä¿¡ä¼˜åŒ–                              â”‚
â”‚  â””â”€ Filter FQNs: é€‰æ‹©æ€§åº”ç”¨                                 â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  TorchAO (Implementation Layer)                            â”‚
â”‚  â”œâ”€ Float8Linear: Float8 çŸ©é˜µä¹˜æ³•                           â”‚
â”‚  â”œâ”€ Float8LinearConfig: é…ç½®ç®¡ç†                            â”‚
â”‚  â”œâ”€ convert_to_float8_training: æ¨¡å‹è½¬æ¢                    â”‚
â”‚  â””â”€ Recipes: tensorwise, rowwise, ...                      â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  PyTorch (Kernel Layer)                                    â”‚
â”‚  â”œâ”€ torch._scaled_mm: Float8 çŸ©é˜µä¹˜æ³• kernel                â”‚
â”‚  â”œâ”€ torch.compile: Kernel èåˆ                              â”‚
â”‚  â””â”€ torch.float8_e4m3fn: Float8 æ•°æ®ç±»å‹                    â”‚
â”‚                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                             â”‚
â”‚  CUDA (Hardware Layer)                                     â”‚
â”‚  â”œâ”€ FP8 Tensor Core: ç¡¬ä»¶åŠ é€Ÿï¼ˆH100+ï¼‰                      â”‚
â”‚  â””â”€ CUTLASS: é«˜æ€§èƒ½ GEMM library                            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. å‚è€ƒèµ„æ–™

### TorchTitan æ–‡æ¡£
- [docs/float8.md](../../docs/float8.md) - Float8 ä½¿ç”¨æŒ‡å—
- [benchmarks/llama3_h100_202412_torchtitan.md](../../benchmarks/llama3_h100_202412_torchtitan.md) - æ€§èƒ½ Benchmark

### TorchAO æ–‡æ¡£
- [torchao/float8](https://github.com/pytorch/ao/tree/main/torchao/float8) - Float8 å®ç°å’Œ API
- [torchao/float8 Performance](https://github.com/pytorch/ao/tree/main/torchao/float8#performance) - Microbenchmark

### PyTorch æ–‡æ¡£
- [torch.float8_e4m3fn](https://pytorch.org/docs/stable/tensors.html#torch.float8_e4m3fn) - Float8 æ•°æ®ç±»å‹
- [torch._scaled_mm](https://pytorch.org/docs/stable/generated/torch._scaled_mm.html) - Float8 çŸ©é˜µä¹˜æ³•

### å­¦æœ¯è®ºæ–‡
- **FP8 Formats for Deep Learning**: [arXiv:2209.05433](https://arxiv.org/abs/2209.05433)
- **FP8 Training**: NVIDIA çš„ FP8 è®­ç»ƒç™½çš®ä¹¦

### æºç ä½ç½®
- `torchtitan/components/quantization/float8.py` - Float8 è½¬æ¢å™¨
- `torchtitan/config/job_config.py:667-689` - Float8 é…ç½®
- `torchtitan/models/llama3/infra/parallelize.py:69-86` - Float8 ä¸ TP é›†æˆ

---

**æœ€åæ›´æ–°**ï¼š2025å¹´11æœˆ25æ—¥
