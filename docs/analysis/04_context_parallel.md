# Context Parallel (CP) å®ç°è¯¦è§£

## ç›®å½•
- [1. ä»€ä¹ˆæ˜¯ Context Parallelï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-context-parallel)
- [2. æ¬æ¡Œå­çš„æ–°æ¯”å–»](#2-æ¬æ¡Œå­çš„æ–°æ¯”å–»)
- [3. Ring Attention åŸç†](#3-ring-attention-åŸç†)
- [4. æºç å®ç°è¯¦è§£](#4-æºç å®ç°è¯¦è§£)
- [5. æ€§èƒ½åˆ†æ](#5-æ€§èƒ½åˆ†æ)
- [6. ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ](#6-ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯ Context Parallelï¼Ÿ

### 1.1 é•¿åºåˆ—çš„æŒ‘æˆ˜

åœ¨è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ç»å¸¸é¢ä¸´**åºåˆ—å¤ªé•¿**çš„é—®é¢˜ï¼š

```python
# Llama3 8B æ¨¡å‹çš„ Attention è®¡ç®—
batch_size = 8
seq_len = 8192  # 8K tokens
n_heads = 32
head_dim = 128

# Q, K, V çš„å½¢çŠ¶
Q = [8, 8192, 32, 128]  # éœ€è¦ 256 MB (bfloat16)
K = [8, 8192, 32, 128]  # éœ€è¦ 256 MB
V = [8, 8192, 32, 128]  # éœ€è¦ 256 MB

# Attention çŸ©é˜µ
Attention_weights = Q @ K^T  # [8, 32, 8192, 8192]
                             # éœ€è¦ 16 GBï¼ ğŸ˜±
```

**é—®é¢˜**ï¼š
- **å†…å­˜çˆ†ç‚¸**ï¼šAttention çŸ©é˜µæ˜¯ `O(seq_lenÂ²)`ï¼Œåºåˆ—è¶Šé•¿ï¼Œå†…å­˜å ç”¨å‘ˆ**å¹³æ–¹å¢é•¿**
- **å• GPU æ”¾ä¸ä¸‹**ï¼šå³ä½¿ç”¨ Flash Attention ä¼˜åŒ–ï¼Œè¶…é•¿åºåˆ—ï¼ˆ> 32Kï¼‰ä»ç„¶ä¼š OOM
- **Tensor Parallel ä¸å¤Ÿ**ï¼šTP åªåˆ‡åˆ† headsï¼Œä¸åˆ‡åˆ† sequence

### 1.2 Context Parallel çš„æ ¸å¿ƒæ€æƒ³

**æŠŠåºåˆ—åˆ‡åˆ†åˆ°å¤šä¸ª GPUï¼Œæ¯ä¸ª GPU å¤„ç†ä¸€æ®µ**

```
åŸå§‹åºåˆ— (seq_len = 8192):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token 0, 1, 2, ..., 8191             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    æ‰€æœ‰åœ¨ GPU 0

Context Parallel (CP = 4):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0 - 2047â”‚  â”‚2048-4095â”‚  â”‚4096-6143â”‚  â”‚6144-8191â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   GPU 0        GPU 1        GPU 2        GPU 3

æ¯ä¸ª GPU å¤„ç† 2048 ä¸ª tokens
```

**å…³é”®æŠ€æœ¯**ï¼š
- **Ring Attention**ï¼šç”¨"æ¥åŠ›"çš„æ–¹å¼è®©æ¯ä¸ª GPU çœ‹åˆ°å®Œæ•´çš„ä¸Šä¸‹æ–‡
- **åºåˆ—ç»´åº¦åˆ‡åˆ†**ï¼šä¸æ˜¯åˆ‡æ¨¡å‹ï¼Œè€Œæ˜¯åˆ‡è¾“å…¥
- **é€šä¿¡ä¼˜åŒ–**ï¼šä½¿ç”¨ All-Gather æˆ– All-to-All äº¤æ¢ KV cache

---

## 2. æ¬æ¡Œå­çš„æ–°æ¯”å–»

### 2.1 åœºæ™¯å›é¡¾ï¼šä»€ä¹ˆæ˜¯ TPï¼Ÿ

å›é¡¾ Tensor Parallelï¼š
- **æ¡Œå­** = ç¥ç»ç½‘ç»œå±‚çš„æƒé‡
- **TP** = æŠŠæ¡Œå­æœ¬èº«åˆ‡æˆå‡ å—ï¼Œåˆ†æ•£åˆ°å¤šä¸ª GPU

ä½† TP ä¸è§£å†³åºåˆ—å¤ªé•¿çš„é—®é¢˜ï¼

### 2.2 Context Parallelï¼šåˆ‡åˆ†å·¥ä½œé‡

**Context Parallel ä¸åˆ‡æ¡Œå­ï¼Œè€Œæ˜¯åˆ‡å·¥ä½œé‡**

æƒ³è±¡ä½ è¦åœ¨ä¸€å¼ **è¶…å¤§é»‘æ¿**ä¸Šå†™ä½œä¸šï¼ˆè¿™æ˜¯åºåˆ—ï¼‰ï¼š

```
ä¼ ç»Ÿæ–¹å¼ (æ²¡æœ‰ CP):
ä½ ä¸€ä¸ªäººåœ¨é»‘æ¿ä¸Šä»å·¦åˆ°å³å†™ 10000 ä¸ªå­—
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å­—1, å­—2, å­—3, ..., å­—10000                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ä¸€ä¸ªäººå†™ï¼Œç´¯æ­»äº† ğŸ˜“
        éœ€è¦è®°ä½å‰é¢æ‰€æœ‰å†™è¿‡çš„å­—ï¼ˆå†…å­˜çˆ†ç‚¸ï¼‰
```

**Context Parallel (CP = 4):**

```
æŠŠé»‘æ¿åˆ†æˆ 4 æ®µï¼Œ4 ä¸ªäººåŒæ—¶å†™

äºº1: â”‚ å­—1-2500    â”‚
äºº2: â”‚ å­—2501-5000 â”‚
äºº3: â”‚ å­—5001-7500 â”‚
äºº4: â”‚ å­—7501-10000â”‚

ä½†é—®é¢˜æ¥äº†ï¼šå†™å­—æ—¶è¦å‚è€ƒå‰é¢çš„å†…å®¹ï¼
æ¯”å¦‚äºº3å†™åˆ°"ä»–"ï¼Œè¦çŸ¥é“"ä»–"æŒ‡çš„æ˜¯è°ï¼ˆåœ¨äºº1çš„éƒ¨åˆ†ï¼‰
```

### 2.3 Ring Attentionï¼šæ¥åŠ›ä¼ é€’ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆï¼šåƒæ¥åŠ›èµ›ä¸€æ ·ä¼ é€’ä¿¡æ¯**

```
ç¬¬1è½®: æ¯ä¸ªäººæ‹¿ç€è‡ªå·±çš„çº¸æ¡
äºº1: [æ®µ1]         äºº2: [æ®µ2]         äºº3: [æ®µ3]         äºº4: [æ®µ4]

ç¬¬2è½®: ä¼ é€’çº¸æ¡ (é¡ºæ—¶é’ˆ)
äºº1: [æ®µ1, æ®µ2]    äºº2: [æ®µ2, æ®µ3]    äºº3: [æ®µ3, æ®µ4]    äºº4: [æ®µ4, æ®µ1]
     â†‘ æ¥åˆ°äºº2çš„çº¸æ¡     â†‘ æ¥åˆ°äºº3çš„       â†‘ æ¥åˆ°äºº4çš„       â†‘ æ¥åˆ°äºº1çš„

ç¬¬3è½®: ç»§ç»­ä¼ é€’
äºº1: [æ®µ1,æ®µ2,æ®µ3] äºº2: [æ®µ2,æ®µ3,æ®µ4] äºº3: [æ®µ3,æ®µ4,æ®µ1] äºº4: [æ®µ4,æ®µ1,æ®µ2]

ç¬¬4è½®: æœ€åä¸€æ¬¡ä¼ é€’
äºº1: [æ®µ1,2,3,4]   äºº2: [æ®µ2,3,4,1]   äºº3: [æ®µ3,4,1,2]   äºº4: [æ®µ4,1,2,3]
     â†‘ æ‰€æœ‰äººéƒ½çœ‹åˆ°äº†å®Œæ•´çš„å†…å®¹ï¼
```

**å…³é”®ç‚¹**ï¼š
- **åˆ†æ®µå¤„ç†**ï¼šæ¯äººåªè´Ÿè´£ä¸€æ®µï¼Œå‡å°‘å•äººå·¥ä½œé‡
- **æ¥åŠ›ä¼ é€’**ï¼šé€šè¿‡å¤šè½®ä¼ é€’ï¼Œè®©æ¯äººæœ€ç»ˆçœ‹åˆ°å…¨éƒ¨å†…å®¹
- **å¹¶è¡Œè®¡ç®—**ï¼š4 ä¸ªäººåŒæ—¶å·¥ä½œï¼Œæ•ˆç‡æå‡ 4 å€

### 2.4 å…·ä½“åˆ° Attention è®¡ç®—

```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd) @ V

ä¼ ç»Ÿæ–¹å¼ (seq_len = 8192):
Q: [batch, 8192, hidden]  åœ¨ GPU 0
K: [batch, 8192, hidden]  åœ¨ GPU 0
V: [batch, 8192, hidden]  åœ¨ GPU 0

è®¡ç®— Q @ K^T: [batch, 8192, 8192]  éœ€è¦ 16 GBï¼

Context Parallel (CP = 4):
æ¯ä¸ª GPU åªå¤„ç† 2048 ä¸ª query tokens

GPU 0: Q[0:2048]    çœ‹åˆ°å®Œæ•´çš„ K, V
GPU 1: Q[2048:4096] çœ‹åˆ°å®Œæ•´çš„ K, V
GPU 2: Q[4096:6144] çœ‹åˆ°å®Œæ•´çš„ K, V
GPU 3: Q[6144:8192] çœ‹åˆ°å®Œæ•´çš„ K, V

æ¯ä¸ª GPU çš„ Attention çŸ©é˜µ: [batch, 2048, 8192]  åªéœ€è¦ 4 GB
æ€»å†…å­˜: 4 GB Ã— 4 = 16 GB (æ²¡å˜ï¼Œä½†åˆ†æ•£äº†ï¼)
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**
- **Query åˆ‡åˆ†**ï¼šæ¯ä¸ª GPU åªè®¡ç®—ä¸€éƒ¨åˆ† query çš„ attention
- **KV è½®æ¢**ï¼šé€šè¿‡ Ring æœºåˆ¶ï¼Œè®©æ¯ä¸ª GPU çœ‹åˆ°å®Œæ•´çš„ K, V
- **å†…å­˜é™ä½**ï¼šå• GPU å†…å­˜ä» 16 GB é™åˆ° 4 GB

---

## 3. Ring Attention åŸç†

### 3.1 ä¼ ç»Ÿ Attention çš„è®¡ç®—

```python
# ä¼ªä»£ç 
def attention(Q, K, V):
    # Q: [batch, seq_len, hidden]
    # K, V: [batch, seq_len, hidden]

    scores = Q @ K.T / sqrt(d)       # [batch, seq_len, seq_len]
    weights = softmax(scores)        # [batch, seq_len, seq_len]
    output = weights @ V             # [batch, seq_len, hidden]
    return output
```

**é—®é¢˜**ï¼š`scores` çŸ©é˜µæ˜¯ `O(seq_lenÂ²)`

### 3.2 Ring Attention çš„è®¡ç®—æµç¨‹

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠ K, V åˆ‡æˆå¤šå—ï¼Œä¾æ¬¡å¤„ç†ï¼Œæœ€ååˆå¹¶

```python
# Context Parallel with Ring Attention (CP = 4)

# åˆå§‹çŠ¶æ€ï¼šæ¯ä¸ª GPU æœ‰è‡ªå·±çš„ä¸€æ®µ
GPU 0: Q0 [0:2048],    K0 [0:2048],    V0 [0:2048]
GPU 1: Q1 [2048:4096], K1 [2048:4096], V1 [2048:4096]
GPU 2: Q2 [4096:6144], K2 [4096:6144], V2 [4096:6144]
GPU 3: Q3 [6144:8192], K3 [6144:8192], V3 [6144:8192]

# æ¯ä¸ª GPU è¦è®¡ç®—è‡ªå·±çš„ Q å¯¹å®Œæ•´ K, V çš„ attention
# ä½†å®Œæ•´çš„ K, V åˆ†æ•£åœ¨ 4 ä¸ª GPU ä¸Š

# === ç¬¬ 1 è½®ï¼šè®¡ç®—æœ¬åœ°çš„ KV ===
GPU 0: output0 = attention(Q0, K0, V0)  # éƒ¨åˆ†ç»“æœ
GPU 1: output1 = attention(Q1, K1, V1)
GPU 2: output2 = attention(Q2, K2, V2)
GPU 3: output3 = attention(Q3, K3, V3)

# === ç¬¬ 2 è½®ï¼šRing ä¼ é€’ KV ===
# æ¯ä¸ª GPU æŠŠ KV å‘ç»™ä¸‹ä¸€ä¸ª GPUï¼ˆç¯å½¢ï¼‰
GPU 0 æ¥æ”¶ K3, V3 (æ¥è‡ª GPU 3)
GPU 1 æ¥æ”¶ K0, V0 (æ¥è‡ª GPU 0)
GPU 2 æ¥æ”¶ K1, V1 (æ¥è‡ª GPU 1)
GPU 3 æ¥æ”¶ K2, V2 (æ¥è‡ª GPU 2)

# è®¡ç®—å¹¶ç´¯åŠ 
GPU 0: output0 += attention(Q0, K3, V3)
GPU 1: output1 += attention(Q1, K0, V0)
GPU 2: output2 += attention(Q2, K1, V1)
GPU 3: output3 += attention(Q3, K2, V2)

# === ç¬¬ 3 è½®ï¼šç»§ç»­ä¼ é€’ ===
GPU 0 æ¥æ”¶ K2, V2
GPU 1 æ¥æ”¶ K3, V3
GPU 2 æ¥æ”¶ K0, V0
GPU 3 æ¥æ”¶ K1, V1

GPU 0: output0 += attention(Q0, K2, V2)
GPU 1: output1 += attention(Q1, K3, V3)
GPU 2: output2 += attention(Q2, K0, V0)
GPU 3: output3 += attention(Q3, K1, V1)

# === ç¬¬ 4 è½®ï¼šæœ€åä¸€æ¬¡ä¼ é€’ ===
GPU 0 æ¥æ”¶ K1, V1
GPU 1 æ¥æ”¶ K2, V2
GPU 2 æ¥æ”¶ K3, V3
GPU 3 æ¥æ”¶ K0, V0

GPU 0: output0 += attention(Q0, K1, V1)
GPU 1: output1 += attention(Q1, K2, V2)
GPU 2: output2 += attention(Q2, K3, V3)
GPU 3: output3 += attention(Q3, K0, V0)

# å®Œæˆï¼æ¯ä¸ª GPU ç°åœ¨æœ‰å®Œæ•´çš„ attention è¾“å‡º
```

**å…³é”®ä¼˜åŒ–**ï¼š
- **é‡å è®¡ç®—å’Œé€šä¿¡**ï¼šåœ¨è®¡ç®— round N æ—¶ï¼ŒåŒæ—¶ä¼ é€’ round N+1 çš„ KV
- **å› æœæ©ç ä¼˜åŒ–**ï¼šå¯¹äºå› æœ attentionï¼Œä¸éœ€è¦ä¼ é€’æ‰€æœ‰ KVï¼ˆåªéœ€è¦å·¦è¾¹çš„ï¼‰

### 3.3 Softmax çš„æ•°å€¼ç¨³å®šæ€§

**æŒ‘æˆ˜**ï¼šSoftmax éœ€è¦çœ‹åˆ°æ‰€æœ‰ scores æ‰èƒ½å½’ä¸€åŒ–

```python
# ä¼ ç»Ÿ Softmax
scores = Q @ K.T  # éœ€è¦å®Œæ•´çš„ scores çŸ©é˜µ
max_score = max(scores)  # æ‰¾æœ€å¤§å€¼
exp_scores = exp(scores - max_score)  # æ•°å€¼ç¨³å®šçš„ exp
weights = exp_scores / sum(exp_scores)  # å½’ä¸€åŒ–
```

**Ring Attention çš„è§£å†³æ–¹æ¡ˆ**ï¼š**åœ¨çº¿æ›´æ–° Softmax**

```python
# åˆå§‹åŒ–
output = 0
sum_exp = 0
max_score = -inf

# é€å—å¤„ç† KV
for kv_chunk in [KV0, KV1, KV2, KV3]:
    # è®¡ç®—å½“å‰ chunk çš„ scores
    scores_chunk = Q @ kv_chunk.K.T

    # æ›´æ–°å…¨å±€æœ€å¤§å€¼
    new_max = max(max_score, max(scores_chunk))

    # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„ç»“æœ (å› ä¸ºæœ€å¤§å€¼å˜äº†)
    scale_factor = exp(max_score - new_max)
    output *= scale_factor
    sum_exp *= scale_factor

    # è®¡ç®—å½“å‰ chunk çš„è´¡çŒ®
    exp_scores_chunk = exp(scores_chunk - new_max)
    sum_exp += sum(exp_scores_chunk)
    output += exp_scores_chunk @ kv_chunk.V

    # æ›´æ–°æœ€å¤§å€¼
    max_score = new_max

# æœ€ç»ˆå½’ä¸€åŒ–
output /= sum_exp
```

**è¿™ä¸ªç®—æ³•å¾ˆå·§å¦™**ï¼š
- **å¢é‡æ›´æ–°**ï¼šä¸éœ€è¦ä¸€æ¬¡æ€§çœ‹åˆ°æ‰€æœ‰ scores
- **æ•°å€¼ç¨³å®š**ï¼šå§‹ç»ˆç”¨æœ€æ–°çš„ max_score ä¿è¯ç¨³å®šæ€§
- **æ”¯æŒæµå¼è®¡ç®—**ï¼šå¯ä»¥è¾¹æ¥æ”¶ KV è¾¹è®¡ç®—

### 3.4 å› æœæ©ç ä¼˜åŒ–

å¯¹äº **Causal Attention**ï¼ˆå¦‚ GPTï¼‰ï¼Œtoken åªèƒ½çœ‹åˆ°å®ƒ**å·¦è¾¹**çš„ tokensï¼š

```
Token 0: åªèƒ½çœ‹ Token 0
Token 1: åªèƒ½çœ‹ Token 0, 1
Token 2: åªèƒ½çœ‹ Token 0, 1, 2
...
Token 2047: å¯ä»¥çœ‹ Token 0 - 2047

å‡è®¾ CP = 4, æ¯ä¸ª GPU å¤„ç† 2048 tokens:

GPU 0 (Token 0-2047):
  ä¸éœ€è¦æ¥æ”¶å…¶ä»– GPU çš„ KV (å› ä¸ºå³è¾¹çš„ token éƒ½åœ¨æœªæ¥)

GPU 1 (Token 2048-4095):
  åªéœ€è¦æ¥æ”¶ GPU 0 çš„ KV (Token 0-2047)
  ä¸éœ€è¦ GPU 2, 3 çš„ KV

GPU 2 (Token 4096-6143):
  éœ€è¦æ¥æ”¶ GPU 0, 1 çš„ KV (Token 0-4095)
  ä¸éœ€è¦ GPU 3 çš„ KV

GPU 3 (Token 6144-8191):
  éœ€è¦æ¥æ”¶ GPU 0, 1, 2 çš„ KV (Token 0-6143)
```

**é€šä¿¡é‡ä¼˜åŒ–**ï¼š
```
æ²¡æœ‰å› æœæ©ç : æ¯ä¸ª GPU æ¥æ”¶ 3 æ¬¡ KV (ä¼ é€’ 3 è½®)
æœ‰å› æœæ©ç :
  GPU 0: 0 æ¬¡æ¥æ”¶
  GPU 1: 1 æ¬¡æ¥æ”¶
  GPU 2: 2 æ¬¡æ¥æ”¶
  GPU 3: 3 æ¬¡æ¥æ”¶
  å¹³å‡: 1.5 æ¬¡æ¥æ”¶ (å‡å°‘ 50% é€šä¿¡é‡ï¼)
```

---

## 4. æºç å®ç°è¯¦è§£

### 4.1 æ ¸å¿ƒ APIï¼šcontext_parallel

```python
# æ¥è‡ª: torchtitan/distributed/utils.py:198-220

def create_context_parallel_ctx(
    cp_mesh: DeviceMesh,              # CP çš„ device mesh
    cp_buffers: list[torch.Tensor],   # éœ€è¦åœ¨åºåˆ—ç»´åº¦åˆ‡åˆ†çš„ tensors
    cp_seq_dims: list[int],           # æ¯ä¸ª buffer çš„åºåˆ—ç»´åº¦ç´¢å¼•
    cp_no_restore_buffers: set[torch.Tensor],  # ä¸éœ€è¦æ¢å¤çš„ buffers
    cp_rotate_method: str,            # "allgather" æˆ– "alltoall"
):
    try:
        from torch.distributed.tensor.experimental import context_parallel
        from torch.distributed.tensor.experimental._attention import set_rotate_method
    except ImportError:
        print(
            f"PyTorch version {torch.__version__} does not include the experimental "
            "Context Parallel API. Please update to a newer version."
        )

    # è®¾ç½®è½®æ¢æ–¹æ³•
    set_rotate_method(cp_rotate_method)

    # è¿”å› context manager
    return context_parallel(
        cp_mesh,
        buffers=cp_buffers,
        buffer_seq_dims=cp_seq_dims,
        no_restore_buffers=cp_no_restore_buffers,
    )
```

**å‚æ•°è¯´æ˜**ï¼š

1. **cp_mesh**ï¼šContext Parallel çš„ device mesh
   - ä¾‹å¦‚ï¼š`world_mesh["cp"]` - CP ç»´åº¦çš„ mesh
   - å¦‚æœ CP = 4ï¼Œåˆ™åŒ…å« 4 ä¸ª GPU

2. **cp_buffers**ï¼šéœ€è¦åˆ‡åˆ†çš„ tensors
   ```python
   # æ¥è‡ª: torchtitan/train.py:478-482
   cp_buffers = [inputs, labels]
   # inputs: [batch, seq_len, hidden]  - è¾“å…¥åºåˆ—
   # labels: [batch, seq_len]          - æ ‡ç­¾åºåˆ—

   if hasattr(model_parts[0], "freqs_cis"):
       # freqs_cis: [max_seq_len, head_dim] - RoPE çš„é¢‘ç‡
       cp_buffers += [m.freqs_cis for m in model_parts]
   ```

3. **cp_seq_dims**ï¼šæ¯ä¸ª buffer çš„åºåˆ—ç»´åº¦
   ```python
   # æ¥è‡ª: torchtitan/train.py:479-482
   cp_seq_dims = [1, 1]  # inputs å’Œ labels çš„ seq ç»´åº¦éƒ½æ˜¯ dim=1

   if hasattr(model_parts[0], "freqs_cis"):
       # freqs_cis çš„ seq ç»´åº¦æ˜¯ dim=0
       cp_seq_dims += [0 for _ in model_parts]
   ```

4. **cp_no_restore_buffers**ï¼šä¸éœ€è¦æ¢å¤çš„ buffers
   ```python
   # æ¥è‡ª: torchtitan/train.py:489
   cp_no_restore_buffers = {inputs, labels}
   ```
   - è¾“å…¥å’Œæ ‡ç­¾ä¸éœ€è¦æ¢å¤ï¼ˆå› ä¸ºå®ƒä»¬å·²ç»è¢«åˆ‡åˆ†ï¼Œåç»­ä¸éœ€è¦å®Œæ•´çš„ï¼‰
   - freqs_cis éœ€è¦æ¢å¤ï¼ˆå› ä¸ºåç»­å±‚éœ€è¦å®Œæ•´çš„é¢‘ç‡ä¿¡æ¯ï¼‰

5. **cp_rotate_method**ï¼šKV è½®æ¢çš„é€šä¿¡æ–¹å¼
   - `"allgather"`ï¼šæ¯ä¸ª GPU All-Gather å…¶ä»– GPU çš„ KV
   - `"alltoall"`ï¼šAll-to-All äº¤æ¢ KV chunks

### 4.2 ä½¿ç”¨æ–¹å¼

```python
# æ¥è‡ª: torchtitan/train.py:484-494

# åˆ›å»º CP context
optional_context_parallel_ctx = (
    dist_utils.create_context_parallel_ctx(
        cp_mesh=parallel_dims.world_mesh["cp"],
        cp_buffers=[inputs, labels] + [m.freqs_cis for m in model_parts],
        cp_seq_dims=[1, 1] + [0 for _ in model_parts],
        cp_no_restore_buffers={inputs, labels},
        cp_rotate_method=job_config.parallelism.context_parallel_rotate_method,
    )
    if parallel_dims.cp_enabled
    else None
)

# åœ¨è®­ç»ƒä¸­ä½¿ç”¨
with self.train_context(optional_context_parallel_ctx):
    pred = model_parts[0](inputs, **extra_inputs, **extra_kwargs)
    loss = self.loss_fn(pred, labels)
    loss.backward()
```

**å·¥ä½œæµç¨‹**ï¼š

1. **è¿›å…¥ context**ï¼š
   ```python
   with context_parallel(...):
       # è‡ªåŠ¨åˆ‡åˆ† cp_buffers åœ¨åºåˆ—ç»´åº¦
       # inputs: [batch, seq_len, hidden] â†’ [batch, seq_len/CP, hidden]
       # labels: [batch, seq_len] â†’ [batch, seq_len/CP]
   ```

2. **Forward pass**ï¼š
   - æ¨¡å‹çš„ Attention å±‚ä¼šè‡ªåŠ¨ä½¿ç”¨ Ring Attention
   - æ¯ä¸ª GPU åªè®¡ç®— `seq_len / CP` ä¸ª query çš„ attention
   - é€šè¿‡ Ring æœºåˆ¶çœ‹åˆ°å®Œæ•´çš„ K, V

3. **é€€å‡º context**ï¼š
   ```python
   # è‡ªåŠ¨æ¢å¤ cp_buffers (é™¤äº† no_restore_buffers)
   # freqs_cis: [max_seq_len/CP, head_dim] â†’ [max_seq_len, head_dim]
   ```

### 4.3 Attention Wrapper ä¸ CP çš„é…åˆ

```python
# æ¥è‡ª: torchtitan/models/attention.py:86-127

class FlexAttentionWrapper(torch.nn.Module):
    """Wrapper around `flex_attention` to make it torch.compile and CP compatible.

    This wrapper serves two purposes:
    1) Invoke `torch.compile` with a valid mode "max-autotune-no-cudagraphs" to
       achieve good performance.
    2) Being a wrapper allows us to apply _ContextParallel to it.

    Note:
        The forward function must have q, k, v as the first three arguments, and
        block_mask as a keyword argument to be compatible with _ContextParallel.
    """

    def forward(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        *,
        block_mask: BlockMask,
        scale: float | None = None,
        return_lse: bool = False,
    ):
        return FlexAttentionWrapper._compiled_flex_attn(
            q, k, v,
            block_mask=block_mask,
            scale=scale,
            return_lse=return_lse,
        )
```

**ä¸ºä»€ä¹ˆéœ€è¦ Wrapperï¼Ÿ**

1. **CP éœ€è¦ nn.Module**ï¼š
   - `F.scaled_dot_product_attention` ä¸æ˜¯ nn.Module
   - CP çš„ Ring Attention éœ€è¦ hook åˆ° nn.Module çš„ forward

2. **å‚æ•°çº¦å®š**ï¼š
   - Forward çš„å‰ 3 ä¸ªå‚æ•°å¿…é¡»æ˜¯ `q, k, v`
   - CP çš„ Ring æœºåˆ¶ä¼šè‡ªåŠ¨ hook è¿™äº›å‚æ•°

3. **è‡ªåŠ¨åº”ç”¨ Ring Attention**ï¼š
   ```python
   # åœ¨ CP context å†…
   output = attention_wrapper(q, k, v, block_mask=mask)
   # CP è‡ªåŠ¨å°†å…¶è½¬æ¢ä¸º Ring Attention è®¡ç®—
   ```

### 4.4 Device Mesh çš„æ„å»º

```python
# æ¥è‡ª: torchtitan/distributed/parallel_dims.py:147-190

def _build_mesh_without_ep(self) -> DeviceMesh:
    dims = []
    names = []
    for d, name in zip(
        [self.pp, self.dp_replicate, self.dp_shard, self.cp, self.tp],
        ["pp", "dp_replicate", "dp_shard", "cp", "tp"],
    ):
        if d > 1:
            dims.append(d)
            names.append(name)

    # ä¾‹å¦‚: dims=[2, 2, 2], names=["dp_shard", "cp", "tp"]
    # è¡¨ç¤º 8 ä¸ª GPUï¼Œ2è·¯ FSDPï¼Œ2è·¯ CPï¼Œ2è·¯ TP
    mesh = init_device_mesh(device_type, dims, mesh_dim_names=names)

    # åˆ›å»ºç»„åˆ mesh
    dp_shard_cp_mesh_dim_names = []
    if self.dp_shard_enabled:
        dp_shard_cp_mesh_dim_names.append("dp_shard")
    if self.cp_enabled:
        dp_shard_cp_mesh_dim_names.append("cp")

    if dp_shard_cp_mesh_dim_names:
        mesh[tuple(dp_shard_cp_mesh_dim_names)]._flatten(
            mesh_dim_name="dp_shard_cp"
        )

    return mesh
```

**ä¸ºä»€ä¹ˆ CP å’Œ FSDP ç»„åˆï¼Ÿ**

```
å‡è®¾ 8 ä¸ª GPUï¼Œdp_shard = 2, cp = 2, tp = 2

mesh = [
    [                                      # dp_shard group 0
        [GPU0, GPU1],  # cp group 0, tp group
        [GPU2, GPU3],  # cp group 1, tp group
    ],
    [                                      # dp_shard group 1
        [GPU4, GPU5],  # cp group 0, tp group
        [GPU6, GPU7],  # cp group 1, tp group
    ]
]

FSDP åœ¨ dp_shard ç»´åº¦åšå‚æ•°åˆ‡åˆ†
CP åœ¨ cp ç»´åº¦åšåºåˆ—åˆ‡åˆ†
TP åœ¨ tp ç»´åº¦åšæ¨¡å‹åˆ‡åˆ†

ä¸‰è€…æ­£äº¤ï¼Œå¯ä»¥ç»„åˆä½¿ç”¨ï¼
```

### 4.5 é…ç½®é€‰é¡¹

```toml
# æ¥è‡ª: torchtitan/models/llama3/train_configs/llama3_8b.toml:43

[parallelism]
context_parallel_degree = 1  # CP å¹¶è¡Œåº¦ï¼Œ1 è¡¨ç¤ºç¦ç”¨

context_parallel_rotate_method = "allgather"  # "allgather" æˆ– "alltoall"
```

**ä¸¤ç§è½®æ¢æ–¹æ³•**ï¼š

1. **All-Gather**ï¼š
   ```
   æ¯ä¸ª GPU ä¾æ¬¡ All-Gather å…¶ä»– GPU çš„ KV

   Round 1: GPU 0 all-gather GPU 0's KV (no-op)
   Round 2: GPU 0 all-gather GPU 1's KV
   Round 3: GPU 0 all-gather GPU 2's KV
   Round 4: GPU 0 all-gather GPU 3's KV

   ä¼˜ç‚¹ï¼šå®ç°ç®€å•ï¼Œæ¯è½®åªæœ‰ä¸€ä¸ª GPU å‘é€
   ç¼ºç‚¹ï¼šé€šä¿¡é‡å¤§ (æ¯ä¸ª GPU éƒ½è¦æ¥æ”¶å®Œæ•´çš„ KV)
   ```

2. **All-to-All**ï¼š
   ```
   æ¯ä¸ª GPU åŒæ—¶å‘é€å’Œæ¥æ”¶ä¸åŒçš„ KV chunks

   ä¼˜ç‚¹ï¼šé€šä¿¡æ›´å‡è¡¡ï¼Œå¯ä»¥é‡å 
   ç¼ºç‚¹ï¼šå®ç°å¤æ‚ï¼Œéœ€è¦ç²¾ç¡®çš„é€šä¿¡è°ƒåº¦
   ```

---

## 5. æ€§èƒ½åˆ†æ

### 5.1 å†…å­˜èŠ‚çœ

**ä¼ ç»Ÿæ–¹å¼** (æ—  CP)ï¼š

```python
# Llama3 8B, seq_len = 32768
batch_size = 8
seq_len = 32768
n_heads = 32
head_dim = 128

# å•ä¸ª Attention å±‚çš„å†…å­˜
Q = [8, 32768, 32, 128]  = 1 GB
K = [8, 32768, 32, 128]  = 1 GB
V = [8, 32768, 32, 128]  = 1 GB

# Flash Attention çš„å·¥ä½œå†…å­˜ (ç®€åŒ–)
# è™½ç„¶ä¸ä¿å­˜å®Œæ•´çš„ attention matrixï¼Œä½†ä»éœ€è¦å¤§é‡ä¸´æ—¶å†…å­˜
Work_memory â‰ˆ 4 GB

Total â‰ˆ 7 GB per layer
```

**Context Parallel (CP = 4)**ï¼š

```python
# æ¯ä¸ª GPU å¤„ç† 8192 tokens
Q_local = [8, 8192, 32, 128]  = 256 MB
K_chunk = [8, 8192, 32, 128]  = 256 MB (è½®æ¢çš„)
V_chunk = [8, 8192, 32, 128]  = 256 MB (è½®æ¢çš„)

Work_memory â‰ˆ 1 GB

Total â‰ˆ 1.8 GB per layer per GPU
```

**èŠ‚çœæ¯”ä¾‹**ï¼š
```
7 GB / 1.8 GB = 3.9x å†…å­˜èŠ‚çœ
```

### 5.2 é€šä¿¡å¼€é”€

**CP çš„é€šä¿¡é‡**ï¼š

å‡è®¾ï¼š
- CP = 4
- seq_len = 32768
- hidden_dim = 4096
- dtype = bfloat16 (2 bytes)

**æ¯ä¸ª Transformer Layer çš„é€šä¿¡**ï¼š

```python
# KV cache å¤§å° (æ¯ä¸ª chunk)
kv_chunk_size = seq_len / CP * hidden_dim * 2 * 2
              = 32768 / 4 * 4096 * 2 * 2
              = 256 MB

# Ring Attention éœ€è¦ä¼ é€’ (CP - 1) è½®
# å› æœæ©ç ä¼˜åŒ–åï¼Œå¹³å‡ä¼ é€’ (CP - 1) / 2 è½®

# æ²¡æœ‰å› æœæ©ç ä¼˜åŒ–
total_comm = kv_chunk_size * (CP - 1)
           = 256 MB * 3
           = 768 MB per layer

# æœ‰å› æœæ©ç ä¼˜åŒ–
total_comm_causal = kv_chunk_size * (CP - 1) / 2
                  = 256 MB * 1.5
                  = 384 MB per layer

# Llama3 8B æœ‰ 32 layers
total_comm_per_fwd = 384 MB * 32
                   = 12 GB
```

**å¯¹æ¯” Tensor Parallel (TP)**ï¼š

```python
# TP = 4 çš„é€šä¿¡é‡ (æ¯å±‚ 2 æ¬¡ All-Reduce)
tp_comm_per_layer = 2 * batch_size * seq_len * hidden_dim * 2
                  = 2 * 8 * 32768 * 4096 * 2
                  = 4 GB per layer

tp_comm_total = 4 GB * 32 = 128 GB

CP é€šä¿¡é‡ (12 GB) << TP é€šä¿¡é‡ (128 GB)
```

**ä¸ºä»€ä¹ˆ CP é€šä¿¡é‡æ›´å°‘ï¼Ÿ**
- CP åªä¼ é€’ KV cacheï¼Œä¸ä¼ é€’å®Œæ•´çš„æ¿€æ´»
- TP éœ€è¦åœ¨æ¯ä¸ªçº¿æ€§å±‚ååš All-Reduce
- CP çš„é€šä¿¡åªåœ¨ Attention å±‚

### 5.3 è®¡ç®—æ•ˆç‡

**CP ä¸æ”¹å˜è®¡ç®—é‡**ï¼š

```
ä¼ ç»Ÿ Attention: O(seq_lenÂ² * hidden_dim)
Ring Attention:  O(seq_lenÂ² * hidden_dim)

è®¡ç®—é‡ç›¸åŒï¼åªæ˜¯åˆ†æ•£åˆ°å¤šä¸ª GPU
```

**ä½†æœ‰é¢å¤–å¼€é”€**ï¼š

1. **é€šä¿¡å»¶è¿Ÿ**ï¼š
   ```
   éœ€è¦ä¼ é€’ (CP - 1) è½® KV
   æ¯è½®å»¶è¿Ÿ â‰ˆ 256 MB / bandwidth
   ```

2. **åŒæ­¥å¼€é”€**ï¼š
   ```
   Ring çš„æ¯ä¸€è½®éœ€è¦åŒæ­¥
   slow GPU ä¼šæ‹–æ…¢æ•´ä¸ª ring
   ```

### 5.4 æ‰©å±•æ€§åˆ†æ

**ç†æƒ³åŠ é€Ÿæ¯”**ï¼š

```
CP = 2: 2x åºåˆ—é•¿åº¦ï¼Œé€šä¿¡å¼€é”€ ~10%  â†’ å®é™… 1.8x
CP = 4: 4x åºåˆ—é•¿åº¦ï¼Œé€šä¿¡å¼€é”€ ~20%  â†’ å®é™… 3.2x
CP = 8: 8x åºåˆ—é•¿åº¦ï¼Œé€šä¿¡å¼€é”€ ~35%  â†’ å®é™… 5.2x
```

**å½±å“å› ç´ **ï¼š

1. **ç½‘ç»œå¸¦å®½**ï¼š
   - NVLink (900 GB/s): é€šä¿¡å¼€é”€å°ï¼Œæ‰©å±•æ€§å¥½
   - InfiniBand (200 GB/s): é€šä¿¡å¼€é”€ä¸­ç­‰
   - PCIe (64 GB/s): é€šä¿¡å¼€é”€å¤§ï¼Œæ‰©å±•æ€§å·®

2. **åºåˆ—é•¿åº¦**ï¼š
   - seq_len = 8K: è®¡ç®—æ—¶é—´çŸ­ï¼Œé€šä¿¡å æ¯”é«˜
   - seq_len = 32K: è®¡ç®—æ—¶é—´é•¿ï¼Œé€šä¿¡å æ¯”ä½
   - seq_len = 128K: è®¡ç®—ä¸»å¯¼ï¼Œé€šä¿¡å¼€é”€å¯å¿½ç•¥

3. **CP å¹¶è¡Œåº¦**ï¼š
   - CP = 2: é€šä¿¡ 1 è½®ï¼Œå¼€é”€æœ€å°
   - CP = 4: é€šä¿¡ 3 è½®ï¼Œå¼€é”€é€‚ä¸­
   - CP = 8: é€šä¿¡ 7 è½®ï¼Œå¼€é”€è¾ƒå¤§

**æœ€ä½³å®è·µ**ï¼š

```
çŸ­åºåˆ— (< 8K):    ä¸å»ºè®®ç”¨ CP
ä¸­ç­‰åºåˆ— (8K-32K):  CP = 2 æˆ– 4
é•¿åºåˆ— (32K-128K):  CP = 4 æˆ– 8
è¶…é•¿åºåˆ— (> 128K):  CP = 8 æˆ– 16
```

---

## 6. ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ

### 6.1 ä½•æ—¶åº”è¯¥ä½¿ç”¨ Context Parallelï¼Ÿ

**æ¨èä½¿ç”¨çš„åœºæ™¯**ï¼š

âœ… **è¶…é•¿åºåˆ—è®­ç»ƒ (> 8K tokens)**
   - Llama3 with seq_len = 32K
   - é•¿æ–‡æ¡£ç†è§£
   - ä»£ç ç”Ÿæˆï¼ˆé•¿ä¸Šä¸‹æ–‡ï¼‰

âœ… **å†…å­˜å—é™**
   - å• GPU æ”¾ä¸ä¸‹é•¿åºåˆ—çš„ Attention
   - å³ä½¿ç”¨äº† Flash Attention ä»ç„¶ OOM

âœ… **ä¸ FSDP ç»„åˆ**
   - CP å¤„ç†åºåˆ—ï¼ŒFSDP å¤„ç†å‚æ•°
   - ä¸¤è€…æ­£äº¤ï¼Œå¯ä»¥å®Œç¾ç»„åˆ

âœ… **æœ‰é«˜é€Ÿäº’è”**
   - NVLink: 900 GB/s (H100)
   - InfiniBand: 200 GB/s
   - Ring éœ€è¦é¢‘ç¹é€šä¿¡

**ä¸æ¨èä½¿ç”¨çš„åœºæ™¯**ï¼š

âŒ **çŸ­åºåˆ— (< 8K)**
   - å†…å­˜è¶³å¤Ÿï¼Œä¸éœ€è¦ CP
   - é€šä¿¡å¼€é”€å¾—ä¸å¿å¤±

âŒ **åªæœ‰ PCIe è¿æ¥**
   - å¸¦å®½ä½ (64 GB/s)
   - é€šä¿¡æˆä¸ºç“¶é¢ˆ

âŒ **éå› æœ Attention**
   - æ— æ³•ä½¿ç”¨å› æœæ©ç ä¼˜åŒ–
   - é€šä¿¡é‡ç¿»å€

âŒ **æ¨ç†åœºæ™¯**
   - æ¨ç†é€šå¸¸ batch_size = 1ï¼Œä¸éœ€è¦å¹¶è¡Œ
   - KV cache å·²ç»æ˜¯å¢é‡çš„

### 6.2 é…ç½®æ–¹æ³•

**TOML é…ç½®**ï¼š

```toml
[training]
seq_len = 32768  # é•¿åºåˆ—

[parallelism]
data_parallel_shard_degree = 8   # FSDP
context_parallel_degree = 4      # CP = 4
tensor_parallel_degree = 1       # å¯é€‰ï¼Œé€šå¸¸ CP ä¸ TP ä¸åŒæ—¶ç”¨

context_parallel_rotate_method = "allgather"  # æˆ– "alltoall"
```

**åºåˆ—é•¿åº¦è¦æ±‚**ï¼š

```python
# æ¥è‡ª: torchtitan/distributed/parallel_dims.py:252-259

@property
def seq_len_divisor(self):
    # Sequence Parallel requires that seq_len be divisible by TP degree.
    # Context Parallel requires that seq_len be divisible by 2 * CP degree
    return self.tp * (self.cp * 2)
```

**é…ç½®ç¤ºä¾‹**ï¼š

```toml
# åœºæ™¯1: ä¸­ç­‰åºåˆ— + FSDP + CP
[training]
seq_len = 16384  # 16K tokens

[parallelism]
data_parallel_shard_degree = 8
context_parallel_degree = 2
tensor_parallel_degree = 1

# seq_len å¿…é¡»èƒ½è¢« 2 * CP = 4 æ•´é™¤ âœ“ (16384 % 4 = 0)
```

```toml
# åœºæ™¯2: é•¿åºåˆ— + FSDP + CP + TP
[training]
seq_len = 32768  # 32K tokens

[parallelism]
data_parallel_shard_degree = 4
context_parallel_degree = 4
tensor_parallel_degree = 2

# seq_len å¿…é¡»èƒ½è¢« TP * 2 * CP = 16 æ•´é™¤ âœ“ (32768 % 16 = 0)
```

### 6.3 ä¸å…¶ä»–å¹¶è¡Œçš„ç»„åˆ

**æ¨èç»„åˆ**ï¼š

| åœºæ™¯ | FSDP | CP | TP | PP | è¯´æ˜ |
|------|------|----|----|----|----|
| **çŸ­åºåˆ—å°æ¨¡å‹** | âœ“ | âœ— | âœ— | âœ— | åªç”¨ FSDP |
| **é•¿åºåˆ—å°æ¨¡å‹** | âœ“ | âœ“ | âœ— | âœ— | FSDP + CP |
| **çŸ­åºåˆ—å¤§æ¨¡å‹** | âœ“ | âœ— | âœ“ | âœ— | FSDP + TP |
| **é•¿åºåˆ—å¤§æ¨¡å‹** | âœ“ | âœ“ | âœ“ | âœ— | FSDP + CP + TP |
| **è¶…å¤§æ¨¡å‹** | âœ“ | âœ“ | âœ“ | âœ“ | 4D å¹¶è¡Œ |

**é…ç½®ç¤ºä¾‹ï¼ˆLlama3 70B + 32K contextï¼‰**ï¼š

```toml
[model]
name = "llama3"
flavor = "70B"

[training]
seq_len = 32768
local_batch_size = 1

[parallelism]
# 256 GPUs = 32 FSDP Ã— 4 CP Ã— 2 TP
data_parallel_shard_degree = 32
context_parallel_degree = 4
tensor_parallel_degree = 2
pipeline_parallel_degree = 1

context_parallel_rotate_method = "allgather"
```

**ä¸ºä»€ä¹ˆè¿™æ ·ç»„åˆï¼Ÿ**
- **FSDP (32è·¯)**ï¼šå¤„ç† 70B å‚æ•°ï¼Œæ¯ä¸ª GPU çº¦ 2.2B å‚æ•°
- **CP (4è·¯)**ï¼šå¤„ç† 32K åºåˆ—ï¼Œæ¯ä¸ª GPU çº¦ 8K tokens
- **TP (2è·¯)**ï¼šå‡å°‘å•å±‚å†…å­˜ï¼ŒåŠ é€Ÿé€šä¿¡

### 6.4 è°ƒè¯•å’ŒéªŒè¯

**å¦‚ä½•éªŒè¯ CP æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ**

1. **æ£€æŸ¥å†…å­˜ä½¿ç”¨**ï¼š
   ```bash
   # ä¸å¯ç”¨ CP
   context_parallel_degree = 1
   # è§‚å¯Ÿ nvidia-smiï¼Œè®°å½•å³°å€¼å†…å­˜

   # å¯ç”¨ CP
   context_parallel_degree = 4
   # å³°å€¼å†…å­˜åº”è¯¥é™ä½çº¦ 3-4x
   ```

2. **æ£€æŸ¥åºåˆ—åˆ‡åˆ†**ï¼š
   ```python
   # åœ¨æ¨¡å‹ forward ä¸­æ‰“å°è¾“å…¥å½¢çŠ¶
   print(f"Input shape: {inputs.shape}")

   # ä¸å¯ç”¨ CP
   # Input shape: [batch, 32768, hidden]

   # å¯ç”¨ CP = 4
   # Input shape: [batch, 8192, hidden]  â† åºåˆ—è¢«åˆ‡åˆ†äº†
   ```

3. **Profiling é€šä¿¡**ï¼š
   ```python
   from torch.profiler import profile, ProfilerActivity

   with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
       model(input)

   prof.export_chrome_trace("trace.json")
   # åœ¨ chrome://tracing ä¸­æŸ¥çœ‹ all_gather çš„é¢‘ç‡å’Œè€—æ—¶
   ```

**å¸¸è§é—®é¢˜**ï¼š

â“ **å¯ç”¨ CP åè®­ç»ƒå˜æ…¢ï¼Ÿ**
- æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦è¶³å¤Ÿé•¿ (éœ€è¦ > 8K)
- æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦ä¸º NVLink/IB
- æ£€æŸ¥ CP å¹¶è¡Œåº¦æ˜¯å¦å¤ªé«˜ (CP > 8 é€šå¸¸ä¸æ¨è)

â“ **Loss ä¸æ”¶æ•›ï¼Ÿ**
- CP åº”è¯¥æ˜¯æ•°å€¼ç­‰ä»·çš„ï¼Œloss åº”è¯¥å’Œä¸ç”¨ CP ä¸€è‡´
- æ£€æŸ¥æ˜¯å¦æ­£ç¡®è®¾ç½®äº† `cp_no_restore_buffers`
- æ£€æŸ¥ batch_size å’Œ learning rate æ˜¯å¦éœ€è¦è°ƒæ•´

â“ **OOM é”™è¯¯ï¼Ÿ**
- CP é™ä½äº† Attention çš„å†…å­˜ï¼Œä½†æ²¡æœ‰é™ä½æ¨¡å‹å‚æ•°å†…å­˜
- éœ€è¦é…åˆ FSDP ä½¿ç”¨
- æ£€æŸ¥ `cp_buffers` æ˜¯å¦åŒ…å«äº†æ‰€æœ‰éœ€è¦åˆ‡åˆ†çš„ tensor

### 6.5 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**1. é€‰æ‹©åˆé€‚çš„ rotate_method**ï¼š

```toml
# All-Gather (é»˜è®¤)
context_parallel_rotate_method = "allgather"
# ä¼˜ç‚¹ï¼šå®ç°ç®€å•ç¨³å®š
# é€‚ç”¨ï¼šCP <= 4, ç½‘ç»œå¸¦å®½å……è¶³

# All-to-All
context_parallel_rotate_method = "alltoall"
# ä¼˜ç‚¹ï¼šé€šä¿¡æ›´å‡è¡¡
# é€‚ç”¨ï¼šCP > 4, éœ€è¦æ›´å¥½çš„æ‰©å±•æ€§
```

**2. é…åˆ Flash Attention**ï¼š

```python
# CP ä¸ Flash Attention æ˜¯æ­£äº¤çš„
# Flash Attention å‡å°‘å†…å­˜ï¼ŒCP åˆ‡åˆ†åºåˆ—
# ä¸¤è€…ç»“åˆæ•ˆæœæœ€å¥½

# ä½¿ç”¨ flex_attention (è‡ªåŠ¨å¯ç”¨ Flash Attention)
[model.llama3]
attn_type = "flex"  # æˆ– "sdpa" (ä¹Ÿä¼šç”¨ Flash Attention)
```

**3. ä¼˜åŒ– CP å¹¶è¡Œåº¦**ï¼š

```python
# ç»éªŒå…¬å¼
optimal_CP = ceil(seq_len / 8192)

# ç¤ºä¾‹
seq_len = 16384  â†’ CP = 2
seq_len = 32768  â†’ CP = 4
seq_len = 65536  â†’ CP = 8
seq_len = 131072 â†’ CP = 16
```

**4. å¹³è¡¡ CP å’Œ TP**ï¼š

```
æ€» GPU æ•°é‡å›ºå®šæ—¶ï¼Œéœ€è¦åœ¨ CP å’Œ TP ä¹‹é—´æƒè¡¡

32 GPUs, é€‰æ‹©ï¼š
- CP = 1, TP = 8, FSDP = 4  â†’ é€‚åˆçŸ­åºåˆ—å¤§æ¨¡å‹
- CP = 4, TP = 2, FSDP = 4  â†’ é€‚åˆé•¿åºåˆ—å¤§æ¨¡å‹
- CP = 8, TP = 1, FSDP = 4  â†’ é€‚åˆè¶…é•¿åºåˆ—

é€šä¿¡é‡å¯¹æ¯”ï¼š
CP = 8: 12 GB per forward (ä¸»è¦åœ¨ Attention)
TP = 8: 128 GB per forward (éå¸ƒæ‰€æœ‰å±‚)

ä¸€èˆ¬æ¥è¯´ï¼šCP çš„é€šä¿¡æ•ˆç‡æ›´é«˜
```

---

## 7. æ€»ç»“

### 7.1 æ ¸å¿ƒè¦ç‚¹

ç”¨**æ¥åŠ›èµ›**æ€»ç»“ Context Parallelï¼š

```
ä¼ ç»Ÿ Attention = ä¸€ä¸ªäººçœ‹å®Œæ•´æœ¬ä¹¦
    å†…å­˜çˆ†ç‚¸ï¼ˆè¦è®°ä½æ•´æœ¬ä¹¦ï¼‰

Context Parallel = 4 ä¸ªäººæ¥åŠ›è¯»ä¹¦
    äºº1è¯» ç¬¬1ç« ï¼Œä¼ ç»™äºº2
    äºº2è¯» ç¬¬2ç« ï¼Œä¼ ç»™äºº3
    ...
    æ¯ä¸ªäººåªéœ€è¦è®°ä½ 1/4 çš„å†…å®¹
    ä½†é€šè¿‡æ¥åŠ›ï¼Œæ¯ä¸ªäººæœ€ç»ˆç†è§£äº†æ•´æœ¬ä¹¦
```

**ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯**ï¼š

1. **åºåˆ—åˆ‡åˆ†**ï¼šæŠŠè¾“å…¥åºåˆ—åˆ‡æˆå¤šå—ï¼Œæ¯ä¸ª GPU å¤„ç†ä¸€å—
2. **Ring Attention**ï¼šé€šè¿‡ç¯å½¢ä¼ é€’ KVï¼Œè®©æ¯ä¸ª GPU çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡
3. **åœ¨çº¿ Softmax**ï¼šå¢é‡æ›´æ–° Softmaxï¼Œæ”¯æŒæµå¼è®¡ç®—

### 7.2 æ€§èƒ½ç‰¹ç‚¹

**å†…å­˜èŠ‚çœ**ï¼š
- CP = 4: **3-4x** å†…å­˜èŠ‚çœ
- CP = 8: **6-8x** å†…å­˜èŠ‚çœ
- å¯ä»¥è®­ç»ƒ**æ›´é•¿çš„åºåˆ—**ï¼ˆ32K â†’ 128K â†’ 1Mï¼‰

**é€šä¿¡å¼€é”€**ï¼š
- CP çš„é€šä¿¡é‡ **è¿œå°äº** TP
- å› æœæ©ç ä¼˜åŒ–å¯ä»¥å‡å°‘ **50%** é€šä¿¡
- éœ€è¦é«˜é€Ÿäº’è”ï¼ˆNVLink / InfiniBandï¼‰

**è®¡ç®—æ•ˆç‡**ï¼š
- **ä¸å¢åŠ è®¡ç®—é‡**
- æœ‰é€šä¿¡å»¶è¿Ÿï¼ˆ10-35%ï¼‰
- é•¿åºåˆ—æ—¶é€šä¿¡å æ¯”å°ï¼Œæ•ˆç‡é«˜

### 7.3 ä½¿ç”¨å»ºè®®

**æ¨èä½¿ç”¨**ï¼š
- âœ… é•¿åºåˆ—è®­ç»ƒ (> 8K tokens)
- âœ… å†…å­˜å—é™åœºæ™¯
- âœ… é…åˆ FSDP ä½¿ç”¨
- âœ… æœ‰ NVLink äº’è”

**ä¸æ¨èä½¿ç”¨**ï¼š
- âŒ çŸ­åºåˆ— (< 8K tokens)
- âŒ PCIe è¿æ¥
- âŒ æ¨ç†åœºæ™¯
- âŒ éå› æœ Attention

**é…ç½®è¦ç‚¹**ï¼š
```toml
[training]
seq_len = 32768  # å¿…é¡»èƒ½è¢« 2 * CP æ•´é™¤

[parallelism]
context_parallel_degree = 4
context_parallel_rotate_method = "allgather"

# æ¨èç»„åˆ
data_parallel_shard_degree = 8  # FSDP
tensor_parallel_degree = 1      # TPï¼ˆå¯é€‰ï¼‰
```

### 7.4 ä¸å…¶ä»–å¹¶è¡Œçš„å¯¹æ¯”

| ç‰¹æ€§ | Data Parallel | Tensor Parallel | Context Parallel |
|------|--------------|-----------------|------------------|
| **åˆ‡åˆ†å¯¹è±¡** | æ•°æ® | æ¨¡å‹ | åºåˆ— |
| **å†…å­˜èŠ‚çœ** | å‚æ•° | å‚æ•° + æ¿€æ´» | æ¿€æ´» (Attention) |
| **é€šä¿¡é‡** | ä¸­ | å¤§ | å° |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨ | å¤§æ¨¡å‹ | é•¿åºåˆ— |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ | å¤æ‚ |
| **æ•°å€¼ç­‰ä»·æ€§** | âœ“ | âœ“ | âœ“ |

### 7.5 æœªæ¥å‘å±•æ–¹å‘

**å¯èƒ½çš„æ”¹è¿›**ï¼š

1. **æ›´é«˜æ•ˆçš„ Ring ç®—æ³•**ï¼š
   - å½“å‰ï¼šé¡ºåºä¼ é€’ KV
   - æœªæ¥ï¼šå¹¶è¡Œä¼ é€’å¤šä¸ª KV chunks

2. **è‡ªé€‚åº” CP**ï¼š
   - å½“å‰ï¼šå›ºå®šçš„ CP å¹¶è¡Œåº¦
   - æœªæ¥ï¼šæ ¹æ®åºåˆ—é•¿åº¦è‡ªåŠ¨è°ƒæ•´

3. **ä¸ KV cache ä¼˜åŒ–ç»“åˆ**ï¼š
   - å½“å‰ï¼šå®Œæ•´ä¼ é€’ KV
   - æœªæ¥ï¼šåªä¼ é€’å¢é‡ KV (é€‚åˆæ¨ç†)

4. **æ”¯æŒæ›´å¤š Attention å˜ä½“**ï¼š
   - å½“å‰ï¼šä¸»è¦æ”¯æŒ Causal Attention
   - æœªæ¥ï¼šSliding Window, Sparse Attention ç­‰

---

## 8. å‚è€ƒèµ„æ–™

**æºç æ–‡ä»¶**ï¼š
- `torchtitan/distributed/utils.py:198-220` - CP context åˆ›å»º
- `torchtitan/train.py:478-494` - CP çš„ä½¿ç”¨
- `torchtitan/models/attention.py` - Attention Wrapper
- `torchtitan/distributed/parallel_dims.py` - å¹¶è¡Œç»´åº¦ç®¡ç†

**PyTorch å®˜æ–¹èµ„æº**ï¼š
- [Experimental Context Parallel API](https://pytorch.org/docs/stable/distributed.tensor.experimental.html)
- [Ring Attention Implementation](https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/experimental/_attention.py)

**ç›¸å…³è®ºæ–‡**ï¼š
- Ring Attention with Blockwise Transformers for Near-Infinite Context (Liu et al., 2023)
- Blockwise Parallel Transformer for Large Context Models (Liu et al., 2023)
- FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Dao et al., 2022)

**ç›¸å…³æ–‡æ¡£**ï¼š
- `docs/analysis/02_tensor_parallel_implementation.md` - Tensor Parallel è¯¦è§£
- `docs/analysis/03_async_tensor_parallel.md` - Async TP è¯¦è§£
- `docs/converging.md` - æ”¶æ•›æ€§éªŒè¯ï¼ˆåŒ…å« CP æµ‹è¯•ï¼‰

---

## é™„å½•ï¼šé«˜çº§è¯é¢˜

### A.1 Ring Attention çš„æ•°å­¦æ¨å¯¼

**é—®é¢˜**ï¼šå¦‚ä½•åœ¨ä¸ä¿å­˜å®Œæ•´ attention matrix çš„æƒ…å†µä¸‹è®¡ç®— Softmaxï¼Ÿ

**å…³é”®æ´å¯Ÿ**ï¼šSoftmax å¯ä»¥å¢é‡æ›´æ–°

```python
# ä¼ ç»Ÿ Softmax
scores = [s1, s2, s3, s4]  # æ‰€æœ‰ scores
max_s = max(scores)
exp_scores = exp(scores - max_s)
softmax = exp_scores / sum(exp_scores)

# å¢é‡ Softmax
# Step 1: åªæœ‰ s1
max_s = s1
exp_s1 = exp(s1 - max_s) = 1
sum_exp = 1
result = exp_s1 / sum_exp = 1

# Step 2: åŠ å…¥ s2
max_s_new = max(max_s, s2)
# é‡æ–°ç¼©æ”¾ä¹‹å‰çš„ç»“æœ
exp_s1 *= exp(max_s - max_s_new)
sum_exp *= exp(max_s - max_s_new)
# åŠ å…¥æ–°çš„ score
exp_s2 = exp(s2 - max_s_new)
sum_exp += exp_s2
# æ›´æ–°
max_s = max_s_new

# Step 3, 4: ç±»ä¼¼...
```

**åº”ç”¨åˆ° Attention**ï¼š

```python
def ring_attention(Q, K_chunks, V_chunks):
    output = 0
    sum_exp = 0
    max_score = -inf

    for K_chunk, V_chunk in zip(K_chunks, V_chunks):
        # è®¡ç®—å½“å‰ chunk çš„ scores
        scores = Q @ K_chunk.T  # [batch, q_len, k_len]

        # æ›´æ–°å…¨å±€æœ€å¤§å€¼
        chunk_max = scores.max(dim=-1, keepdim=True)
        new_max = torch.maximum(max_score, chunk_max)

        # é‡æ–°ç¼©æ”¾
        exp_old_max = torch.exp(max_score - new_max)
        exp_new_scores = torch.exp(scores - new_max)

        # æ›´æ–°ç´¯åŠ å™¨
        output = output * exp_old_max + exp_new_scores @ V_chunk
        sum_exp = sum_exp * exp_old_max + exp_new_scores.sum(dim=-1, keepdim=True)

        max_score = new_max

    # æœ€ç»ˆå½’ä¸€åŒ–
    output = output / sum_exp
    return output
```

### A.2 Load Balancing é€‰é¡¹

```python
# æ¥è‡ª: torchtitan/models/flux/infra/parallelize.py:54-56

from torch.distributed.tensor.experimental._attention import _cp_options

_cp_options.enable_load_balance = False
```

**ä»€ä¹ˆæ˜¯ Load Balancingï¼Ÿ**

- åœ¨éå› æœ Attention ä¸­ï¼Œæ¯ä¸ª GPU è®¡ç®—çš„ attention çŸ©é˜µå¤§å°ä¸åŒ
- Load Balancing å°è¯•å¹³è¡¡å„ä¸ª GPU çš„è®¡ç®—é‡
- ä½†ä¼šå¢åŠ é€šä¿¡å¤æ‚åº¦

**ä½•æ—¶ç¦ç”¨ï¼Ÿ**
- å› æœ Attentionï¼šå·²ç»è‡ªç„¶å¹³è¡¡ï¼ˆåé¢çš„ token çœ‹æ›´å¤šï¼‰
- Flux æ¨¡å‹ï¼šä¸ä½¿ç”¨å› æœæ©ç ï¼Œä½†ä»ç¦ç”¨ä»¥ç®€åŒ–å®ç°

### A.3 CP ä¸ Variable Length Attention

Context Parallel ä¹Ÿå¯ä»¥ä¸ Variable Length Attention ç»“åˆï¼š

```python
# ä¸åŒæ–‡æ¡£æœ‰ä¸åŒé•¿åº¦
batch = [
    "æ–‡æ¡£1: 1000 tokens",
    "æ–‡æ¡£2: 5000 tokens",
    "æ–‡æ¡£3: 200 tokens",
    "æ–‡æ¡£4: 3000 tokens",
]

# Padding åˆ°æœ€å¤§é•¿åº¦ 5000
padded_batch = pad(batch, max_len=5000)

# ä½¿ç”¨ CP = 4 åˆ‡åˆ†
# æ¯ä¸ª GPU å¤„ç† 1250 tokens
# ä½†å¤§éƒ¨åˆ†æ˜¯ paddingï¼

# ä¼˜åŒ–ï¼šVariable Length Attention
# åŠ¨æ€å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œä¸æµªè´¹è®¡ç®—åœ¨ padding ä¸Š
```

è¿™æ˜¯æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚
