# Activation Checkpointing æ¿€æ´»æ£€æŸ¥ç‚¹è¯¦è§£

## ç›®å½•
- [1. ä»€ä¹ˆæ˜¯ Activation Checkpointingï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-activation-checkpointing)
- [2. æ¬æ¡Œå­çš„æ¯”å–»ï¼šè‰ç¨¿çº¸ç­–ç•¥](#2-æ¬æ¡Œå­çš„æ¯”å–»è‰ç¨¿çº¸ç­–ç•¥)
- [3. ä¸‰ç§ AC æ¨¡å¼å¯¹æ¯”](#3-ä¸‰ç§-ac-æ¨¡å¼å¯¹æ¯”)
- [4. Full AC å®ç°](#4-full-ac-å®ç°)
- [5. Selective AC - Layer å±‚çº§](#5-selective-ac---layer-å±‚çº§)
- [6. Selective AC - Operator ç®—å­çº§](#6-selective-ac---operator-ç®—å­çº§)
- [7. æºç å®ç°è¯¦è§£](#7-æºç å®ç°è¯¦è§£)
- [8. ä¸ torch.compile çš„äº¤äº’](#8-ä¸-torchcompile-çš„äº¤äº’)
- [9. Memory Budget æ¨¡å¼](#9-memory-budget-æ¨¡å¼)

---

## 1. ä»€ä¹ˆæ˜¯ Activation Checkpointingï¼Ÿ

### 1.1 åŸºæœ¬æ¦‚å¿µ

**Activation Checkpointing (AC)** = åœ¨åå‘ä¼ æ’­æ—¶ï¼Œé‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼Œè€Œä¸æ˜¯åœ¨å‰å‘ä¼ æ’­æ—¶å…¨éƒ¨ä¿å­˜ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨**è®¡ç®—æ¢å†…å­˜** - èˆå¼ƒéƒ¨åˆ†æ¿€æ´»å€¼ï¼Œéœ€è¦æ—¶é‡æ–°è®¡ç®—ã€‚

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ ACï¼Ÿ

è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå†…å­˜å ç”¨ä¸»è¦æ¥è‡ªä¸‰éƒ¨åˆ†ï¼š

```
GPU å†…å­˜å ç”¨ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ¨¡å‹å‚æ•° (Parameters)       â”‚  20%
â”‚    - weights, biases           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. ä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer)      â”‚  40%
â”‚    - momentum, variance        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. æ¿€æ´»å€¼ (Activations) ğŸ’¥     â”‚  40%
â”‚    - ä¸­é—´è®¡ç®—ç»“æœ              â”‚
â”‚    - éœ€è¦ç”¨äºåå‘ä¼ æ’­          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é—®é¢˜ï¼šæ¿€æ´»å€¼å ç”¨å¤§é‡å†…å­˜ï¼
```

**å…·ä½“ä¾‹å­** - Llama3 8B è®­ç»ƒï¼š

```
é…ç½®ï¼š
- Batch size = 2
- Sequence length = 8192
- Hidden dim = 4096
- 32 layers

æ¿€æ´»å€¼å†…å­˜å ç”¨ï¼š
æ¯å±‚çš„æ¿€æ´»å€¼ â‰ˆ batch Ã— seq_len Ã— hidden_dim Ã— sizeof(dtype)
             â‰ˆ 2 Ã— 8192 Ã— 4096 Ã— 2 bytes
             â‰ˆ 128 MB

æ€»æ¿€æ´»å€¼ â‰ˆ 128 MB Ã— 32 layers = 4 GB

å¦‚æœæ²¡æœ‰ ACï¼š
  éœ€è¦ä¿å­˜æ‰€æœ‰å±‚çš„æ¿€æ´»å€¼ â†’ 4 GB
  å¯ç”¨äºå¢å¤§ batch size æˆ– seq_len å—é™

ä½¿ç”¨ ACï¼š
  åªä¿å­˜å°‘é‡æ¿€æ´»å€¼ â†’ å¯èƒ½é™åˆ° 1 GB
  å¯ç”¨å†…å­˜å¢åŠ  â†’ batch size å¯ä»¥å¢å¤§ 2-4x
```

### 1.3 AC çš„æƒè¡¡

```
ä¸ä½¿ç”¨ AC:
  ä¼˜ç‚¹: âœ… å¿«ï¼ˆä¸éœ€è¦é‡æ–°è®¡ç®—ï¼‰
  ç¼ºç‚¹: âŒ å†…å­˜å ç”¨é«˜

ä½¿ç”¨ AC:
  ä¼˜ç‚¹: âœ… å†…å­˜å ç”¨ä½ï¼ˆå¯ä»¥è®­ç»ƒæ›´å¤§æ¨¡å‹/batchï¼‰
  ç¼ºç‚¹: âŒ æ…¢ï¼ˆéœ€è¦é‡æ–°è®¡ç®—æ¿€æ´»å€¼ï¼‰

Time-Memory Tradeoff:
  Full AC:      æœ€çœå†…å­˜ï¼Œæœ€æ…¢ï¼ˆ~20% æ…¢ï¼‰
  Selective AC: å¹³è¡¡ï¼ˆ~10% æ…¢ï¼‰
  No AC:        æœ€å¿«ï¼Œæœ€è€—å†…å­˜
```

---

## 2. æ¬æ¡Œå­çš„æ¯”å–»ï¼šè‰ç¨¿çº¸ç­–ç•¥

### 2.1 å›é¡¾æ¬æ¡Œå­çš„åœºæ™¯

è¿˜è®°å¾—æˆ‘ä»¬ç”¨æ¬æ¡Œå­æ¯”å–»è®­ç»ƒè¿‡ç¨‹å—ï¼Ÿï¼ˆ[FSDP æ–‡æ¡£](./01_fsdp2_per_parameter_sharding.md)ï¼‰

```
Forward Pass (æ­£å‘æ¬æ¡Œå­):
æˆ¿é—´ A â†’ æˆ¿é—´ B â†’ æˆ¿é—´ C â†’ æˆ¿é—´ D

æ¯ç»è¿‡ä¸€ä¸ªæˆ¿é—´ï¼Œéƒ½ä¼šäº§ç”Ÿä¸€äº›"ä¸­é—´çŠ¶æ€"ï¼š
- æ¬åˆ°å“ªäº†ï¼Ÿ
- ç”¨äº†ä»€ä¹ˆå·¥å…·ï¼Ÿ
- æ¡Œå­ç°åœ¨çš„ä½ç½®ï¼Ÿ

è¿™äº›"ä¸­é—´çŠ¶æ€" = Activationï¼ˆæ¿€æ´»å€¼ï¼‰
```

### 2.2 ä¸ä½¿ç”¨ ACï¼šå…¨éƒ¨è®°å½•ï¼ˆå†…å­˜çˆ†ç‚¸ï¼‰

**åœºæ™¯**ï¼šæ¬æ¡Œå­æ—¶ï¼Œåœ¨æ¯ä¸ªæˆ¿é—´éƒ½è¯¦ç»†è®°å½•ã€‚

```
æ¬æ¡Œå­è¿‡ç¨‹ï¼ˆForwardï¼‰ï¼š
æˆ¿é—´ A â†’ æˆ¿é—´ B â†’ æˆ¿é—´ C â†’ æˆ¿é—´ D
  â†“        â†“        â†“        â†“
[ç¬”è®°1]  [ç¬”è®°2]  [ç¬”è®°3]  [ç¬”è®°4]
å†™æ»¡äº†   å†™æ»¡äº†   å†™æ»¡äº†   å†™æ»¡äº†
10 é¡µ    10 é¡µ    10 é¡µ    10 é¡µ

æ€»å…±ï¼š40 é¡µç¬”è®°ï¼ˆå†…å­˜ï¼‰

æ£€æŸ¥å·¥ä½œï¼ˆBackwardï¼‰ï¼š
éœ€è¦ç”¨è¿™äº›ç¬”è®°å›é¡¾æ¯ä¸€æ­¥
ç¬”è®°å…¨åœ¨æ‰‹è¾¹ï¼ŒæŸ¥é˜…å¾ˆå¿« âœ…

é—®é¢˜ï¼š
âŒ ç¬”è®°æœ¬ç”¨å®Œäº†ï¼ˆå†…å­˜ä¸è¶³ï¼‰
âŒ èƒŒç€ 40 é¡µç¬”è®°å¾ˆé‡ï¼ˆå†…å­˜é™åˆ¶ï¼‰
```

### 2.3 Full ACï¼šåªè®°å…³é”®ç‚¹ï¼ˆæè‡´çœå†…å­˜ï¼‰

**åœºæ™¯**ï¼šåªåœ¨æœ€å¼€å§‹è®°å½•ï¼Œéœ€è¦æ—¶é‡æ–°æ¬ä¸€éã€‚

```
æ¬æ¡Œå­è¿‡ç¨‹ï¼ˆForwardï¼‰ï¼š
æˆ¿é—´ A â†’ æˆ¿é—´ B â†’ æˆ¿é—´ C â†’ æˆ¿é—´ D
  â†“
[ç¬”è®°1]  [ä¸¢å¼ƒ]  [ä¸¢å¼ƒ]  [ä¸¢å¼ƒ]
åªä¿ç•™
èµ·ç‚¹

æ€»å…±ï¼šåªæœ‰ 1 é¡µç¬”è®°ï¼ˆèŠ‚çœå†…å­˜ï¼ï¼‰

æ£€æŸ¥å·¥ä½œï¼ˆBackwardï¼‰ï¼š
éœ€è¦æˆ¿é—´ C çš„ä¿¡æ¯ï¼Ÿ
  â†’ ä»èµ·ç‚¹é‡æ–°æ¬ä¸€éï¼šA â†’ B â†’ Cï¼ˆé‡æ–°è®¡ç®—ï¼‰
  â†’ æŸ¥çœ‹ä¿¡æ¯
  â†’ ç»§ç»­æ£€æŸ¥

éœ€è¦æˆ¿é—´ B çš„ä¿¡æ¯ï¼Ÿ
  â†’ ä»èµ·ç‚¹é‡æ–°æ¬ä¸€éï¼šA â†’ Bï¼ˆé‡æ–°è®¡ç®—ï¼‰
  â†’ æŸ¥çœ‹ä¿¡æ¯

ä¼˜ç‚¹ï¼š
âœ… åªéœ€è¦ 1 é¡µç¬”è®°ï¼ˆå†…å­˜å ç”¨æä½ï¼‰

ç¼ºç‚¹ï¼š
âŒ æ¯æ¬¡éƒ½è¦é‡æ–°æ¬ï¼ˆè®¡ç®—æ—¶é—´å¢åŠ  ~20%ï¼‰
```

### 2.4 Selective AC - Layerï¼šéš”å‡ ä¸ªæˆ¿é—´è®°å½•

**åœºæ™¯**ï¼šæ¯éš” N ä¸ªæˆ¿é—´åšä¸€æ¬¡è®°å½•ã€‚

```
æ¬æ¡Œå­è¿‡ç¨‹ï¼ˆForwardï¼‰- æ¯ 2 ä¸ªæˆ¿é—´è®°å½•ï¼š
æˆ¿é—´ A â†’ æˆ¿é—´ B â†’ æˆ¿é—´ C â†’ æˆ¿é—´ D â†’ ... â†’ æˆ¿é—´ H
  â†“                  â†“                         â†“
[ç¬”è®°1]            [ç¬”è®°2]                   [ç¬”è®°3]
è®°å½•              è®°å½•                       è®°å½•

æ€»å…±ï¼š3 é¡µç¬”è®°ï¼ˆçœäº†ä¸€åŠå†…å­˜ï¼‰

æ£€æŸ¥å·¥ä½œï¼ˆBackwardï¼‰ï¼š
éœ€è¦æˆ¿é—´ C çš„ä¿¡æ¯ï¼Ÿ
  â†’ ç›´æ¥æŸ¥ç¬”è®°2 âœ…ï¼ˆå·²ä¿å­˜ï¼‰

éœ€è¦æˆ¿é—´ B çš„ä¿¡æ¯ï¼Ÿ
  â†’ ä»ç¬”è®°1é‡æ–°æ¬ï¼šA â†’ Bï¼ˆåªéœ€é‡ç®— 1 æ­¥ï¼‰
  â†’ æŸ¥çœ‹ä¿¡æ¯

ä¼˜ç‚¹ï¼š
âœ… ç¬”è®°å‡å°‘ 50%ï¼ˆå†…å­˜å ç”¨ä¸­ç­‰ï¼‰
âœ… é‡æ–°è®¡ç®—çš„æ¬¡æ•°å°‘ï¼ˆé€Ÿåº¦æŸå¤± ~10%ï¼‰

ç¼ºç‚¹ï¼š
âš ï¸ éœ€è¦æƒè¡¡è®°å½•é¢‘ç‡
```

### 2.5 Selective AC - Operatorï¼šèªæ˜çš„è‰ç¨¿çº¸

**åœºæ™¯**ï¼šæ ¹æ®é‡è¦æ€§å†³å®šè®°å½•ä»€ä¹ˆã€‚

```
æ¬æ¡Œå­è¿‡ç¨‹ä¸­çš„ä¸åŒæ“ä½œï¼š
1. æµ‹é‡æ¡Œå­å°ºå¯¸         â†’ ç®€å•ï¼Œé‡ç®—å¾ˆå¿«
2. æ‹†è§£æ¡Œå­è…¿           â†’ ç®€å•ï¼Œé‡ç®—å¾ˆå¿«
3. ç”¨èµ·é‡æœºæ¬ä¸»ä½“ ğŸ—ï¸    â†’ å¤æ‚ï¼é‡ç®—å¾ˆæ…¢ï¼
4. è°ƒæ•´æ¡Œå­æ–¹å‘         â†’ ç®€å•ï¼Œé‡ç®—å¾ˆå¿«
5. ç²¾å¯†æ‹¼è£… ğŸ”§          â†’ å¤æ‚ï¼é‡ç®—å¾ˆæ…¢ï¼

ç­–ç•¥ï¼š
âœ… ä¿å­˜ï¼šå¤æ‚æ“ä½œçš„ç»“æœï¼ˆèµ·é‡æœºã€ç²¾å¯†æ‹¼è£…ï¼‰
âŒ ä¸¢å¼ƒï¼šç®€å•æ“ä½œçš„ç»“æœï¼ˆæµ‹é‡ã€æ‹†è§£ï¼‰

æ€»å…±ï¼šåªè®°å½•å…³é”®æ“ä½œï¼ˆæœ€ä¼˜çš„å†…å­˜-é€Ÿåº¦å¹³è¡¡ï¼‰

æ£€æŸ¥å·¥ä½œï¼ˆBackwardï¼‰ï¼š
éœ€è¦èµ·é‡æœºçš„ä¿¡æ¯ï¼Ÿ
  â†’ ç›´æ¥æŸ¥ç¬”è®° âœ…ï¼ˆå·²ä¿å­˜ï¼Œå› ä¸ºé‡ç®—å¤ªæ…¢ï¼‰

éœ€è¦æµ‹é‡çš„ä¿¡æ¯ï¼Ÿ
  â†’ é‡æ–°æµ‹é‡ä¸€æ¬¡ âœ…ï¼ˆå¾ˆå¿«ï¼Œä¸å€¼å¾—ä¿å­˜ï¼‰

ä¼˜ç‚¹ï¼š
âœ… å†…å­˜å ç”¨ä½ï¼ˆåªä¿å­˜é‡è¦çš„ï¼‰
âœ… é€Ÿåº¦æŸå¤±å°ï¼ˆé‡ç®—çš„éƒ½æ˜¯å¿«é€Ÿæ“ä½œï¼‰

ç¼ºç‚¹ï¼š
âš ï¸ éœ€è¦çŸ¥é“å“ªäº›æ“ä½œ"é‡è¦"
```

### 2.6 å®é™…å¯¹åº”å…³ç³»

```
è‰ç¨¿çº¸æ¯”å–» â†’ æŠ€æœ¯å®ç°ï¼š

1. ç¬”è®° = Activationï¼ˆæ¿€æ´»å€¼ï¼‰
   - Forward æ—¶äº§ç”Ÿ
   - Backward æ—¶ä½¿ç”¨

2. é‡æ–°æ¬æ¡Œå­ = Recomputeï¼ˆé‡æ–°è®¡ç®—ï¼‰
   - æ²¡æœ‰ç¬”è®°æ—¶
   - ä» checkpoint å¼€å§‹é‡æ–° forward

3. ç¬”è®°æœ¬å®¹é‡ = GPU å†…å­˜
   - æœ‰é™çš„èµ„æº
   - éœ€è¦æƒè¡¡ä½¿ç”¨

4. æ¬æ¡Œå­çš„æ—¶é—´ = è®­ç»ƒæ—¶é—´
   - Recompute å¢åŠ æ—¶é—´
   - ä½†æ¢æ¥å†…å­˜èŠ‚çœ
```

---

## 3. ä¸‰ç§ AC æ¨¡å¼å¯¹æ¯”

### 3.1 æ¨¡å¼æ€»è§ˆ

TorchTitan æ”¯æŒ 4 ç§ AC æ¨¡å¼ï¼š

```python
# æ¥è‡ª: torchtitan/config/job_config.py:586

mode: Literal["selective", "full", "memory_budget", "none"] = "selective"
```

| æ¨¡å¼ | å†…å­˜èŠ‚çœ | é€Ÿåº¦æŸå¤± | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **None** | 0% | 0% | å°æ¨¡å‹ã€å†…å­˜å……è¶³ |
| **Selective (Layer)** | 30-50% | ~10% | ä¸­ç­‰æ¨¡å‹ï¼ˆæ¨èï¼‰ |
| **Selective (Op)** | 40-60% | ~12% | å¤§æ¨¡å‹ã€æœ€ä¼˜å¹³è¡¡ |
| **Full** | 50-70% | ~20% | è¶…å¤§æ¨¡å‹ã€å†…å­˜ç´§å¼  |
| **Memory Budget** | è‡ªå®šä¹‰ | è‡ªå®šä¹‰ | é«˜çº§ä¼˜åŒ–ã€è‡ªåŠ¨æœç´¢ |

### 3.2 Full AC

**ç­–ç•¥**ï¼šæ¯ä¸ª TransformerBlock éƒ½ä¸¢å¼ƒæ‰€æœ‰æ¿€æ´»å€¼ã€‚

```
Forward (32 layers):
Layer 0:  [Compute] â†’ [Save input only] â†’ [Discard activations]
Layer 1:  [Compute] â†’ [Save input only] â†’ [Discard activations]
...
Layer 31: [Compute] â†’ [Save input only] â†’ [Discard activations]

ä¿å­˜ï¼š
- æ¯å±‚çš„è¾“å…¥ï¼ˆ32 ä¸ª checkpointï¼‰
- æœ€ç»ˆçš„è¾“å‡º

Backward (32 layers):
Layer 31: [Recompute forward] â†’ [Compute backward]
Layer 30: [Recompute forward] â†’ [Compute backward]
...
Layer 0:  [Recompute forward] â†’ [Compute backward]

æ¯å±‚éƒ½éœ€è¦é‡æ–°è®¡ç®—ä¸€æ¬¡ forwardï¼
```

**é…ç½®**ï¼š

```toml
[activation_checkpoint]
mode = "full"
```

### 3.3 Selective AC - Layer (æ¯ N å±‚)

**ç­–ç•¥**ï¼šæ¯éš” N å±‚ä¿å­˜æ¿€æ´»å€¼ï¼Œä¸­é—´å±‚ä¸¢å¼ƒã€‚

```
Forward (32 layers, N=2):
Layer 0:  [Compute] â†’ âœ… Save activations
Layer 1:  [Compute] â†’ âŒ Discard activations
Layer 2:  [Compute] â†’ âœ… Save activations
Layer 3:  [Compute] â†’ âŒ Discard activations
...
Layer 30: [Compute] â†’ âœ… Save activations
Layer 31: [Compute] â†’ âŒ Discard activations

ä¿å­˜ï¼š16 å±‚çš„æ¿€æ´»å€¼ï¼ˆèŠ‚çœ 50%ï¼‰

Backward:
Layer 31: âŒ éœ€è¦ä» Layer 30 é‡æ–°è®¡ç®—
Layer 30: âœ… ç›´æ¥ä½¿ç”¨ä¿å­˜çš„æ¿€æ´»å€¼
Layer 29: âŒ éœ€è¦ä» Layer 28 é‡æ–°è®¡ç®—
Layer 28: âœ… ç›´æ¥ä½¿ç”¨ä¿å­˜çš„æ¿€æ´»å€¼
...

æ¯å±‚æœ€å¤šé‡ç®— 1 æ¬¡ forward
```

**é…ç½®**ï¼š

```toml
[activation_checkpoint]
mode = "selective"
selective_ac_option = "2"  # æ¯ 2 å±‚ä¿å­˜ä¸€æ¬¡
# æˆ– "3" æ¯ 3 å±‚, "4" æ¯ 4 å±‚...
```

### 3.4 Selective AC - Operator (ç®—å­çº§)

**ç­–ç•¥**ï¼šä¿å­˜"æ˜‚è´µ"çš„ç®—å­ç»“æœï¼Œä¸¢å¼ƒ"ä¾¿å®œ"çš„ã€‚

```
TransformerBlock å†…éƒ¨æ“ä½œï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. LayerNorm            â†’ Recompute â”‚  (ç®€å•)
â”‚ 2. QKV Projection (mm)  â†’ SAVE âœ…   â”‚  (çŸ©é˜µä¹˜æ³•ï¼Œé‡ç®—æ…¢)
â”‚ 3. Attention (SDPA) ğŸ”¥  â†’ SAVE âœ…   â”‚  (å¤æ‚ï¼Œé‡ç®—å¾ˆæ…¢)
â”‚ 4. Output Proj (mm)     â†’ SAVE âœ…   â”‚  (çŸ©é˜µä¹˜æ³•)
â”‚ 5. Add & Norm           â†’ Recompute â”‚  (ç®€å•)
â”‚ 6. FFN W1 (mm)          â†’ SAVE âœ…   â”‚  (çŸ©é˜µä¹˜æ³•)
â”‚ 7. Activation (SiLU)    â†’ Recompute â”‚  (ç®€å•)
â”‚ 8. FFN W2 (mm)          â†’ SAVE âœ…   â”‚  (çŸ©é˜µä¹˜æ³•)
â”‚ 9. Add                  â†’ Recompute â”‚  (ç®€å•)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¿å­˜ï¼š5 ä¸ªå…³é”®ç®—å­çš„è¾“å‡º
ä¸¢å¼ƒï¼š4 ä¸ªç®€å•æ“ä½œçš„è¾“å‡º

å†…å­˜èŠ‚çœï¼š~50%
é€Ÿåº¦æŸå¤±ï¼š~12%ï¼ˆé‡ç®—çš„éƒ½æ˜¯ç®€å•æ“ä½œï¼‰
```

**é…ç½®**ï¼š

```toml
[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"
```

### 3.5 å†…å­˜å ç”¨å¯¹æ¯”

å‡è®¾ Llama3 8Bï¼Œbatch=2ï¼Œseq_len=8192ï¼š

```
No AC:
  Activations: ~4 GB
  å¯ç”¨å†…å­˜: 80 GB - 4 GB - 12 GB (å‚æ•°+ä¼˜åŒ–å™¨) = 64 GB

Selective (Layer, N=2):
  Activations: ~2 GB (èŠ‚çœ 50%)
  å¯ç”¨å†…å­˜: 80 GB - 2 GB - 12 GB = 66 GB
  â†’ å¯å¢å¤§ batch size 30%

Selective (Op):
  Activations: ~1.8 GB (èŠ‚çœ 55%)
  å¯ç”¨å†…å­˜: 80 GB - 1.8 GB - 12 GB = 66.2 GB
  â†’ å¯å¢å¤§ batch size 35%

Full AC:
  Activations: ~1 GB (èŠ‚çœ 75%)
  å¯ç”¨å†…å­˜: 80 GB - 1 GB - 12 GB = 67 GB
  â†’ å¯å¢å¤§ batch size 70%
```

---

## 4. Full AC å®ç°

### 4.1 æ ¸å¿ƒåŸç†

Full AC ä½¿ç”¨ PyTorch çš„ `checkpoint_wrapper`ï¼š

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:139-155

def _apply_full_ac(module: nn.Module, ac_config: ACConfig) -> nn.Module:
    """Apply full activation checkpointing to the module."""

    return ptd_checkpoint_wrapper(
        module,
        preserve_rng_state=ac_config.preserve_rng_state,  # ä¿æŒéšæœºæ€§
        determinism_check=ac_config.determinism_check,    # ç¡®å®šæ€§æ£€æŸ¥
        early_stop=ac_config.early_stop,                  # æ—©åœä¼˜åŒ–
        debug=ac_config.debug,                            # Debug æ¨¡å¼
    )
```

**checkpoint_wrapper åšäº†ä»€ä¹ˆï¼Ÿ**

```python
# ä¼ªä»£ç ï¼Œå±•ç¤ºåŸç†

class CheckpointWrapper(nn.Module):
    def __init__(self, module):
        self.module = module

    def forward(self, *args):
        # 1. ä¿å­˜è¾“å…¥
        saved_inputs = args

        # 2. æ­£å¸¸æ‰§è¡Œ forwardï¼ˆä½†ä¸ä¿å­˜ä¸­é—´æ¿€æ´»ï¼‰
        with torch.no_grad():
            output = self.module(*args)

        # 3. æ³¨å†Œ backward hook
        output.register_hook(lambda grad_output:
            self._backward_with_recompute(saved_inputs, grad_output)
        )

        return output

    def _backward_with_recompute(self, saved_inputs, grad_output):
        # Backward æ—¶ï¼š
        # 1. é‡æ–°è®¡ç®— forwardï¼ˆè¿™æ¬¡ä¿å­˜æ¿€æ´»ï¼‰
        with torch.enable_grad():
            output = self.module(*saved_inputs)

        # 2. è®¡ç®—æ¢¯åº¦
        output.backward(grad_output)
```

### 4.2 åº”ç”¨åˆ° Transformer

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:323-332

# å¯¹æ¯ä¸ª TransformerBlock åº”ç”¨ Full AC
for layer_id, transformer_block in model.layers.named_children():
    transformer_block = _apply_full_ac(transformer_block, ac_config)
    model.layers.register_module(layer_id, transformer_block)
```

**æ•ˆæœ**ï¼š

```
æœªåŒ…è£…çš„ TransformerBlock:
forward():
  x = layer_norm(x)          â†’ ä¿å­˜ x
  q, k, v = qkv_proj(x)      â†’ ä¿å­˜ q, k, v
  attn_out = attention(q,k,v) â†’ ä¿å­˜ attn_out
  x = out_proj(attn_out)     â†’ ä¿å­˜ x
  ...

åŒ…è£…åçš„ TransformerBlock:
forward():
  [ä¿å­˜è¾“å…¥ x0]
  with no_grad():
    x = layer_norm(x)        â†’ ä¸ä¿å­˜
    q, k, v = qkv_proj(x)    â†’ ä¸ä¿å­˜
    attn_out = attention()   â†’ ä¸ä¿å­˜
    ...
  [åªä¿å­˜æœ€ç»ˆè¾“å‡º]

backward():
  [é‡æ–°è®¡ç®—æ•´ä¸ª forward]
  with enable_grad():
    x = layer_norm(x)        â†’ ä¿å­˜ï¼ˆç”¨äºæ¢¯åº¦ï¼‰
    q, k, v = qkv_proj(x)    â†’ ä¿å­˜
    ...
  [è®¡ç®—æ¢¯åº¦]
```

### 4.3 å†…å­˜èŠ‚çœåˆ†æ

```
TransformerBlock æ¿€æ´»å€¼å¤§å°ï¼š
  layer_norm:    batch Ã— seq Ã— hidden â‰ˆ 128 MB
  qkv_proj:      3 Ã— 128 MB = 384 MB
  attention:     batch Ã— heads Ã— seq Ã— seq â‰ˆ 512 MB
  out_proj:      128 MB
  ffn_norm:      128 MB
  ffn_w1/w3:     2 Ã— 256 MB = 512 MB
  ffn_w2:        128 MB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  æ€»è®¡: ~2 GB / layer

Full AC:
  åªä¿å­˜è¾“å…¥: 128 MB / layer
  èŠ‚çœ: 2 GB - 128 MB = 1.87 GB / layer

32 å±‚æ€»èŠ‚çœ: 1.87 GB Ã— 32 = 60 GBï¼
```

---

## 5. Selective AC - Layer å±‚çº§

### 5.1 Layer SAC åŸç†

**ç­–ç•¥**ï¼šæ¯ N å±‚ä¿å­˜ä¸€æ¬¡å®Œæ•´æ¿€æ´»ã€‚

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:26-48

_layer_sac_count = 0  # å…¨å±€è®¡æ•°å™¨

def _apply_layer_sac(module: nn.Module, ac_config: ACConfig) -> nn.Module:
    global _layer_sac_count
    _layer_sac_count += 1  # æ¯è°ƒç”¨ä¸€æ¬¡ +1

    ac_freq = int(ac_config.selective_ac_option)  # ä¾‹å¦‚ "2"

    if _layer_sac_count % ac_freq == 0:
        # ç¬¬ 0, 2, 4, 6, ... å±‚ï¼šä¸ä½¿ç”¨ ACï¼Œä¿å­˜æ¿€æ´»
        return module
    else:
        # ç¬¬ 1, 3, 5, 7, ... å±‚ï¼šä½¿ç”¨ ACï¼Œä¸¢å¼ƒæ¿€æ´»
        return ptd_checkpoint_wrapper(module, ...)
```

**åº”ç”¨ç¤ºä¾‹** (N=2)ï¼š

```
Layer 0:  No AC  â†’ ä¿å­˜æ‰€æœ‰æ¿€æ´» âœ…
Layer 1:  AC     â†’ ä¸¢å¼ƒæ¿€æ´» âŒ
Layer 2:  No AC  â†’ ä¿å­˜æ‰€æœ‰æ¿€æ´» âœ…
Layer 3:  AC     â†’ ä¸¢å¼ƒæ¿€æ´» âŒ
...
Layer 30: No AC  â†’ ä¿å­˜æ‰€æœ‰æ¿€æ´» âœ…
Layer 31: AC     â†’ ä¸¢å¼ƒæ¿€æ´» âŒ

ä¿å­˜: 16 å±‚çš„æ¿€æ´»
ä¸¢å¼ƒ: 16 å±‚çš„æ¿€æ´»
èŠ‚çœ: 50%
```

### 5.2 Backward é‡è®¡ç®—

```
Backward æ—¶ï¼ˆä»åå‘å‰ï¼‰ï¼š

Layer 31 (AC):
  éœ€è¦æ¿€æ´» â†’ ä» Layer 30 çš„è¾“å‡ºé‡æ–°è®¡ç®—
  é‡ç®—æ¬¡æ•°: 1 æ¬¡

Layer 30 (No AC):
  ç›´æ¥ä½¿ç”¨ä¿å­˜çš„æ¿€æ´» âœ…
  é‡ç®—æ¬¡æ•°: 0 æ¬¡

Layer 29 (AC):
  éœ€è¦æ¿€æ´» â†’ ä» Layer 28 çš„è¾“å‡ºé‡æ–°è®¡ç®—
  é‡ç®—æ¬¡æ•°: 1 æ¬¡

Layer 28 (No AC):
  ç›´æ¥ä½¿ç”¨ä¿å­˜çš„æ¿€æ´» âœ…
  é‡ç®—æ¬¡æ•°: 0 æ¬¡

...

å¹³å‡é‡ç®—æ¬¡æ•°: 16 / 32 = 0.5 æ¬¡/å±‚
```

### 5.3 è°ƒä¼˜ç­–ç•¥

**é€‰æ‹© N çš„å‡†åˆ™**ï¼š

```
N = 1: æ¯å±‚éƒ½ä¿å­˜
  - å†…å­˜èŠ‚çœ: 0%
  - é€Ÿåº¦æŸå¤±: 0%
  - ç›¸å½“äº No AC

N = 2: æ¯ 2 å±‚ä¿å­˜ä¸€æ¬¡ï¼ˆæ¨èï¼‰
  - å†…å­˜èŠ‚çœ: 50%
  - é€Ÿåº¦æŸå¤±: ~10%
  - å¹³è¡¡ç‚¹

N = 3: æ¯ 3 å±‚ä¿å­˜ä¸€æ¬¡
  - å†…å­˜èŠ‚çœ: 67%
  - é€Ÿåº¦æŸå¤±: ~15%

N = 4: æ¯ 4 å±‚ä¿å­˜ä¸€æ¬¡
  - å†…å­˜èŠ‚çœ: 75%
  - é€Ÿåº¦æŸå¤±: ~18%
  - æ¥è¿‘ Full AC

N = âˆ: ç›¸å½“äº Full AC
  - å†…å­˜èŠ‚çœ: ~80%
  - é€Ÿåº¦æŸå¤±: ~20%
```

**å®é™…é€‰æ‹©**ï¼š

```toml
# å†…å­˜å……è¶³ï¼Œè¿½æ±‚é€Ÿåº¦
[activation_checkpoint]
mode = "selective"
selective_ac_option = "2"  # æˆ– "1"ï¼ˆå‡ ä¹ä¸ç”¨ ACï¼‰

# å†…å­˜ç´§å¼ ï¼Œæ„¿æ„ç‰ºç‰²é€Ÿåº¦
[activation_checkpoint]
mode = "selective"
selective_ac_option = "4"  # æˆ–ä½¿ç”¨ "full"
```

---

## 6. Selective AC - Operator ç®—å­çº§

### 6.1 Op SAC åŸç†

**ç­–ç•¥**ï¼šæ ¹æ®ç®—å­çš„é‡ç®—ä»£ä»·å†³å®šä¿å­˜è¿˜æ˜¯ä¸¢å¼ƒã€‚

```python
# æ¥è‡ª: torchtitan/models/llama3/infra/parallelize.py:34-44

# å®šä¹‰"æ˜‚è´µ"çš„ç®—å­ï¼ˆå¿…é¡»ä¿å­˜ï¼‰
_op_sac_save_list = {
    torch.ops.aten.mm.default,  # çŸ©é˜µä¹˜æ³•ï¼ˆMatmulï¼‰
    torch.ops.aten._scaled_dot_product_efficient_attention.default,  # Attention
    torch.ops.aten._scaled_dot_product_flash_attention.default,      # Flash Attention
    torch.ops._c10d_functional.reduce_scatter_tensor.default,        # é€šä¿¡ç®—å­
    torch.ops.aten.max.default,  # Maxï¼ˆç”¨äº float8 é‡åŒ–ï¼‰
    torch._higher_order_ops.flex_attention,  # Flex Attention
}
```

**ä¸ºä»€ä¹ˆè¿™äº›ç®—å­éœ€è¦ä¿å­˜ï¼Ÿ**

```
1. torch.ops.aten.mm.default (çŸ©é˜µä¹˜æ³•):
   è®¡ç®—é‡: O(NÂ³)  (N = 4096)
   é‡ç®—ä»£ä»·: éå¸¸é«˜ ğŸ˜±
   â†’ å¿…é¡»ä¿å­˜ âœ…

2. Attention (SDPA/Flash):
   è®¡ç®—é‡: O(NÂ² Ã— d)  (N = 8192, d = 128)
   é‡ç®—ä»£ä»·: æé«˜ ğŸ˜±ğŸ˜±
   â†’ å¿…é¡»ä¿å­˜ âœ…

3. Reduce-Scatter (é€šä¿¡):
   ä»£ä»·: ç½‘ç»œé€šä¿¡
   é‡ç®—ä»£ä»·: é«˜ï¼ˆéœ€è¦é‡æ–°é€šä¿¡ï¼‰
   â†’ å¿…é¡»ä¿å­˜ âœ…

4. LayerNorm, Add, SiLU:
   è®¡ç®—é‡: O(N)
   é‡ç®—ä»£ä»·: å¾ˆä½ âœ…
   â†’ å¯ä»¥ä¸¢å¼ƒï¼Œéœ€è¦æ—¶é‡ç®—
```

### 6.2 è‡ªå®šä¹‰ Policy

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:97-123

def _get_custom_policy(meta):
    def _custom_policy(ctx, func, *args, **kwargs):
        # è§„åˆ™ 1: æ°¸è¿œä¸è¦ä¸¢å¼ƒ GPU â†’ CPU çš„æ‹·è´
        if (
            func == torch.ops.aten._to_copy.default
            and "cuda" in str(args[0].device)
            and "device" in kwargs
            and str(kwargs["device"]) == "cpu"
        ):
            return CheckpointPolicy.MUST_SAVE

        # è§„åˆ™ 2: Matmul çš„æ™ºèƒ½ç­–ç•¥
        if func == torch.ops.aten.mm.default:
            # æ£€æŸ¥ shapeï¼ˆæŸäº›ç‰¹å®š shape å¼ºåˆ¶é‡ç®—ï¼‰
            if args[1].shape in mm_recompute_shapes:
                return CheckpointPolicy.PREFER_RECOMPUTE

            # æ¯éš”ä¸€ä¸ª mm ä¿å­˜ä¸€æ¬¡ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            meta["mm_count"] += 1
            if meta["mm_count"] % 2 == 0:
                return CheckpointPolicy.PREFER_RECOMPUTE

        # è§„åˆ™ 3: åœ¨ save_list ä¸­çš„ç®—å­ â†’ ä¿å­˜
        to_save = func in op_sac_save_list
        return (
            CheckpointPolicy.MUST_SAVE if to_save
            else CheckpointPolicy.PREFER_RECOMPUTE
        )

    return _custom_policy
```

**ä¸‰ç§ Policy**ï¼š

```
MUST_SAVE:
  - å¿…é¡»ä¿å­˜ï¼Œä¸å¯ä¸¢å¼ƒ
  - ç”¨äºï¼šGPUâ†’CPU æ‹·è´

MUST_RECOMPUTE:
  - å¿…é¡»ä¸¢å¼ƒï¼Œå¼ºåˆ¶é‡ç®—
  - ç”¨äºï¼šè°ƒè¯•ã€ç‰¹å®šä¼˜åŒ–

PREFER_RECOMPUTE:
  - ä¼˜å…ˆä¸¢å¼ƒï¼Œå¯ä»¥é‡ç®—
  - ç”¨äºï¼šå¤§éƒ¨åˆ†ç®—å­ï¼ˆé»˜è®¤ï¼‰
```

### 6.3 å®é™…æ‰§è¡Œæµç¨‹

```
Forward Pass (TransformerBlock):

1. LayerNorm(x)
   Policy: PREFER_RECOMPUTE
   â†’ ä¸¢å¼ƒè¾“å‡º âŒ

2. QKV Projection (3ä¸ª mm)
   Policy: MUST_SAVE (åœ¨ save_list)
   â†’ ä¿å­˜ q, k, v âœ…

3. Attention (SDPA)
   Policy: MUST_SAVE (åœ¨ save_list)
   â†’ ä¿å­˜ attn_output âœ…

4. Output Projection (mm)
   Policy: MUST_SAVE (åœ¨ save_list)
   â†’ ä¿å­˜è¾“å‡º âœ…

5. Add
   Policy: PREFER_RECOMPUTE
   â†’ ä¸¢å¼ƒ âŒ

6. LayerNorm
   Policy: PREFER_RECOMPUTE
   â†’ ä¸¢å¼ƒ âŒ

7. FFN W1/W3 (2ä¸ª mm)
   Policy: MUST_SAVE
   â†’ ä¿å­˜ âœ…

8. SiLU Activation
   Policy: PREFER_RECOMPUTE
   â†’ ä¸¢å¼ƒ âŒ

9. FFN W2 (mm)
   Policy: MUST_SAVE
   â†’ ä¿å­˜ âœ…

æ€»ç»“:
  ä¿å­˜: 7 ä¸ªå…³é”®ç®—å­
  ä¸¢å¼ƒ: 4 ä¸ªç®€å•ç®—å­
  å†…å­˜èŠ‚çœ: ~40-50%
```

### 6.4 ä¸ Matmul çš„ç‰¹æ®Šå¤„ç†

```python
# æ¯éš”ä¸€ä¸ª mm ä¿å­˜ä¸€æ¬¡
if func == torch.ops.aten.mm.default:
    meta["mm_count"] += 1
    if meta["mm_count"] % 2 == 0:
        return CheckpointPolicy.PREFER_RECOMPUTE  # ä¸¢å¼ƒ

ä¸ºä»€ä¹ˆï¼Ÿ
  TransformerBlock æœ‰å¾ˆå¤š mm:
  - QKV: 3 ä¸ª mm
  - Output: 1 ä¸ª mm
  - FFN: 3 ä¸ª mm (w1, w2, w3)

  å…¨éƒ¨ä¿å­˜: å†…å­˜å ç”¨é«˜
  å…¨éƒ¨ä¸¢å¼ƒ: é‡ç®—ä»£ä»·é«˜

  æŠ˜ä¸­: ä¿å­˜ä¸€åŠï¼Œä¸¢å¼ƒä¸€åŠ
    â†’ å†…å­˜å ç”¨ä¸­ç­‰
    â†’ é‡ç®—ä»£ä»·ä¸­ç­‰
```

---

## 7. æºç å®ç°è¯¦è§£

### 7.1 å…¥å£å‡½æ•°

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:286-334

def apply_ac(
    model: nn.Module,
    ac_config: ACConfig,
    *,
    model_compile_enabled: bool = False,
    use_flex_attn: bool = False,
    op_sac_save_list: set[torch._ops.OpOverload] | None = None,
) -> None:
    """Apply activation checkpointing to the model."""

    # ç‰¹æ®Šæ¨¡å¼ï¼šMemory Budget (è‡ªåŠ¨æœç´¢æœ€ä¼˜ç­–ç•¥)
    if ac_config.mode == "memory_budget":
        assert model_compile_enabled, "Memory budget éœ€è¦ compile"
        torch._functorch.config.activation_memory_budget = ac_config.memory_budget
        return

    # æ ‡å‡†æ¨¡å¼ï¼šå¯¹æ¯ä¸ª TransformerBlock åº”ç”¨ AC
    for layer_id, transformer_block in model.layers.named_children():
        transformer_block = _apply_ac_to_transformer_block(
            transformer_block,
            ac_config,
            base_fqn=f"layers.{layer_id}",
            model_compile_enabled=model_compile_enabled,
            use_flex_attn=use_flex_attn,
            op_sac_save_list=op_sac_save_list,
        )
        model.layers.register_module(layer_id, transformer_block)
```

### 7.2 TransformerBlock çš„åŒ…è£…

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:233-283

def _apply_ac_to_transformer_block(
    module: nn.Module,
    ac_config: ACConfig,
    ...
) -> nn.Module:
    # 1. æ£€æŸ¥æ¨¡å¼
    if ac_config.mode == "full":
        return _apply_full_ac(module, ac_config)

    # 2. Selective AC
    assert ac_config.mode == "selective"

    # 2.1 åˆ¤æ–­æ˜¯ Layer SAC è¿˜æ˜¯ Op SAC
    use_op_sac = (ac_config.selective_ac_option == "op")
    use_layer_sac = ac_config.selective_ac_option.isdigit()  # ä¾‹å¦‚ "2"

    if use_op_sac:
        # 2.2 Op SAC
        if use_flex_attn:
            # Flex Attention ç‰¹æ®Šå¤„ç†ï¼ˆé¿å…ä¸ compile å†²çªï¼‰
            return _apply_op_sac_to_transformer_block_with_flex(...)
        else:
            return _apply_op_sac(module, ac_config, ...)

    # 2.3 Layer SAC
    return _apply_layer_sac(module, ac_config)
```

### 7.3 Checkpoint Wrapper åŸç†

```python
# PyTorch å†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

def checkpoint(function, *args, preserve_rng_state=True):
    """Checkpoint å‡½æ•°çš„æ ¸å¿ƒé€»è¾‘"""

    class CheckpointFunction(torch.autograd.Function):
        @staticmethod
        def forward(ctx, *args):
            # Forward: ä¸ä¿å­˜ä¸­é—´ç»“æœ
            ctx.save_for_backward(*args)  # åªä¿å­˜è¾“å…¥

            with torch.no_grad():
                outputs = function(*args)

            return outputs

        @staticmethod
        def backward(ctx, *grad_outputs):
            # Backward: é‡æ–°è®¡ç®—
            inputs = ctx.saved_tensors

            # é‡æ–°æ‰§è¡Œ forwardï¼ˆè¿™æ¬¡ä¿ç•™æ¢¯åº¦ï¼‰
            with torch.enable_grad():
                detached_inputs = [x.detach().requires_grad_() for x in inputs]
                outputs = function(*detached_inputs)

            # è®¡ç®—æ¢¯åº¦
            torch.autograd.backward(outputs, grad_outputs)

            # è¿”å›è¾“å…¥çš„æ¢¯åº¦
            grads = [x.grad for x in detached_inputs]
            return tuple(grads)

    return CheckpointFunction.apply(*args)
```

### 7.4 Selective Checkpoint Context

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:125-136

def _apply_op_sac(module, ac_config, ...):
    # åˆ›å»º selective checkpoint context
    def selective_checkpointing_context_fn():
        meta = defaultdict(int)  # çŠ¶æ€è¿½è¸ªï¼ˆmm è®¡æ•°ç­‰ï¼‰
        return create_selective_checkpoint_contexts(
            _get_custom_policy(meta)
        )

    # åŒ…è£… module
    return ptd_checkpoint_wrapper(
        module,
        context_fn=selective_checkpointing_context_fn,  # è‡ªå®šä¹‰ç­–ç•¥
        ...
    )
```

**create_selective_checkpoint_contexts åšäº†ä»€ä¹ˆï¼Ÿ**

```python
# PyTorch å†…éƒ¨ï¼ˆç®€åŒ–ï¼‰

def create_selective_checkpoint_contexts(policy_fn):
    """åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ‹¦æˆªæ¯ä¸ªç®—å­è°ƒç”¨"""

    class SelectiveCheckpoint:
        def __enter__(self):
            # æ³¨å†Œ dispatch hook
            self.handle = torch._C._register_dispatch_key_hook(
                self._hook
            )

        def _hook(self, func, *args, **kwargs):
            # æ¯ä¸ªç®—å­è°ƒç”¨æ—¶ï¼š
            # 1. è°ƒç”¨ policy_fn åˆ¤æ–­æ˜¯å¦ä¿å­˜
            policy = policy_fn(ctx, func, *args, **kwargs)

            if policy == CheckpointPolicy.MUST_SAVE:
                # ä¿å­˜ï¼šæ­£å¸¸æ‰§è¡Œï¼Œä¿ç•™æ¢¯åº¦
                return func(*args, **kwargs)
            else:
                # ä¸¢å¼ƒï¼šæ‰§è¡Œä½†ä¸ä¿ç•™æ¢¯åº¦
                with torch.no_grad():
                    return func(*args, **kwargs)

        def __exit__(self, ...):
            self.handle.remove()
```

---

## 8. ä¸ torch.compile çš„äº¤äº’

### 8.1 é—®é¢˜ï¼šAC ä¸ Compile çš„å†²çª

```
é—®é¢˜ï¼š
  torch.compile éœ€è¦å®Œæ•´çš„è®¡ç®—å›¾
  AC ä¼šåœ¨è¿è¡Œæ—¶é‡æ–°è®¡ç®— â†’ ç ´åè®¡ç®—å›¾

ç¤ºä¾‹ï¼š
  # æ²¡æœ‰ AC
  model = torch.compile(model)  # ç¼–è¯‘æ•´ä¸ªæ¨¡å‹ âœ…

  # æœ‰ AC
  model = checkpoint_wrapper(model)
  model = torch.compile(model)  # ç¼–è¯‘å¤±è´¥æˆ–æ•ˆæœå·® âŒ
```

### 8.2 TorchTitan çš„è§£å†³æ–¹æ¡ˆ

**ç­–ç•¥ 1ï¼šå…ˆ ACï¼Œå Compile**

```python
# æ¥è‡ª: torchtitan/models/llama3/infra/parallelize.py:96-108

# 1. å…ˆåº”ç”¨ AC
apply_ac(model, ac_config, ...)

# 2. å† compileï¼ˆæ¯ä¸ª TransformerBlockï¼‰
if model_compile_enabled:
    apply_compile(model, compile_config)

# ä¸ºä»€ä¹ˆæŒ‰è¿™ä¸ªé¡ºåºï¼Ÿ
# - AC åŒ…è£…åï¼Œæ¯ä¸ª TransformerBlock æ˜¯ç‹¬ç«‹çš„ checkpoint å•å…ƒ
# - Compile å•ç‹¬ç¼–è¯‘æ¯ä¸ª TransformerBlock
# - ä¸ä¼šç ´å AC çš„é‡è®¡ç®—é€»è¾‘
```

**ç­–ç•¥ 2ï¼šFlex Attention çš„ç‰¹æ®Šå¤„ç†**

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:158-230

if use_flex_attn:
    # Flex Attention å¿…é¡» compile æ‰èƒ½é«˜æ•ˆ
    # ä½† AC ä¼šç ´å compile

    # è§£å†³æ–¹æ¡ˆï¼šåˆ†æ¨¡å—å¤„ç†
    if hasattr(module, "moe"):
        # MoE å±‚ï¼šéƒ½ç”¨ Op SAC
        wrap_submodule("moe", full_ac=False)

        if model_compile_enabled:
            wrap_submodule("attention", full_ac=False)  # Op SAC
        else:
            wrap_submodule("attention", full_ac=True)   # Full AC
    else:
        # Dense å±‚
        if model_compile_enabled:
            # æ•´ä¸ª block ç”¨ Op SACï¼ˆä¿ç•™ compileï¼‰
            module = _apply_op_sac(module, ...)
        else:
            # åˆ†å¼€åŒ…è£…
            wrap_submodule("feed_forward", full_ac=False)  # Op SAC
            wrap_submodule("attention", full_ac=True)      # Full AC
```

**ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**

```
Flex Attention éœ€æ±‚ï¼š
  âœ… å¿…é¡» compile
  âŒ ä¸èƒ½æœ‰ Full ACï¼ˆç ´åå›¾ï¼‰
  âœ… å¯ä»¥æœ‰ Op SACï¼ˆä¿ç•™å›¾ç»“æ„ï¼‰

ç­–ç•¥ï¼š
  compile = True:  ç”¨ Op SACï¼ˆå…¨éƒ¨ç®—å­çº§ï¼‰
  compile = False: Attention ç”¨ Full ACï¼ŒFFN ç”¨ Op SAC
```

### 8.3 Compile ä¸ AC çš„æ€§èƒ½å¯¹æ¯”

```
Llama3 8B (8 GPUs):

No AC + No Compile:
  é€Ÿåº¦: 5,762 tok/s/GPU
  å†…å­˜: 24 GB

No AC + Compile:
  é€Ÿåº¦: 6,667 tok/s/GPU (+15.7%)
  å†…å­˜: 24 GB

Selective AC + No Compile:
  é€Ÿåº¦: 5,186 tok/s/GPU (-10%)
  å†…å­˜: 18 GB (-25%)

Selective AC + Compile:
  é€Ÿåº¦: 6,000 tok/s/GPU (+4.1%)
  å†…å­˜: 18 GB (-25%)

ç»“è®ºï¼š
  AC + Compile å¯ä»¥å…¼å¾—ï¼š
  - å†…å­˜èŠ‚çœ 25%
  - é€Ÿåº¦ä»æœ‰æå‡
```

---

## 9. Memory Budget æ¨¡å¼

### 9.1 ä»€ä¹ˆæ˜¯ Memory Budgetï¼Ÿ

**Memory Budget** = è‡ªåŠ¨æœç´¢æœ€ä¼˜çš„ AC ç­–ç•¥ï¼Œç»™å®šå†…å­˜é¢„ç®—ã€‚

```
ä¼ ç»Ÿ AC:
  æ‰‹åŠ¨é€‰æ‹©ï¼šFull / Selective(2) / Selective(op)
  é—®é¢˜ï¼šä¸çŸ¥é“å“ªä¸ªæœ€ä¼˜

Memory Budget:
  è‡ªåŠ¨æœç´¢ï¼šåœ¨å†…å­˜é¢„ç®—å†…ï¼Œæ‰¾æœ€å¿«çš„ç­–ç•¥

ç¤ºä¾‹ï¼š
  å†…å­˜é¢„ç®— = 20 GB
  â†’ è‡ªåŠ¨å°è¯•ä¸åŒçš„ AC ç»„åˆ
  â†’ æ‰¾åˆ°æœ€ä¼˜ï¼šæŸäº›å±‚ Full ACï¼ŒæŸäº›å±‚ Selective
```

### 9.2 é…ç½®

```toml
[activation_checkpoint]
mode = "memory_budget"
memory_budget = 0.8  # 80% çš„å¯ç”¨å†…å­˜

# å¯è§†åŒ–æœç´¢è¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
visualize_memory_budget_pareto = true

# å¿…é¡»å¯ç”¨ compile
[compile]
enable = true
components = ["model"]
```

### 9.3 å·¥ä½œåŸç†

```python
# æ¥è‡ª: torchtitan/distributed/activation_checkpoint.py:311-321

if ac_config.mode == "memory_budget":
    # 1. å¿…é¡»å¯ç”¨ compileï¼ˆä¾èµ–ç¼–è¯‘å™¨åˆ†æï¼‰
    assert model_compile_enabled

    # 2. è®¾ç½®å†…å­˜é¢„ç®—
    torch._functorch.config.activation_memory_budget = ac_config.memory_budget

    # 3. ï¼ˆå¯é€‰ï¼‰å¯è§†åŒ– Pareto æ›²çº¿
    if ac_config.visualize_memory_budget_pareto:
        torch._functorch.config.visualize_memory_budget_pareto = True

    # Compile æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨ï¼š
    # - åˆ†ææ¯ä¸ªç®—å­çš„å†…å­˜å’Œè®¡ç®—ä»£ä»·
    # - æœç´¢æœ€ä¼˜çš„ checkpoint ç­–ç•¥
    # - ç”Ÿæˆæ»¡è¶³é¢„ç®—çš„æœ€å¿«ä»£ç 
```

### 9.4 Pareto æ›²çº¿

```
Memory Budget ä¼šç”Ÿæˆ Pareto æ›²çº¿ï¼š

Speed (tokens/sec)
  â–²
  â”‚                   â—‹ Full AC
  â”‚                  /
  â”‚                 /
  â”‚                â—‹ Selective (op)
  â”‚               /
  â”‚              /
  â”‚             â—‹ Selective (2)
  â”‚            /
  â”‚           /
  â”‚          â—‹ No AC
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Memory Usage (GB)
     10      15      20      25      30

ç»™å®šå†…å­˜é¢„ç®— 20 GB:
  â†’ é€‰æ‹© Selective (op)ï¼ˆåœ¨é¢„ç®—å†…ï¼Œé€Ÿåº¦æœ€å¿«ï¼‰
```

### 9.5 é€‚ç”¨åœºæ™¯

```
æ¨èä½¿ç”¨ Memory Budgetï¼š
âœ… å¤§è§„æ¨¡å®éªŒï¼ˆæœ‰æ—¶é—´æœç´¢ï¼‰
âœ… å›ºå®šç¡¬ä»¶ï¼ˆä¸€æ¬¡æœç´¢ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰
âœ… è¿½æ±‚æè‡´æ€§èƒ½

ä¸æ¨èï¼š
âŒ å¿«é€ŸåŸå‹ï¼ˆæœç´¢è€—æ—¶ï¼‰
âŒ ç¡¬ä»¶å¤šå˜ï¼ˆæ¯æ¬¡éƒ½è¦é‡æ–°æœç´¢ï¼‰
âŒ ç®€å•ä»»åŠ¡ï¼ˆSelective å·²è¶³å¤Ÿï¼‰
```

---

## 10. å®æˆ˜æ¡ˆä¾‹

### 10.1 Llama3 8B (8 GPUs)

**åœºæ™¯**ï¼šå•æœºè®­ç»ƒï¼Œå†…å­˜å……è¶³ã€‚

```toml
# é…ç½® 1: No ACï¼ˆBaselineï¼‰
[activation_checkpoint]
mode = "none"

# å†…å­˜å ç”¨: 24 GB
# é€Ÿåº¦: 5,762 tok/s/GPU
# Batch size: 2

# é…ç½® 2: Selective (Layer, N=2)
[activation_checkpoint]
mode = "selective"
selective_ac_option = "2"

# å†…å­˜å ç”¨: 18 GB (-25%)
# é€Ÿåº¦: 5,186 tok/s/GPU (-10%)
# Batch size: å¯å¢è‡³ 3 (+50%)

# é…ç½® 3: Selective (Op) - æ¨è
[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"

# å†…å­˜å ç”¨: 17 GB (-29%)
# é€Ÿåº¦: 5,300 tok/s/GPU (-8%)
# Batch size: å¯å¢è‡³ 3 (+50%)

# é…ç½® 4: Full AC
[activation_checkpoint]
mode = "full"

# å†…å­˜å ç”¨: 15 GB (-37.5%)
# é€Ÿåº¦: 4,610 tok/s/GPU (-20%)
# Batch size: å¯å¢è‡³ 4 (+100%)
```

**é€‰æ‹©å»ºè®®**ï¼š

```
å†…å­˜å……è¶³ (> 30 GB å¯ç”¨):
  â†’ mode = "selective", selective_ac_option = "2"
  â†’ æˆ–ä¸ä½¿ç”¨ AC

å†…å­˜ç´§å¼  (20-30 GB):
  â†’ mode = "selective", selective_ac_option = "op"

å†…å­˜éå¸¸ç´§å¼  (< 20 GB):
  â†’ mode = "full"
```

### 10.2 Llama3 70B (256 GPUs)

**åœºæ™¯**ï¼šå¤šæœºè®­ç»ƒï¼Œè¿½æ±‚ååã€‚

```toml
[parallelism]
data_parallel_shard_degree = 32
tensor_parallel_degree = 8

[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"  # Op SACï¼ˆæœ€ä¼˜å¹³è¡¡ï¼‰

[compile]
enable = true
components = ["model"]

# æ•ˆæœ:
# - å†…å­˜å ç”¨: 42 GB/GPUï¼ˆå¯æ¥å—ï¼‰
# - é€Ÿåº¦: æ¥è¿‘æ—  AC çš„ 90%
# - å¯è®­ç»ƒ seq_len = 8192
```

### 10.3 Llama3 405B (512 GPUs)

**åœºæ™¯**ï¼šè¶…å¤§æ¨¡å‹ï¼Œå†…å­˜æåº¦ç´§å¼ ã€‚

```toml
[parallelism]
data_parallel_shard_degree = 8
tensor_parallel_degree = 8
pipeline_parallel_degree = 8

[activation_checkpoint]
mode = "full"  # Full ACï¼ˆæœ€çœå†…å­˜ï¼‰

[compile]
enable = true

# æ•ˆæœ:
# - å†…å­˜å ç”¨: 70 GB/GPUï¼ˆå‹‰å¼ºæ”¾ä¸‹ï¼‰
# - é€Ÿåº¦: æ…¢ 20%ï¼ˆå¯æ¥å—ï¼‰
# - æ²¡æœ‰ AC æ ¹æœ¬æ— æ³•è®­ç»ƒ
```

### 10.4 è°ƒè¯•åœºæ™¯

```toml
# è°ƒè¯•æ—¶ï¼šå…³é—­ ACï¼ˆæ›´å¿«è¿­ä»£ï¼‰
[activation_checkpoint]
mode = "none"

# éªŒè¯æ•°å€¼æ­£ç¡®æ€§
[activation_checkpoint]
mode = "selective"
determinism_check = "deterministic"  # æ£€æŸ¥é‡ç®—æ˜¯å¦ä¸€è‡´

# Debug æ¨¡å¼ï¼ˆæ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰
[activation_checkpoint]
mode = "selective"
debug = true
```

---

## 11. è°ƒè¯•ä¸ä¼˜åŒ–

### 11.1 å¸¸è§é—®é¢˜

**Q1: AC åè®­ç»ƒå˜æ…¢äº†å¾ˆå¤š**

```
ç—‡çŠ¶:
  å¯ç”¨ AC åï¼Œé€Ÿåº¦æ…¢ > 30%

åŸå› :
1. ä½¿ç”¨äº† Full ACï¼ˆé¢„æœŸ 20% æ…¢ï¼‰
2. é‡ç®—ä»£ä»·å¾ˆé«˜çš„ç®—å­ï¼ˆå¦‚å¤šæ¬¡ recompute Attentionï¼‰
3. ä¸ torch.compile å†²çª

è§£å†³:
1. ä½¿ç”¨ Selective (op) è€Œä¸æ˜¯ Full
2. æ£€æŸ¥ op_sac_save_listï¼Œç¡®ä¿å…³é”®ç®—å­è¢«ä¿å­˜
3. å¯ç”¨ compile:
   [compile]
   enable = true
```

**Q2: å†…å­˜æ²¡æœ‰å‡å°‘**

```
ç—‡çŠ¶:
  å¯ç”¨ AC åï¼Œå†…å­˜å ç”¨æ²¡å˜åŒ–

åŸå› :
1. å…¶ä»–éƒ¨åˆ†å å†…å­˜ï¼ˆå‚æ•°ã€ä¼˜åŒ–å™¨ï¼‰
2. AC æ²¡æœ‰æ­£ç¡®åº”ç”¨
3. ä½¿ç”¨ Selective(1) ç›¸å½“äºä¸ç”¨ AC

æ£€æŸ¥:
1. å¯¹æ¯”æ¿€æ´»å€¼å†…å­˜ï¼ˆä¸æ˜¯æ€»å†…å­˜ï¼‰:
   device_memory_monitor.get_peak_stats()
2. ç¡®è®¤ AC æ¨¡å¼:
   logger.info(f"Applied {mode} AC")
3. è°ƒæ•´ selective_ac_option:
   "2" â†’ "4" æˆ– "full"
```

**Q3: æ•°å€¼ä¸ä¸€è‡´**

```
ç—‡çŠ¶:
  å¯ç”¨ AC åï¼Œloss ä¸åŒæˆ–è®­ç»ƒä¸ç¨³å®š

åŸå› :
1. RNG çŠ¶æ€ä¸ä¸€è‡´ï¼ˆDropout ç­‰ï¼‰
2. é‡ç®—æ—¶æ•°å€¼è¯¯å·®ç´¯ç§¯

è§£å†³:
1. å¯ç”¨ preserve_rng_state:
   [activation_checkpoint]
   preserve_rng_state = true
2. ä½¿ç”¨ç¡®å®šæ€§æ£€æŸ¥:
   determinism_check = "deterministic"
3. æ£€æŸ¥æ˜¯å¦æœ‰ in-place æ“ä½œ
```

**Q4: ä¸ FSDP/TP å†²çª**

```
ç—‡çŠ¶:
  AC + FSDP åå‡ºé”™æˆ–é€Ÿåº¦å¾ˆæ…¢

åŸå› :
1. AC åœ¨ FSDP ä¹‹å‰åº”ç”¨ï¼ˆé¡ºåºé”™è¯¯ï¼‰
2. AC åŒ…è£…äº†æ•´ä¸ªæ¨¡å‹ï¼ˆåº”è¯¥åŒ…è£… layerï¼‰

è§£å†³:
1. ç¡®ä¿é¡ºåºï¼š
   apply_tp() â†’ apply_ac() â†’ apply_compile() â†’ apply_fsdp()
2. åªåŒ…è£… TransformerBlock:
   for layer in model.layers:
       layer = checkpoint_wrapper(layer)
```

### 11.2 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**æŠ€å·§ 1: é€‰æ‹©åˆé€‚çš„ AC æ¨¡å¼**

```python
# å†³ç­–æ ‘
if memory_is_sufficient:
    mode = "selective"
    selective_ac_option = "2"  # æœ€å°é€Ÿåº¦æŸå¤±
elif memory_is_tight:
    mode = "selective"
    selective_ac_option = "op"  # æœ€ä¼˜å¹³è¡¡
else:  # memory_is_very_tight
    mode = "full"  # æœ€çœå†…å­˜
```

**æŠ€å·§ 2: å¾®è°ƒ op_sac_save_list**

```python
# æ·»åŠ è‡ªå®šä¹‰ç®—å­åˆ° save_list
_op_sac_save_list = {
    torch.ops.aten.mm.default,
    torch.ops.aten._scaled_dot_product_efficient_attention.default,
    # æ·»åŠ ï¼šå¦‚æœä½ çš„æ¨¡å‹æœ‰ç‰¹æ®Šçš„æ˜‚è´µç®—å­
    torch.ops.my_custom.expensive_op.default,
}

# ç§»é™¤ï¼šå¦‚æœæŸä¸ªç®—å­é‡ç®—å¾ˆå¿«
_op_sac_save_list = {
    # torch.ops.aten.mm.default,  # ç§»é™¤ mmï¼ˆå¦‚æœé‡ç®—å¿«ï¼‰
    ...
}
```

**æŠ€å·§ 3: ä¸ Batch Size è”åŠ¨**

```python
# æ²¡æœ‰ AC
batch_size = 2
activation_memory = 4 GB

# æœ‰ AC (èŠ‚çœ 50%)
batch_size = 3  # å¢å¤§ 50%
activation_memory = 3 GB (èŠ‚çœ 1 GB)
# æ€»åå: batch_size â†‘ 50%, é€Ÿåº¦ â†“ 10%
# å‡€æ”¶ç›Š: +35% ååï¼
```

**æŠ€å·§ 4: Profile æ¿€æ´»å€¼å†…å­˜**

```python
# å¼€å¯å†…å­˜ profiling
torch.cuda.memory._record_memory_history()

# è®­ç»ƒå‡ æ­¥
for i, batch in enumerate(dataloader):
    if i == 10:
        break
    loss = model(batch)
    loss.backward()

# æŸ¥çœ‹å†…å­˜å¿«ç…§
torch.cuda.memory._dump_snapshot("memory.pickle")

# åˆ†ææ¿€æ´»å€¼å æ¯”
# ä½¿ç”¨ PyTorch Memory Profiler Visualizer
```

### 11.3 ç›‘æ§æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**ï¼š

```python
# 1. æ¿€æ´»å€¼å†…å­˜å ç”¨
import torch.cuda

before_forward = torch.cuda.memory_allocated()
output = model(input)
after_forward = torch.cuda.memory_allocated()
activation_memory = after_forward - before_forward

logger.info(f"Activation memory: {activation_memory / 1e9:.2f} GB")

# 2. Recompute å¼€é”€
import time

start = time.time()
for _ in range(100):
    loss = model(input)
    loss.backward()
forward_backward_time = time.time() - start

logger.info(f"Forward+Backward time: {forward_backward_time:.2f}s")

# 3. é€Ÿåº¦æŸå¤±ç™¾åˆ†æ¯”
no_ac_speed = 5762  # tok/s/GPU (baseline)
with_ac_speed = 5186  # tok/s/GPU (with AC)
slowdown = (1 - with_ac_speed / no_ac_speed) * 100

logger.info(f"AC slowdown: {slowdown:.1f}%")
```

---

## 12. æ€»ç»“

### 12.1 AC çš„æ ¸å¿ƒæ€æƒ³

ç”¨**è‰ç¨¿çº¸**çš„æ¯”å–»æ€»ç»“ï¼š

1. **No AC**ï¼šè®°å½•æ‰€æœ‰æ­¥éª¤
   - âœ… æŸ¥é˜…å¿«ï¼ˆé€Ÿåº¦å¿«ï¼‰
   - âŒ ç¬”è®°å¤šï¼ˆå†…å­˜é«˜ï¼‰

2. **Full AC**ï¼šåªè®°èµ·ç‚¹
   - âœ… ç¬”è®°å°‘ï¼ˆå†…å­˜ä½ï¼‰
   - âŒ é‡æ–°æ¨å¯¼ï¼ˆé€Ÿåº¦æ…¢ 20%ï¼‰

3. **Selective (Layer)**ï¼šéš”å‡ æ­¥è®°å½•
   - âœ… ç¬”è®°ä¸­ç­‰ï¼ˆå†…å­˜ä¸­ç­‰ï¼‰
   - âœ… é‡æ–°æ¨å¯¼å°‘ï¼ˆé€Ÿåº¦æ…¢ 10%ï¼‰

4. **Selective (Op)**ï¼šåªè®°é‡è¦çš„
   - âœ… ç¬”è®°å°‘ï¼ˆå†…å­˜ä½ï¼‰
   - âœ… é‡æ–°æ¨å¯¼å°‘ï¼ˆé€Ÿåº¦æ…¢ 12%ï¼‰
   - **æœ€ä¼˜å¹³è¡¡**

### 12.2 é€‰æ‹©å»ºè®®

```
åœºæ™¯ 1: å°æ¨¡å‹ (< 10B)ï¼Œå†…å­˜å……è¶³
  â†’ mode = "none" æˆ– "selective", option = "2"

åœºæ™¯ 2: ä¸­æ¨¡å‹ (10B-70B)ï¼Œæ ‡å‡†è®­ç»ƒ
  â†’ mode = "selective", option = "op"  ï¼ˆæ¨èï¼‰

åœºæ™¯ 3: å¤§æ¨¡å‹ (> 70B)ï¼Œå†…å­˜ç´§å¼ 
  â†’ mode = "full"

åœºæ™¯ 4: ç ”ç©¶ä¼˜åŒ–ï¼Œè¿½æ±‚æè‡´
  â†’ mode = "memory_budget"
```

### 12.3 é…ç½®é€ŸæŸ¥

```toml
# æ¨èé…ç½®ï¼ˆé€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼‰
[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"
preserve_rng_state = true

# è°ƒè¯•é…ç½®
[activation_checkpoint]
mode = "selective"
selective_ac_option = "op"
determinism_check = "deterministic"
debug = true

# æè‡´å†…å­˜ä¼˜åŒ–
[activation_checkpoint]
mode = "full"
preserve_rng_state = true

# å…³é—­ ACï¼ˆè°ƒè¯•æ—¶ï¼‰
[activation_checkpoint]
mode = "none"
```

### 12.4 ä¸å…¶ä»–æŠ€æœ¯çš„å…³ç³»

```
FSDP:
  åˆ‡åˆ†å‚æ•°ï¼ŒèŠ‚çœå‚æ•°å†…å­˜

AC:
  ä¸¢å¼ƒæ¿€æ´»ï¼ŒèŠ‚çœæ¿€æ´»å†…å­˜

FSDP + AC:
  â†’ å¯è®­ç»ƒ 2-4x æ›´å¤§çš„æ¨¡å‹
  â†’ æˆ– 2-4x æ›´å¤§çš„ batch

Compile:
  åŠ é€Ÿè®¡ç®—

AC + Compile:
  â†’ å¯å…¼å¾—ï¼ˆéœ€è¦æ­£ç¡®é…ç½®ï¼‰
  â†’ TorchTitan å·²ä¼˜åŒ–é›†æˆ
```

### 12.5 å…³é”®æºç 

```
æ ¸å¿ƒæ–‡ä»¶:
- torchtitan/distributed/activation_checkpoint.py
  - apply_ac: å…¥å£å‡½æ•°
  - _apply_full_ac: Full AC
  - _apply_layer_sac: Layer SAC
  - _apply_op_sac: Operator SAC

é…ç½®:
- torchtitan/config/job_config.py:585-613
  - ActivationCheckpoint é…ç½®ç±»

åº”ç”¨:
- torchtitan/models/llama3/infra/parallelize.py:96-104
  - åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶åº”ç”¨ AC
```

---

## 13. å‚è€ƒèµ„æ–™

**æºç æ–‡ä»¶**ï¼š
- `torchtitan/distributed/activation_checkpoint.py` - AC å®ç°
- `torchtitan/config/job_config.py:585-613` - AC é…ç½®
- `torchtitan/models/llama3/infra/parallelize.py:34-44` - Op SAC save list

**PyTorch å®˜æ–¹æ–‡æ¡£**ï¼š
- [Checkpoint](https://pytorch.org/docs/stable/checkpoint.html)
- [Activation Checkpointing](https://pytorch.org/docs/stable/distributed.algorithms.html#activation-checkpointing)
- [Memory Efficient Training](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)

**ç›¸å…³æ–‡æ¡£**ï¼š
- [01_fsdp2_per_parameter_sharding.md](./01_fsdp2_per_parameter_sharding.md) - FSDP2 å®ç°
- [02_tensor_parallel_implementation.md](./02_tensor_parallel_implementation.md) - TP å®ç°

**å­¦æœ¯è®ºæ–‡**ï¼š
- Training Deep Nets with Sublinear Memory Cost (Gradient Checkpointing)
- Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization

---

**æœ€åæ›´æ–°**ï¼š2025å¹´1æœˆ

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0
