# Distributed Checkpointing åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹è¯¦è§£

## ç›®å½•
- [1. ä»€ä¹ˆæ˜¯ Distributed Checkpointingï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-distributed-checkpointing)
- [2. æ¬æ¡Œå­çš„æ¯”å–»ï¼šæ‹ç…§å­˜æ¡£](#2-æ¬æ¡Œå­çš„æ¯”å–»æ‹ç…§å­˜æ¡£)
- [3. DCP vs ä¼ ç»Ÿ Checkpoint](#3-dcp-vs-ä¼ ç»Ÿ-checkpoint)
- [4. Async Checkpoint ä¸‰ç§æ¨¡å¼](#4-async-checkpoint-ä¸‰ç§æ¨¡å¼)
- [5. æºç å®ç°è¯¦è§£](#5-æºç å®ç°è¯¦è§£)
- [6. State Dict ç®¡ç†](#6-state-dict-ç®¡ç†)
- [7. HuggingFace æ ¼å¼æ”¯æŒ](#7-huggingface-æ ¼å¼æ”¯æŒ)
- [8. ä¸å¹¶è¡Œç­–ç•¥çš„é…åˆ](#8-ä¸å¹¶è¡Œç­–ç•¥çš„é…åˆ)

---

## 1. ä»€ä¹ˆæ˜¯ Distributed Checkpointingï¼Ÿ

### 1.1 åŸºæœ¬æ¦‚å¿µ

**Distributed Checkpointing (DCP)** = åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ª GPU åªä¿å­˜è‡ªå·±çš„é‚£éƒ¨åˆ†å‚æ•°ï¼Œè€Œä¸æ˜¯æ¯ä¸ª GPU éƒ½ä¿å­˜å®Œæ•´çš„æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°±åƒ FSDP åœ¨è®­ç»ƒæ—¶åˆ‡åˆ†å‚æ•°ä¸€æ ·ï¼Œcheckpoint æ—¶ä¹Ÿåˆ‡åˆ†ä¿å­˜ã€‚

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ DCPï¼Ÿ

ä¼ ç»Ÿçš„ checkpoint æœ‰ä¸¤ä¸ªå¤§é—®é¢˜ï¼š

```
é—®é¢˜ 1: å†…å­˜çˆ†ç‚¸
å‡è®¾ Llama3 70B æ¨¡å‹ï¼Œbf16ï¼Œ8 GPUs è®­ç»ƒ

ä¼ ç»Ÿæ–¹å¼ï¼ˆæ¯ä¸ª GPU éƒ½ä¿å­˜å®Œæ•´æ¨¡å‹ï¼‰ï¼š
GPU 0: æ”¶é›†æ‰€æœ‰å‚æ•° â†’ 140 GB â†’ ä¿å­˜åˆ°ç£ç›˜
GPU 1: æ”¶é›†æ‰€æœ‰å‚æ•° â†’ 140 GB â†’ ä¿å­˜åˆ°ç£ç›˜
...
GPU 7: æ”¶é›†æ‰€æœ‰å‚æ•° â†’ 140 GB â†’ ä¿å­˜åˆ°ç£ç›˜

é—®é¢˜ï¼š
- âŒ æ¯ä¸ª GPU éœ€è¦ä¸´æ—¶åˆ†é… 140 GB å†…å­˜ï¼ˆOOMï¼ï¼‰
- âŒ 8 ä¸ª GPU ä¿å­˜ 8 ä»½é‡å¤çš„æ–‡ä»¶ï¼ˆæµªè´¹ï¼ï¼‰
```

```
é—®é¢˜ 2: é€Ÿåº¦æ…¢
å•ä¸ª GPU ä¿å­˜ 140 GB åˆ°ç£ç›˜éœ€è¦å¾ˆé•¿æ—¶é—´
å¦‚æœæ˜¯åŒæ­¥ä¿å­˜ï¼Œè®­ç»ƒä¼šè¢«é˜»å¡ï¼

æ—¶é—´çº¿ï¼ˆåŒæ­¥ä¿å­˜ï¼‰ï¼š
Training â†’ [Pause] â†’ GPU 0 ä¿å­˜ 140GB (5-10 minutes) â†’ [Resume]
                                  â†‘
                          è®­ç»ƒæš‚åœï¼ŒGPU é—²ç½®ï¼
```

**DCP çš„è§£å†³æ–¹æ¡ˆ**ï¼š

```
DCP æ–¹å¼ï¼š
GPU 0: åªä¿å­˜è‡ªå·±çš„ 1/8 å‚æ•° â†’ 17.5 GB
GPU 1: åªä¿å­˜è‡ªå·±çš„ 1/8 å‚æ•° â†’ 17.5 GB
...
GPU 7: åªä¿å­˜è‡ªå·±çš„ 1/8 å‚æ•° â†’ 17.5 GB

æ‰€æœ‰ GPU å¹¶è¡Œä¿å­˜ï¼

å¥½å¤„ï¼š
âœ… æ¯ä¸ª GPU åªéœ€ä¸´æ—¶åˆ†é… 17.5 GBï¼ˆä¸ä¼š OOMï¼‰
âœ… 8 ä¸ª GPU å¹¶è¡Œä¿å­˜ï¼Œé€Ÿåº¦å¿« 8 å€
âœ… ç£ç›˜æ€»å…±åªå­˜ä¸€ä»½æ¨¡å‹ï¼ˆèŠ‚çœç©ºé—´ï¼‰
```

### 1.3 Checkpoint åŒ…å«ä»€ä¹ˆï¼Ÿ

ä¸€ä¸ªå®Œæ•´çš„ checkpoint åŒ…å« 5 ä¸ªéƒ¨åˆ†ï¼š

```
checkpoint/step-1000/
â”œâ”€â”€ __0_0.distcp          â† GPU 0 çš„ model å‚æ•°åˆ†ç‰‡
â”œâ”€â”€ __1_0.distcp          â† GPU 1 çš„ model å‚æ•°åˆ†ç‰‡
â”œâ”€â”€ ...
â”œâ”€â”€ __7_0.distcp          â† GPU 7 çš„ model å‚æ•°åˆ†ç‰‡
â”œâ”€â”€ __0_optimizer_0.distcp â† GPU 0 çš„ optimizer çŠ¶æ€
â”œâ”€â”€ ...
â”œâ”€â”€ __7_optimizer_0.distcp â† GPU 7 çš„ optimizer çŠ¶æ€
â”œâ”€â”€ __0_lr_scheduler.distcp â† LR scheduler çŠ¶æ€
â”œâ”€â”€ __0_dataloader.distcp   â† DataLoader çŠ¶æ€ï¼ˆå½“å‰ä½ç½®ï¼‰
â”œâ”€â”€ __0_train_state.distcp  â† è®­ç»ƒçŠ¶æ€ï¼ˆstepã€ntokens_seenï¼‰
â””â”€â”€ .metadata              â† å…ƒæ•°æ®ï¼ˆå‘Šè¯‰ DCP å¦‚ä½•é‡å»ºï¼‰
```

**5 ä¸ªç»„ä»¶**ï¼š

1. **Model** (`model`): æ¨¡å‹å‚æ•°ï¼ˆweightsï¼‰
2. **Optimizer** (`optimizer`): ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆmomentumã€varianceç­‰ï¼‰
3. **LR Scheduler** (`lr_scheduler`): å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
4. **DataLoader** (`dataloader`): æ•°æ®åŠ è½½å™¨çŠ¶æ€ï¼ˆå½“å‰è¯»åˆ°å“ªäº†ï¼‰
5. **Train State** (`train_state`): è®­ç»ƒçŠ¶æ€ï¼ˆå½“å‰æ­¥æ•°ã€è§è¿‡å¤šå°‘ tokensï¼‰

---

## 2. æ¬æ¡Œå­çš„æ¯”å–»ï¼šæ‹ç…§å­˜æ¡£

### 2.1 å›é¡¾æ¬æ¡Œå­çš„åœºæ™¯

è¿˜è®°å¾—æˆ‘ä»¬ç”¨æ¬æ¡Œå­æ¯”å–»å¹¶è¡Œè®­ç»ƒå—ï¼Ÿï¼ˆ[FSDP æ–‡æ¡£](./01_fsdp2_per_parameter_sharding.md)ï¼‰

```
æˆ¿å­ï¼ˆæ¨¡å‹ï¼‰é‡Œæœ‰å¾ˆå¤šæ¡Œå­ï¼ˆå‚æ•°ï¼‰ï¼š
TransformerBlock 0:
  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ wq   â”‚ â”‚ wk   â”‚ â”‚ wv   â”‚ â”‚ wo   â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ w1   â”‚ â”‚ w2   â”‚ â”‚ w3   â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜

FSDP åˆ‡åˆ†ï¼šæ¯å¼ æ¡Œå­åˆ‡æˆ 4 ä»½ï¼ˆFSDP=4ï¼‰
GPU 0: æ¯å¼ æ¡Œå­çš„ç¬¬ 1 å—
GPU 1: æ¯å¼ æ¡Œå­çš„ç¬¬ 2 å—
GPU 2: æ¯å¼ æ¡Œå­çš„ç¬¬ 3 å—
GPU 3: æ¯å¼ æ¡Œå­çš„ç¬¬ 4 å—
```

### 2.2 Checkpoint = æ‹ç…§å­˜æ¡£

**åœºæ™¯**ï¼šä½ å’Œæœ‹å‹ä»¬æ­£åœ¨æ¬å®¶å…·ï¼ˆè®­ç»ƒæ¨¡å‹ï¼‰ï¼Œçªç„¶éœ€è¦ä¼‘æ¯ä¸€ä¸‹ï¼Œæ˜å¤©ç»§ç»­ã€‚

**æ€ä¹ˆè®°ä½å½“å‰è¿›åº¦ï¼Ÿ** â†’ æ‹ç…§å­˜æ¡£ï¼

### 2.3 ä¼ ç»Ÿæ–¹å¼ï¼šå®Œæ•´æ‹ç…§ï¼ˆé—®é¢˜å¤šå¤šï¼‰

```
ä¼ ç»Ÿ Checkpointï¼ˆéåˆ†å¸ƒå¼ï¼‰ï¼š

æ­¥éª¤ 1: æ”¶é›†æ‰€æœ‰æ¡Œå­ç¢ç‰‡
GPU 0: æŠŠæˆ‘çš„ç¢ç‰‡ç»™ GPU 0
GPU 1: æŠŠæˆ‘çš„ç¢ç‰‡ç»™ GPU 0
GPU 2: æŠŠæˆ‘çš„ç¢ç‰‡ç»™ GPU 0
GPU 3: æŠŠæˆ‘çš„ç¢ç‰‡ç»™ GPU 0
       â†“
GPU 0: æ‹¼æˆå®Œæ•´çš„æˆ¿å­

æ­¥éª¤ 2: GPU 0 æ‹ç…§ä¿å­˜
GPU 0: [å’”åš“] â†’ ä¿å­˜åˆ°ç›¸å†Œï¼ˆç£ç›˜ï¼‰

é—®é¢˜ï¼š
âŒ GPU 0 éœ€è¦ä¸´æ—¶å­˜å‚¨å®Œæ•´çš„æˆ¿å­ï¼ˆå†…å­˜çˆ†ç‚¸ï¼‰
âŒ å…¶ä»– GPU é—²ç½®ç­‰å¾…ï¼ˆæµªè´¹æ—¶é—´ï¼‰
âŒ GPU 0 æ‹ç…§å¾ˆæ…¢ï¼ˆå•çº¿ç¨‹ IOï¼‰
```

### 2.4 DCP æ–¹å¼ï¼šåˆ†ç‰‡æ‹ç…§ï¼ˆé«˜æ•ˆï¼‰

```
Distributed Checkpointï¼ˆåˆ†å¸ƒå¼ï¼‰ï¼š

æ­¥éª¤ 1: æ¯äººæ‹è‡ªå·±çš„éƒ¨åˆ†
GPU 0: [å’”åš“] æ‹æˆ‘è´Ÿè´£çš„æ¡Œå­ç¢ç‰‡ â†’ photo_0.jpg
GPU 1: [å’”åš“] æ‹æˆ‘è´Ÿè´£çš„æ¡Œå­ç¢ç‰‡ â†’ photo_1.jpg
GPU 2: [å’”åš“] æ‹æˆ‘è´Ÿè´£çš„æ¡Œå­ç¢ç‰‡ â†’ photo_2.jpg
GPU 3: [å’”åš“] æ‹æˆ‘è´Ÿè´£çš„æ¡Œå­ç¢ç‰‡ â†’ photo_3.jpg

æ‰€æœ‰äººåŒæ—¶æ‹ç…§ï¼ï¼ˆå¹¶è¡Œä¿å­˜ï¼‰

æ­¥éª¤ 2: å­˜æ¡£ç®¡ç†å™¨è®°å½•æ‹¼å›¾æ–¹æ³•
ç®¡ç†å™¨: è®°å½• {
    GPU 0 çš„ç…§ç‰‡ â†’ æ¡Œå­çš„ç¬¬ 1/4 éƒ¨åˆ†
    GPU 1 çš„ç…§ç‰‡ â†’ æ¡Œå­çš„ç¬¬ 2/4 éƒ¨åˆ†
    GPU 2 çš„ç…§ç‰‡ â†’ æ¡Œå­çš„ç¬¬ 3/4 éƒ¨åˆ†
    GPU 3 çš„ç…§ç‰‡ â†’ æ¡Œå­çš„ç¬¬ 4/4 éƒ¨åˆ†
}
â†’ ä¿å­˜ä¸º .metadata

æ¢å¤æ—¶ï¼š
è¯»å– .metadata â†’ çŸ¥é“æ¯å¼ ç…§ç‰‡å¯¹åº”å“ªéƒ¨åˆ†
æ¯ä¸ª GPU è¯»å–è‡ªå·±çš„ç…§ç‰‡ â†’ æ¢å¤è‡ªå·±è´Ÿè´£çš„ç¢ç‰‡
ç»§ç»­æ¬æ¡Œå­ï¼

å¥½å¤„ï¼š
âœ… æ¯äººåªæ‹è‡ªå·±çš„éƒ¨åˆ†ï¼ˆå†…å­˜å ç”¨ä½ï¼‰
âœ… å¹¶è¡Œæ‹ç…§ï¼ˆé€Ÿåº¦å¿«ï¼‰
âœ… æ€»å…±åªå­˜ä¸€å¥—ç…§ç‰‡ï¼ˆèŠ‚çœç©ºé—´ï¼‰
```

### 2.5 è¿›ä¸€æ­¥çš„æ¯”å–»ï¼šOptimizer = å·¥å…·ç®±

è®­ç»ƒä¸åªæœ‰æ¡Œå­ï¼ˆå‚æ•°ï¼‰ï¼Œè¿˜æœ‰ï¼š

```
1. æ¡Œå­ï¼ˆModelï¼‰ï¼šå®¶å…·æœ¬èº«
   GPU 0: wqçš„1/4, wkçš„1/4, wvçš„1/4, ...

2. å·¥å…·ç®±ï¼ˆOptimizerï¼‰ï¼šæ¬æ¡Œå­ç”¨çš„å·¥å…·
   - åŠ¨é‡ (momentum): æ‰‹æ¨è½¦çš„é€Ÿåº¦
   - æ–¹å·® (variance): æ‰‹æ¨è½¦çš„æ–¹å‘è°ƒæ•´
   GPU 0: wqçš„å·¥å…·ç®±1/4, wkçš„å·¥å…·ç®±1/4, ...

3. è¯´æ˜ä¹¦ï¼ˆLR Schedulerï¼‰ï¼šæ¬å®¶è®¡åˆ’
   - å½“å‰å­¦ä¹ ç‡ï¼šæ¬æ¡Œå­çš„åŠ›åº¦
   - è°ƒåº¦çŠ¶æ€ï¼šæ¬å®¶è¿›åº¦

4. æ¬å®¶æ¸…å•ï¼ˆDataLoaderï¼‰ï¼š
   - å½“å‰ä½ç½®ï¼šæ¬åˆ°ç¬¬ 1000 ä¸ªæ•°æ®äº†

5. è¿›åº¦è®°å½•ï¼ˆTrain Stateï¼‰ï¼š
   - å½“å‰æ­¥æ•°ï¼šç¬¬ 500 æ­¥
   - æ€»å·¥ä½œé‡ï¼šå¤„ç†äº† 2M tokens
```

**Checkpoint å°±æ˜¯æŠŠè¿™ 5 æ ·ä¸œè¥¿éƒ½æ‹ç…§å­˜æ¡£**ï¼

---

## 3. DCP vs ä¼ ç»Ÿ Checkpoint

### 3.1 å¯¹æ¯”è¡¨

| ç‰¹æ€§ | ä¼ ç»Ÿ Checkpoint | Distributed Checkpoint (DCP) |
|-----|----------------|------------------------------|
| **å†…å­˜å ç”¨** | æ¯ä¸ª GPU éœ€è¦å®Œæ•´æ¨¡å‹ | æ¯ä¸ª GPU åªéœ€è‡ªå·±çš„åˆ†ç‰‡ |
| **ä¿å­˜é€Ÿåº¦** | ä¸²è¡Œï¼Œæ…¢ | å¹¶è¡Œï¼Œå¿« 8x-100x |
| **ç£ç›˜å ç”¨** | N ä¸ª GPU å¯èƒ½ä¿å­˜ N ä»½ | åªä¿å­˜ 1 ä»½ï¼ˆåˆ†ç‰‡å­˜å‚¨ï¼‰ |
| **å¯æ‰©å±•æ€§** | å—é™äºå• GPU å†…å­˜ | å¯æ‰©å±•åˆ°ä»»æ„å¤§æ¨¡å‹ |
| **åŠ è½½æ–¹å¼** | æ‰€æœ‰ GPU è¯»å–ç›¸åŒæ–‡ä»¶ | æ¯ä¸ª GPU è¯»å–è‡ªå·±çš„åˆ†ç‰‡ |
| **ä¸å¹¶è¡Œç­–ç•¥** | éœ€è¦æ‰‹åŠ¨å¤„ç† | è‡ªåŠ¨å¤„ç† FSDP/TP/PP |

### 3.2 å†…å­˜å ç”¨å¯¹æ¯”

å‡è®¾ Llama3 70Bï¼Œbf16ï¼Œ8 GPUsï¼š

```
ä¼ ç»Ÿ Checkpoint:
  æ¨¡å‹å‚æ•°: 70B Ã— 2 bytes = 140 GB
  ä¼˜åŒ–å™¨çŠ¶æ€: 140 GB Ã— 2 (Adam) = 280 GB
  æ€»è®¡: 420 GB

  ä¿å­˜æ—¶æ¯ä¸ª GPU éœ€è¦ï¼š
  - è®­ç»ƒæ—¶åˆ†ç‰‡: 52.5 GB (420GB / 8)
  - æ”¶é›†å®Œæ•´çŠ¶æ€: + 420 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  å³°å€¼: 472.5 GB  ğŸ˜± (OOM!)

DCP:
  ä¿å­˜æ—¶æ¯ä¸ª GPU éœ€è¦ï¼š
  - è®­ç»ƒæ—¶åˆ†ç‰‡: 52.5 GB
  - ä¸´æ—¶æ‹·è´ï¼ˆasyncï¼‰: + 52.5 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  å³°å€¼: 105 GB  âœ… (å¯è¡Œ!)

èŠ‚çœ: 367.5 GB / GPU
```

### 3.3 ä¿å­˜é€Ÿåº¦å¯¹æ¯”

å‡è®¾æ¯ä¸ª GPU å†™å…¥å¸¦å®½ = 1 GB/sï¼š

```
ä¼ ç»Ÿ Checkpoint (å• GPU ä¿å­˜):
  æ—¶é—´ = 420 GB / 1 GB/s = 420 seconds = 7 minutes

DCP (8 GPUs å¹¶è¡Œ):
  æ—¶é—´ = 52.5 GB / 1 GB/s = 52.5 seconds

åŠ é€Ÿ: 8x
```

å®é™…ä¸Šï¼ŒDCP è¿˜æœ‰**å¼‚æ­¥ä¿å­˜**ï¼Œè®­ç»ƒå¯ä»¥ç»§ç»­è¿›è¡Œï¼

---

## 4. Async Checkpoint ä¸‰ç§æ¨¡å¼

### 4.1 é—®é¢˜ï¼šCheckpoint é˜»å¡è®­ç»ƒ

å³ä½¿æ˜¯ DCPï¼Œå¦‚æœ**åŒæ­¥ä¿å­˜**ï¼Œè®­ç»ƒè¿˜æ˜¯ä¼šè¢«é˜»å¡ï¼š

```
åŒæ­¥ä¿å­˜ï¼ˆDisabledï¼‰:
Step 495: Training... âœ…
Step 496: Training... âœ…
Step 497: Training... âœ…
Step 498: Training... âœ…
Step 499: Training... âœ…
Step 500: [Checkpoint!]
    â””â”€ æš‚åœè®­ç»ƒ
    â””â”€ æ‹·è´å‚æ•°åˆ° CPU/Staging
    â””â”€ å†™å…¥ç£ç›˜ (52 seconds)
    â””â”€ æ¢å¤è®­ç»ƒ
Step 501: Training... (ç»ˆäºç»§ç»­äº†)

æµªè´¹æ—¶é—´: 52 ç§’ Ã— è®­ç»ƒé¢‘ç‡
```

**è§£å†³æ–¹æ¡ˆ**ï¼š**Async Checkpoint** - ä¿å­˜çš„åŒæ—¶ç»§ç»­è®­ç»ƒï¼

### 4.2 ä¸‰ç§æ¨¡å¼

TorchTitan æ”¯æŒ 3 ç§ checkpoint æ¨¡å¼ï¼š

```python
# æ¥è‡ª: torchtitan/config/job_config.py:525-541

async_mode: Literal["disabled", "async", "async_with_pinned_mem"] = "disabled"
```

#### æ¨¡å¼ 1: Disabledï¼ˆåŒæ­¥ä¿å­˜ï¼‰

```
Timeline:
Training â†’ [Pause] â†’ Copy â†’ Save â†’ [Resume] â†’ Training

ç‰¹ç‚¹:
- è®­ç»ƒæš‚åœï¼Œç­‰å¾…ä¿å­˜å®Œæˆ
- æœ€ç®€å•ï¼Œæœ€å¯é 
- æ…¢ï¼Œé˜»å¡è®­ç»ƒ

é€‚ç”¨ï¼š
- è°ƒè¯•ã€å°æ¨¡å‹
- ä¿å­˜é¢‘ç‡ä½ï¼ˆå¦‚æ¯ 5000 æ­¥ï¼‰
```

#### æ¨¡å¼ 2: Asyncï¼ˆå¼‚æ­¥ä¿å­˜ï¼‰

```
Timeline:
Training â†’ [Copy to CPU] â†’ Training (continues)
                â†“
           [Save in background]

åŸç†:
1. æ‹·è´ GPU tensor åˆ° CPU (å¿«ï¼Œå‡ ç§’é’Ÿ)
2. è®­ç»ƒç»§ç»­
3. åå°çº¿ç¨‹æŠŠ CPU tensor å†™å…¥ç£ç›˜

ç‰¹ç‚¹:
- âœ… è®­ç»ƒå‡ ä¹ä¸é˜»å¡ï¼ˆåªæœ‰æ‹·è´æ—¶é—´ï¼‰
- âœ… ç®€å•çš„å¼‚æ­¥å®ç°
- âŒ éœ€è¦é¢å¤– CPU å†…å­˜ï¼ˆå­˜å‚¨æ‹·è´ï¼‰
- âŒ æ‹·è´æœ¬èº«è¿˜æ˜¯æœ‰å¼€é”€

é€‚ç”¨ï¼š
- ä¸­ç­‰é¢‘ç‡ä¿å­˜ï¼ˆæ¯ 500-1000 æ­¥ï¼‰
- CPU å†…å­˜å……è¶³
```

#### æ¨¡å¼ 3: Async with Pinned Memoryï¼ˆæœ€å¿«ï¼‰

```
Timeline:
Training â†’ [Stage to Pinned Mem] â†’ Training (continues)
                â†“
           [Upload via multiprocess]

åŸç†:
1. æ‹·è´ GPU tensor åˆ° Pinned Memory (è¶…å¿«ï¼ŒDMA)
2. è®­ç»ƒç»§ç»­
3. ç‹¬ç«‹è¿›ç¨‹é€šè¿‡ Pinned Memory ä¸Šä¼ åˆ°ç£ç›˜

ç‰¹ç‚¹:
- âœ… è®­ç»ƒå‡ ä¹é›¶é˜»å¡
- âœ… æ‹·è´é€Ÿåº¦æœ€å¿«ï¼ˆDMAï¼‰
- âœ… ç‹¬ç«‹è¿›ç¨‹ä¸Šä¼ ï¼Œå®Œå…¨ä¸å½±å“è®­ç»ƒ
- âŒ éœ€è¦ Pinned Memoryï¼ˆGPUå¯å¯»å€çš„CPUå†…å­˜ï¼‰
- âŒ å®ç°å¤æ‚

é€‚ç”¨ï¼š
- é«˜é¢‘ä¿å­˜ï¼ˆæ¯ 100-500 æ­¥ï¼‰
- å¤§æ¨¡å‹è®­ç»ƒ
- è¿½æ±‚æè‡´æ€§èƒ½
```

### 4.3 æ—¶é—´å¼€é”€å¯¹æ¯”

å‡è®¾ 52.5 GB å‚æ•°ï¼ˆLlama3 70B / 8 GPUsï¼‰ï¼š

```
Disabled:
  Copy: 0 (ç›´æ¥å†™)
  Save: 52.5s (é˜»å¡è®­ç»ƒ)
  â”€â”€â”€â”€â”€â”€
  è®­ç»ƒæš‚åœ: 52.5s

Async:
  Copy GPUâ†’CPU: 5s (é˜»å¡è®­ç»ƒ)
  Save: 52.5s (åå°)
  â”€â”€â”€â”€â”€â”€
  è®­ç»ƒæš‚åœ: 5s

Async with Pinned Mem:
  Copy GPUâ†’Pinned: 2s (é˜»å¡è®­ç»ƒï¼ŒDMA)
  Save: 52.5s (ç‹¬ç«‹è¿›ç¨‹)
  â”€â”€â”€â”€â”€â”€
  è®­ç»ƒæš‚åœ: 2s

æ•ˆæœå¯¹æ¯”:
  Disabled: 52.5s æš‚åœ
  Async: 5s æš‚åœ (â†“ 90%)
  Async+Pinned: 2s æš‚åœ (â†“ 96%)
```

### 4.4 é…ç½®ç¤ºä¾‹

```toml
# æ¨¡å¼ 1: Disabledï¼ˆåŒæ­¥ï¼‰
[checkpoint]
enable = true
interval = 5000
async_mode = "disabled"

# æ¨¡å¼ 2: Async
[checkpoint]
enable = true
interval = 1000
async_mode = "async"

# æ¨¡å¼ 3: Async with Pinned Memoryï¼ˆæ¨èå¤§æ¨¡å‹ï¼‰
[checkpoint]
enable = true
interval = 500
async_mode = "async_with_pinned_mem"
```

---

## 5. æºç å®ç°è¯¦è§£

### 5.1 CheckpointManager æ ¸å¿ƒæ¶æ„

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:118-175

class CheckpointManager:
    """ç®¡ç† TorchTitan çš„ checkpointing é€»è¾‘"""

    def __init__(
        self,
        dataloader: BaseDataLoader,
        model_parts: list[nn.Module],
        optimizers: OptimizersContainer,
        lr_schedulers: LRSchedulersContainer,
        states: dict[str, Any],  # é¢å¤–çš„çŠ¶æ€ï¼ˆå¦‚ train_stateï¼‰
        checkpoint_config: CheckpointConfig,
        sd_adapter: BaseStateDictAdapter | None,  # HF æ ¼å¼è½¬æ¢å™¨
        ft_manager: FTManager | None = None,  # Fault Tolerance
    ):
        # 1. åŒ…è£…æ¨¡å‹ä¸º ModelWrapper
        self.states = {
            MODEL: ModelWrapper(model_parts),  # æ”¯æŒå¤šä¸ª model parts (PP)
            OPTIMIZER: optimizers,
            LR_SCHEDULER: lr_schedulers,
            DATALOADER: dataloader,
            **states,  # train_state ç­‰
        }

        # 2. é…ç½® Async æ¨¡å¼
        self.async_mode = AsyncMode[checkpoint_config.async_mode.upper()]

        # 3. é…ç½® Pinned Memory Stager
        if self.async_mode == AsyncMode.ASYNC_WITH_PINNED_MEM:
            self.stager = DefaultStager(
                StagingOptions(
                    use_pinned_memory=True,  # ä½¿ç”¨ Pinned Memory
                    use_separate_process=True,  # ç‹¬ç«‹è¿›ç¨‹ä¸Šä¼ 
                )
            )

        # 4. é…ç½®æ¸…ç†ç­–ç•¥
        self.keep_latest_k = checkpoint_config.keep_latest_k
        if self.keep_latest_k > 0:
            self.purge_thread = threading.Thread(
                target=purge_thread,  # åå°åˆ é™¤æ—§ checkpoint
                daemon=True,
            )
```

**å…³é”®ç»„ä»¶**ï¼š

1. **ModelWrapper**ï¼šåŒ…è£…æ¨¡å‹ï¼Œæ”¯æŒ Pipeline Parallel çš„å¤š model parts
2. **States dict**ï¼šç»Ÿä¸€ç®¡ç† 5 ä¸ªç»„ä»¶çš„çŠ¶æ€
3. **Stager**ï¼šPinned Memory ç®¡ç†å™¨
4. **Purge thread**ï¼šåå°æ¸…ç†çº¿ç¨‹

### 5.2 Save æµç¨‹

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:468-541

@torch.no_grad()
def save(self, curr_step: int, last_step: bool = False) -> None:
    """ä¿å­˜ checkpoint"""

    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
    if not self._should_save(curr_step, last_step):
        return

    # 2. ç­‰å¾…ä¸Šä¸€æ¬¡å¼‚æ­¥ä¿å­˜å®Œæˆ
    self._async_wait()

    # 3. åˆ›å»º checkpoint ID
    checkpoint_id = self._create_checkpoint_id(curr_step)
    # checkpoint_id = "outputs/checkpoint/step-500"

    # 4. è·å–è¦ä¿å­˜çš„çŠ¶æ€
    states = self._flattened_model_states_sd()
    # states = {
    #     "model.layers.0.attention.wq": DTensor(...),
    #     "optimizer": {...},
    #     "lr_scheduler": {...},
    #     ...
    # }

    # 5. æ ¹æ®æ¨¡å¼ä¿å­˜
    if self.async_mode == AsyncMode.ASYNC_WITH_PINNED_MEM:
        # æ¨¡å¼ 3: Async + Pinned Memory
        result = self.dcp_save(
            states,
            checkpoint_id=checkpoint_id,
            async_mode=self.async_mode,
        )
        self.save_future = result.upload_completion  # ä¸Šä¼ å®Œæˆ Future
        self.staging_future = result.staging_completion  # Staging å®Œæˆ Future

    elif self.async_mode == AsyncMode.ASYNC:
        # æ¨¡å¼ 2: Async
        self.save_future = self.dcp_save(
            states,
            checkpoint_id=checkpoint_id,
            async_mode=self.async_mode,
        )

    else:
        # æ¨¡å¼ 1: Disabled (åŒæ­¥)
        self.dcp_save(
            states,
            checkpoint_id=checkpoint_id,
            async_mode=AsyncMode.DISABLED,
        )

    # 6. æ¸…ç†æ—§ checkpoint
    self._purge_stale_checkpoints()
```

### 5.3 DCP Save çš„æ ¸å¿ƒ

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:340-426

def dcp_save(
    self,
    state_dict: dict[str, Any],
    checkpoint_id: str,
    async_mode: AsyncMode,
) -> Future | None:
    """ä½¿ç”¨ DCP API ä¿å­˜"""

    # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„ DCP API
    if async_mode == AsyncMode.ASYNC:
        # å¼‚æ­¥ä¿å­˜ï¼ˆæ¨¡å¼ 2ï¼‰
        return dcp.async_save(
            state_dict,
            checkpoint_id=checkpoint_id,
            process_group=self.pg,  # Gloo backend (CPU)
        )

    elif async_mode == AsyncMode.ASYNC_WITH_PINNED_MEM:
        # å¼‚æ­¥ + Pinned Memoryï¼ˆæ¨¡å¼ 3ï¼‰
        return dcp.async_save(
            state_dict,
            checkpoint_id=checkpoint_id,
            process_group=self.pg,
            async_checkpointer_type=AsyncCheckpointerType.PROCESS,  # ç‹¬ç«‹è¿›ç¨‹
            async_stager=self.stager,  # Pinned Memory Stager
        )

    else:
        # åŒæ­¥ä¿å­˜ï¼ˆæ¨¡å¼ 1ï¼‰
        return dcp.save(
            state_dict,
            checkpoint_id=checkpoint_id,
        )
```

**DCP API çš„å·¥ä½œåŸç†**ï¼š

```
dcp.save(state_dict, checkpoint_id) åšäº†ä»€ä¹ˆï¼Ÿ

1. åˆ†æ state_dict ä¸­çš„ DTensor
   state_dict = {
       "model.wq": DTensor(local=[1024, 4096], global=[4096, 4096], Shard(0)),
       "optimizer.wq.momentum": DTensor(local=[1024, 4096], Shard(0)),
       ...
   }

2. æ¯ä¸ª GPU ä¿å­˜è‡ªå·±çš„ local tensor
   GPU 0: ä¿å­˜ wq[0:1024, :] â†’ __0_0.distcp
   GPU 1: ä¿å­˜ wq[1024:2048, :] â†’ __1_0.distcp
   ...

3. Rank 0 ä¿å­˜å…ƒæ•°æ®
   .metadata = {
       "model.wq": {
           "shape": [4096, 4096],
           "chunks": [
               {"rank": 0, "offsets": [0, 0], "lengths": [1024, 4096]},
               {"rank": 1, "offsets": [1024, 0], "lengths": [1024, 4096]},
               ...
           ]
       },
       ...
   }

4. æ‰€æœ‰ GPU barrier åŒæ­¥
   ç¡®ä¿æ‰€æœ‰äººéƒ½ä¿å­˜å®Œæˆ
```

### 5.4 Load æµç¨‹

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:544-638

def load(self, step: int = -1) -> bool:
    """åŠ è½½ checkpoint"""

    # 1. æŸ¥æ‰¾è¦åŠ è½½çš„ step
    if step == -1:
        step = self._find_load_step()  # æ‰¾æœ€æ–°çš„
    if step == -1:
        return False  # æ²¡æœ‰ checkpoint

    # 2. åˆ›å»º checkpoint ID
    checkpoint_id = self._create_checkpoint_id(step)

    # 3. å†³å®šåŠ è½½ä»€ä¹ˆ
    if step == 0:
        # step 0 æ˜¯åˆå§‹åŒ–ï¼ŒåªåŠ è½½æ¨¡å‹
        states = self.states[MODEL].state_dict()
    else:
        # åŠ è½½å®Œæ•´ checkpoint
        states = self._flattened_model_states_sd()

    # 4. ä½¿ç”¨ DCP åŠ è½½
    self.dcp_load(states, checkpoint_id)

    return True
```

**DCP Load çš„å·¥ä½œåŸç†**ï¼š

```
dcp.load(state_dict, checkpoint_id) åšäº†ä»€ä¹ˆï¼Ÿ

1. Rank 0 è¯»å–å…ƒæ•°æ®
   .metadata â†’ çŸ¥é“æ¯ä¸ª tensor çš„åˆ†ç‰‡ä¿¡æ¯

2. æ¯ä¸ª GPU è¯»å–è‡ªå·±çš„åˆ†ç‰‡
   GPU 0: è¯»å– __0_0.distcp â†’ wq[0:1024, :]
   GPU 1: è¯»å– __1_0.distcp â†’ wq[1024:2048, :]
   ...

3. å¡«å……åˆ° state_dict
   state_dict["model.wq"] = DTensor(
       local=wq[0:1024, :],  # åªåŠ è½½è‡ªå·±çš„éƒ¨åˆ†
       global_shape=[4096, 4096],
       placement=Shard(0),
   )

4. è°ƒç”¨ load_state_dict
   model.load_state_dict(state_dict)
   optimizer.load_state_dict(state_dict)
   ...

å®Œæˆï¼æ¯ä¸ª GPU åªè¯»å–è‡ªå·±çš„åˆ†ç‰‡
```

### 5.5 Keep Latest K ç­–ç•¥

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:824-846

def _purge_stale_checkpoints(self):
    """æ¸…ç†æ—§çš„ checkpoint"""

    if self.keep_latest_k > 0 and dist.get_rank() == 0:
        # 1. æ‰«ææ‰€æœ‰ checkpoint
        discovered_checkpoints = []
        for filename in os.listdir(self.folder):
            match = re.search(r"step-(\d+)", filename)
            if match:
                step = int(match.group(1))
                path = os.path.join(self.folder, filename)
                discovered_checkpoints.append((step, path))

        # 2. æŒ‰ step æ’åº
        discovered_checkpoints.sort()

        # 3. åˆ é™¤æ—§çš„ï¼ˆä¿ç•™æœ€æ–°çš„ k ä¸ªï¼‰
        to_delete = discovered_checkpoints[:-self.keep_latest_k]

        # 4. å‘é€åˆ°åå°åˆ é™¤çº¿ç¨‹
        for _, path in to_delete:
            self.purge_queue.put(path)
```

**ä¸ºä»€ä¹ˆç”¨åå°çº¿ç¨‹åˆ é™¤ï¼Ÿ**

```
åˆ é™¤å¤§æ–‡ä»¶å¤¹å¾ˆæ…¢ï¼ˆshutil.rmtreeï¼‰:
checkpoint/step-500/ (52 GB) â†’ åˆ é™¤éœ€è¦ 30-60 ç§’

å¦‚æœåœ¨ä¸»çº¿ç¨‹åˆ é™¤ï¼š
Training â†’ [Pause] â†’ Delete step-500 (60s) â†’ [Resume]
                              â†‘
                      æµªè´¹ 1 åˆ†é’Ÿï¼

åå°çº¿ç¨‹åˆ é™¤ï¼š
Training â†’ Queue.put(step-500) â†’ Training (continues)
                  â†“
        [Background thread deletes it]

è®­ç»ƒä¸å—å½±å“ï¼
```

---

## 6. State Dict ç®¡ç†

### 6.1 ModelWrapperï¼šå¤„ç† Pipeline Parallel

Pipeline Parallel æœ‰ä¸ªé—®é¢˜ï¼šå¤šä¸ª model parts çš„å‚æ•°ä¼šå†²çªã€‚

```python
# é—®é¢˜ç¤ºä¾‹ï¼šPP=2

# Rank 0 (Stage 0: layers 0-15)
model_part_0.layers[0].wq â†’ "layers.0.wq"

# Rank 1 (Stage 1: layers 16-31)
model_part_1.layers[0].wq â†’ "layers.0.wq"  # å†²çªï¼
                                            # å®é™…æ˜¯ layers.16.wq
```

**ModelWrapper çš„è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:58-82

class ModelWrapper(Stateful):
    def __init__(self, model: nn.Module | list[nn.Module]):
        # æ”¯æŒå•ä¸ªæˆ–å¤šä¸ª model parts
        self.model = [model] if isinstance(model, nn.Module) else model

    def state_dict(self) -> dict[str, Any]:
        # ä»æ‰€æœ‰ model parts æ”¶é›† state dict
        state_dict = {
            k: v
            for sd in map(get_model_state_dict, self.model)
            for k, v in sd.items()
        }
        # è‡ªåŠ¨åˆå¹¶ï¼Œé”®ä¸ä¼šå†²çª
        # å› ä¸º Pipeline split ä¿è¯äº†å‚æ•°åå”¯ä¸€
        return state_dict

    def load_state_dict(self, state_dict: dict[str, Any]):
        # åŠ è½½åˆ°æ‰€æœ‰ model parts
        func = functools.partial(
            set_model_state_dict,
            model_state_dict=state_dict,
            options=StateDictOptions(strict=False),  # å…è®¸éƒ¨åˆ†åŠ è½½
        )
        list(map(func, self.model))
```

**ä¸ºä»€ä¹ˆ strict=Falseï¼Ÿ**

```
PP Rank 0 (Stage 0):
  state_dict åŒ…å«: layers.0-15.*

PP Rank 1 (Stage 1):
  state_dict åŒ…å«: layers.16-31.*

åŠ è½½æ—¶ï¼š
  Rank 0: åªåŠ è½½ layers.0-15.*ï¼Œå¿½ç•¥ layers.16-31.* (strict=False)
  Rank 1: åªåŠ è½½ layers.16-31.*ï¼Œå¿½ç•¥ layers.0-15.* (strict=False)

æ¯ä¸ª rank åªåŠ è½½è‡ªå·±éœ€è¦çš„éƒ¨åˆ†ï¼
```

### 6.2 Optimizer State Dict Flattening

Optimizer ä¹Ÿæœ‰ç±»ä¼¼é—®é¢˜ï¼Œéœ€è¦ **flattening**ï¼š

```python
# é—®é¢˜ï¼šOptimizer çš„ state_dict æ˜¯åŸºäº index çš„

# PP Rank 0
optimizer.state_dict() = {
    "state": {
        0: {"momentum": ...},  # å¯¹åº” layers.0.wq
        1: {"momentum": ...},  # å¯¹åº” layers.0.wk
        ...
    },
    "param_groups": [{"params": [0, 1, ...]}]
}

# PP Rank 1
optimizer.state_dict() = {
    "state": {
        0: {"momentum": ...},  # å¯¹åº” layers.16.wq (ä¸æ˜¯ layers.0!)
        1: {"momentum": ...},
        ...
    },
    "param_groups": [{"params": [0, 1, ...]}]
}

# å†²çªï¼ä¸¤ä¸ª rank éƒ½æœ‰ index 0ï¼Œä½†æŒ‡å‘ä¸åŒå‚æ•°
```

**è§£å†³æ–¹æ¡ˆï¼šFlattening**

```python
# PyTorch DCP æä¾›çš„ flattening åŠŸèƒ½

# ä¿å­˜æ—¶ï¼š
optimizer_state_dict = {
    "state": {
        "model.layers.0.wq": {"momentum": ...},  # ç”¨ FQN (å…¨é™å®šå) è€Œä¸æ˜¯ index
        "model.layers.0.wk": {"momentum": ...},
        ...
    }
}

# ç°åœ¨ä¸åŒ rank çš„ FQN ä¸ä¼šå†²çªäº†ï¼
# Rank 0: model.layers.0.wq
# Rank 1: model.layers.16.wq

# DCP ä¼šè‡ªåŠ¨å¤„ç†è¿™ä¸ªè½¬æ¢
# åœ¨ OptimizersContainer ä¸­å¯ç”¨ï¼šflatten_optimizer_state_dict=True
```

### 6.3 State Dict çš„ä¸‰ç§å½¢å¼

TorchTitan ä½¿ç”¨ä¸‰ç§ state dict å½¢å¼ï¼š

```python
# 1. Native State Dictï¼ˆåŸå§‹ï¼‰
model.state_dict() = {
    "tok_embeddings.weight": Tensor(...),
    "layers.0.attention.wq.weight": Tensor(...),
    ...
}

# 2. Sharded State Dict (FSDP/DCP)
get_model_state_dict(model) = {
    "tok_embeddings.weight": DTensor(Shard(0), ...),
    "layers.0.attention.wq.weight": DTensor(Shard(0), ...),
    ...
}

# 3. HuggingFace State Dict (è½¬æ¢å)
sd_adapter.to_hf(state_dict) = {
    "model.embed_tokens.weight": Tensor(...),  # é‡å‘½å
    "model.layers.0.self_attn.q_proj.weight": Tensor(...),
    ...
}
```

**ä½¿ç”¨åœºæ™¯**ï¼š

- **Sharded State Dict**: è®­ç»ƒä¸­ä¿å­˜/åŠ è½½ï¼ˆDCP æ ¼å¼ï¼‰
- **HuggingFace State Dict**: å¯¼å‡ºç»™ HF Transformers ä½¿ç”¨

---

## 7. HuggingFace æ ¼å¼æ”¯æŒ

### 7.1 ä¸ºä»€ä¹ˆéœ€è¦ HF æ ¼å¼ï¼Ÿ

```
é—®é¢˜ï¼šè®­ç»ƒå®Œæˆåï¼Œæƒ³ç”¨ HuggingFace Transformers æ¨ç†

TorchTitan æ ¼å¼:
checkpoint/step-10000/
â”œâ”€â”€ __0_0.distcp
â”œâ”€â”€ __1_0.distcp
â”œâ”€â”€ ...
â””â”€â”€ .metadata

HuggingFace éœ€è¦çš„æ ¼å¼:
checkpoint/
â”œâ”€â”€ config.json
â”œâ”€â”€ model.safetensors.index.json
â”œâ”€â”€ model-00001-of-00004.safetensors
â”œâ”€â”€ model-00002-of-00004.safetensors
â”œâ”€â”€ model-00003-of-00004.safetensors
â””â”€â”€ model-00004-of-00004.safetensors

å®Œå…¨ä¸åŒï¼
```

**TorchTitan çš„è§£å†³æ–¹æ¡ˆ**ï¼š`StateDictAdapter`

### 7.2 StateDictAdapter å·¥ä½œåŸç†

```python
# æ¥è‡ª: torchtitan/protocols/state_dict_adapter.py

class BaseStateDictAdapter:
    """State dict è½¬æ¢å™¨åŸºç±»"""

    def to_hf(self, state_dict: dict) -> dict:
        """TorchTitan â†’ HuggingFace"""
        raise NotImplementedError

    def from_hf(self, hf_state_dict: dict) -> dict:
        """HuggingFace â†’ TorchTitan"""
        raise NotImplementedError

    def get_hf_storage_reader(self, path: str):
        """åˆ›å»º HF æ ¼å¼è¯»å–å™¨"""
        raise NotImplementedError
```

**å®é™…ç¤ºä¾‹**ï¼ˆLlama3ï¼‰ï¼š

```python
# é‡å‘½åè§„åˆ™
LLAMA3_KEY_MAPPING = {
    # TorchTitan â†’ HuggingFace
    "tok_embeddings.weight": "model.embed_tokens.weight",
    "layers.{}.attention.wq.weight": "model.layers.{}.self_attn.q_proj.weight",
    "layers.{}.attention.wk.weight": "model.layers.{}.self_attn.k_proj.weight",
    "layers.{}.attention.wv.weight": "model.layers.{}.self_attn.v_proj.weight",
    "layers.{}.attention.wo.weight": "model.layers.{}.self_attn.o_proj.weight",
    "layers.{}.feed_forward.w1.weight": "model.layers.{}.mlp.gate_proj.weight",
    "layers.{}.feed_forward.w2.weight": "model.layers.{}.mlp.down_proj.weight",
    "layers.{}.feed_forward.w3.weight": "model.layers.{}.mlp.up_proj.weight",
    "norm.weight": "model.norm.weight",
    "output.weight": "lm_head.weight",
}

def to_hf(state_dict):
    hf_state_dict = {}
    for tt_key, tensor in state_dict.items():
        # åº”ç”¨é‡å‘½åè§„åˆ™
        hf_key = apply_mapping(tt_key, LLAMA3_KEY_MAPPING)
        hf_state_dict[hf_key] = tensor
    return hf_state_dict
```

### 7.3 ä¿å­˜ HF æ ¼å¼

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:364-421

# é…ç½®
[checkpoint]
last_save_in_hf = true  # æœ€åä¸€æ­¥ä¿å­˜ä¸º HF æ ¼å¼

# ä¿å­˜æµç¨‹
if to_hf:
    # 1. è½¬æ¢ state dict
    state_dict = self.sd_adapter.to_hf(state_dict)

    # 2. ä½¿ç”¨ HuggingFaceStorageWriter
    storage_writer = HuggingFaceStorageWriter(
        path=checkpoint_id,
        save_distributed=True,  # åˆ†å¸ƒå¼ä¿å­˜
        enable_consolidation=True,  # åˆå¹¶åˆ†ç‰‡
    )

    # 3. DCP ä¿å­˜ï¼ˆä»ç„¶æ˜¯åˆ†å¸ƒå¼ï¼‰
    dcp.save(state_dict, storage_writer=storage_writer)

    # 4. åˆå¹¶æˆæœ€ç»ˆçš„ safetensors
    # checkpoint/
    # â”œâ”€â”€ model-00001-of-00004.safetensors
    # â”œâ”€â”€ model-00002-of-00004.safetensors
    # â”œâ”€â”€ model-00003-of-00004.safetensors
    # â”œâ”€â”€ model-00004-of-00004.safetensors
    # â””â”€â”€ model.safetensors.index.json
```

### 7.4 ä» HF æ ¼å¼åŠ è½½

```toml
# é…ç½®
[checkpoint]
initial_load_in_hf = true
initial_load_path = "/path/to/hf/checkpoint"

# æˆ–è€…ä½¿ç”¨é»˜è®¤ HF assets
[model]
hf_assets_path = "/path/to/hf/llama3-8b"
```

```python
# åŠ è½½æµç¨‹
if from_hf:
    # 1. è½¬æ¢ state dictï¼ˆåˆ›å»ºæ¨¡æ¿ï¼‰
    hf_state_dict = self.sd_adapter.to_hf(state_dict)

    # 2. ä½¿ç”¨ HF Storage Reader
    hf_storage_reader = self.sd_adapter.get_hf_storage_reader(checkpoint_id)

    # 3. DCP åŠ è½½
    dcp.load(hf_state_dict, storage_reader=hf_storage_reader)

    # 4. è½¬æ¢å› TorchTitan æ ¼å¼
    state_dict = self.sd_adapter.from_hf(hf_state_dict)

    # 5. åŠ è½½åˆ°æ¨¡å‹
    model.load_state_dict(state_dict)
```

---

## 8. ä¸å¹¶è¡Œç­–ç•¥çš„é…åˆ

### 8.1 FSDP + Checkpoint

FSDP å·²ç»æŠŠå‚æ•°åˆ‡åˆ†äº†ï¼Œcheckpoint è‡ªç„¶å°±æ˜¯åˆ†å¸ƒå¼çš„ï¼š

```
FSDP (8 GPUs):
GPU 0: wq[0:512, :]
GPU 1: wq[512:1024, :]
...
GPU 7: wq[3584:4096, :]

DCP Save:
GPU 0: ä¿å­˜ wq[0:512, :] â†’ __0_0.distcp
GPU 1: ä¿å­˜ wq[512:1024, :] â†’ __1_0.distcp
...
GPU 7: ä¿å­˜ wq[3584:4096, :] â†’ __7_0.distcp

DCP Load:
GPU 0: è¯»å– __0_0.distcp â†’ wq[0:512, :]
GPU 1: è¯»å– __1_0.distcp â†’ wq[512:1024, :]
...
GPU 7: è¯»å– __7_0.distcp â†’ wq[3584:4096, :]

å®Œç¾é…åˆï¼æ— éœ€é¢å¤–é€šä¿¡
```

### 8.2 TP + Checkpoint

TP åˆ‡åˆ†å•å±‚æƒé‡ï¼Œcheckpoint ä¿å­˜çš„æ˜¯åˆ‡åˆ†åçš„ï¼š

```
TP (4 GPUs):
GPU 0: wq[:, 0:1024]    (åˆ—åˆ‡åˆ†ï¼Œå‰ 1/4)
GPU 1: wq[:, 1024:2048]
GPU 2: wq[:, 2048:3072]
GPU 3: wq[:, 3072:4096]

DCP Save:
GPU 0: ä¿å­˜ wq[:, 0:1024] â†’ __0_0.distcp
GPU 1: ä¿å­˜ wq[:, 1024:2048] â†’ __1_0.distcp
...

DCP Load:
GPU 0: è¯»å– __0_0.distcp â†’ wq[:, 0:1024]
GPU 1: è¯»å– __1_0.distcp â†’ wq[:, 1024:2048]
...

ä¹Ÿæ˜¯å®Œç¾é…åˆï¼
```

### 8.3 FSDP + TP (2D å¹¶è¡Œ)

2D å¹¶è¡Œæ›´å¤æ‚ï¼Œä½† DCP è‡ªåŠ¨å¤„ç†ï¼š

```
é…ç½®: DP=8, TP=8 (64 GPUs)

å‚æ•°å¸ƒå±€ (wq [4096, 4096]):
GPU 0:  wq[0:512, 0:512]      (DP çš„ 1/8, TP çš„ 1/8)
GPU 1:  wq[0:512, 512:1024]   (DP çš„ 1/8, TP çš„ 2/8)
...
GPU 7:  wq[0:512, 3584:4096]  (DP çš„ 1/8, TP çš„ 8/8)
GPU 8:  wq[512:1024, 0:512]   (DP çš„ 2/8, TP çš„ 1/8)
...
GPU 63: wq[3584:4096, 3584:4096] (DP çš„ 8/8, TP çš„ 8/8)

DCP Save:
æ¯ä¸ª GPU ä¿å­˜è‡ªå·±çš„åŒåˆ‡åˆ†å—

Metadata:
{
    "model.wq": {
        "shape": [4096, 4096],
        "chunks": [
            {"rank": 0, "offsets": [0, 0], "lengths": [512, 512]},
            {"rank": 1, "offsets": [0, 512], "lengths": [512, 512]},
            ...
            {"rank": 63, "offsets": [3584, 3584], "lengths": [512, 512]},
        ]
    }
}

DCP è‡ªåŠ¨æ¨æ–­ placement: [Shard(0), Shard(1)]
```

### 8.4 PP + Checkpoint

Pipeline Parallel æœ€å¤æ‚ï¼Œä½†æœ‰ ModelWrapper å¤„ç†ï¼š

```
PP (4 Stages, 16 GPUs, æ¯ä¸ª stage 4 GPUs FSDP):

Stage 0 (Rank 0-3):  layers 0-7
Stage 1 (Rank 4-7):  layers 8-15
Stage 2 (Rank 8-11): layers 16-23
Stage 3 (Rank 12-15): layers 24-31

State Dict:
Rank 0-3:
  "layers.0.wq", "layers.1.wq", ..., "layers.7.wq"

Rank 4-7:
  "layers.8.wq", "layers.9.wq", ..., "layers.15.wq"

å‚æ•°åä¸å†²çªï¼

DCP Save:
æ‰€æœ‰ rank ä¿å­˜è‡ªå·±çš„å‚æ•°
Metadata è®°å½•æ¯ä¸ª layer åœ¨å“ªä¸ª rank

DCP Load:
æ¯ä¸ª rank è¯»å–è‡ªå·±çš„ layers
ModelWrapper ç”¨ strict=False å¿½ç•¥å…¶ä»– layers
```

### 8.5 å®Œæ•´ç¤ºä¾‹ï¼š3D å¹¶è¡Œ

```
Llama3 405B on 512 H100s
é…ç½®: DP=8, TP=8, PP=8

å‚æ•°æ€»é‡: 405B Ã— 2 bytes = 810 GB

æ¯ä¸ª GPU çš„å‚æ•°:
  810 GB / 512 = 1.58 GB

Checkpoint ç»“æ„:
checkpoint/step-1000/
â”œâ”€â”€ __0_0.distcp           (1.58 GB, Rank 0)
â”œâ”€â”€ __1_0.distcp           (1.58 GB, Rank 1)
â”œâ”€â”€ ...
â”œâ”€â”€ __511_0.distcp         (1.58 GB, Rank 511)
â”œâ”€â”€ __0_optimizer_0.distcp (3.16 GB, Rank 0, Adam 2x)
â”œâ”€â”€ ...
â”œâ”€â”€ __511_optimizer_0.distcp (3.16 GB, Rank 511)
â””â”€â”€ .metadata              (è®°å½•æ‰€æœ‰åˆ†ç‰‡ä¿¡æ¯)

æ€»å¤§å°:
  å‚æ•°: 1.58 GB Ã— 512 = 810 GB
  ä¼˜åŒ–å™¨: 3.16 GB Ã— 512 = 1620 GB
  æ€»è®¡: 2430 GB (åˆ†å¸ƒåœ¨ 512 ä¸ªæ–‡ä»¶ä¸­)

ä¿å­˜é€Ÿåº¦ï¼ˆAsync + Pinned Memï¼‰:
  æ¯ä¸ª GPU å†™å…¥: 1.58 GB + 3.16 GB = 4.74 GB
  å†™å…¥æ—¶é—´: 4.74 GB / 1 GB/s = 4.74 seconds
  è®­ç»ƒæš‚åœ: ~2 seconds (Staging)

å¯¹æ¯”ä¼ ç»Ÿ checkpoint:
  éœ€è¦æ”¶é›†å®Œæ•´æ¨¡å‹: 2430 GB (OOM!)
  å†™å…¥æ—¶é—´: 2430 GB / 1 GB/s = 40 minutes

æ•ˆç‡æå‡:
  å†…å­˜: æ— é™ (ä¼ ç»Ÿæ–¹å¼æ ¹æœ¬æ— æ³•å®Œæˆ)
  é€Ÿåº¦: 500x (4.74s vs 40min)
```

---

## 9. å®æˆ˜æ¡ˆä¾‹

### 9.1 Llama3 8B (8 GPUs)

**é…ç½®**ï¼š

```toml
[checkpoint]
enable = true
folder = "checkpoint"
interval = 500
async_mode = "async"
keep_latest_k = 10
```

**Checkpoint å¤§å°**ï¼š

```
æ¨¡å‹å‚æ•°: 8B Ã— 2 bytes = 16 GB
ä¼˜åŒ–å™¨çŠ¶æ€: 16 GB Ã— 2 (Adam) = 32 GB
æ€»è®¡: 48 GB

æ¯ä¸ª GPU:
  å‚æ•°: 16 GB / 8 = 2 GB
  ä¼˜åŒ–å™¨: 32 GB / 8 = 4 GB
  æ€»è®¡: 6 GB

Checkpoint æ–‡ä»¶:
checkpoint/step-500/
â”œâ”€â”€ __0_0.distcp (2 GB)
â”œâ”€â”€ __0_optimizer_0.distcp (4 GB)
â”œâ”€â”€ __1_0.distcp (2 GB)
â”œâ”€â”€ __1_optimizer_0.distcp (4 GB)
â”œâ”€â”€ ...
â”œâ”€â”€ __7_0.distcp (2 GB)
â”œâ”€â”€ __7_optimizer_0.distcp (4 GB)
â””â”€â”€ .metadata (å‡  KB)

æ€»å¤§å°: 48 GB (8 Ã— 6 GB)
```

**æ€§èƒ½**ï¼š

```
Async æ¨¡å¼:
  Copy to CPU: 6 GB â†’ 2 seconds
  è®­ç»ƒç»§ç»­
  Background save: 6 GB @ 1 GB/s â†’ 6 seconds

è®­ç»ƒæš‚åœ: 2 seconds
ååæŸå¤±: ~0.03% (å‡è®¾ 500 æ­¥éœ€è¦ 1 hour)
```

### 9.2 Llama3 70B (256 GPUs)

**é…ç½®**ï¼š

```toml
[checkpoint]
enable = true
interval = 1000
async_mode = "async_with_pinned_mem"
keep_latest_k = 5
```

**Checkpoint å¤§å°**ï¼š

```
æ¨¡å‹å‚æ•°: 70B Ã— 2 bytes = 140 GB
ä¼˜åŒ–å™¨çŠ¶æ€: 140 GB Ã— 2 = 280 GB
æ€»è®¡: 420 GB

æ¯ä¸ª GPU:
  å‚æ•°: 140 GB / 256 = 0.547 GB
  ä¼˜åŒ–å™¨: 280 GB / 256 = 1.094 GB
  æ€»è®¡: 1.641 GB

Checkpoint æ–‡ä»¶æ•°: 256 Ã— 2 = 512 files
æ€»å¤§å°: 420 GB
```

**æ€§èƒ½**ï¼š

```
Async + Pinned Mem æ¨¡å¼:
  Stage to Pinned Memory: 1.641 GB â†’ 1 second (DMA)
  è®­ç»ƒç»§ç»­
  Process upload: 1.641 GB @ 1 GB/s â†’ 1.6 seconds

è®­ç»ƒæš‚åœ: 1 second
å‡ ä¹æ— æ„Ÿï¼
```

### 9.3 Llama3 405B (512 GPUs)

**é…ç½®**ï¼š

```toml
[checkpoint]
enable = true
interval = 500
async_mode = "async_with_pinned_mem"
keep_latest_k = 3
last_save_model_only = true
last_save_in_hf = true
export_dtype = "bfloat16"
```

**Checkpoint ç­–ç•¥**ï¼š

```
è®­ç»ƒä¸­ (æ¯ 500 æ­¥):
  ä¿å­˜å®Œæ•´ checkpoint (æ¨¡å‹ + ä¼˜åŒ–å™¨ + ...)
  æ ¼å¼: DCP åˆ†å¸ƒå¼
  ç”¨äºæ¢å¤è®­ç»ƒ

æœ€åä¸€æ­¥ (step 10000):
  åªä¿å­˜æ¨¡å‹
  æ ¼å¼: HuggingFace safetensors
  ç”¨äºæ¨ç†éƒ¨ç½²
  ç²¾åº¦: bfloat16 (èŠ‚çœç©ºé—´)
```

**Checkpoint å¤§å°**ï¼š

```
è®­ç»ƒä¸­:
  æ¨¡å‹: 405B Ã— 2 bytes = 810 GB
  ä¼˜åŒ–å™¨: 810 GB Ã— 2 = 1620 GB
  æ€»è®¡: 2430 GB
  æ¯ä¸ª GPU: 4.75 GB

æœ€åä¸€æ­¥ï¼ˆæ¨¡å‹ only + bf16ï¼‰:
  æ¨¡å‹: 405B Ã— 2 bytes = 810 GB
  æ¯ä¸ª GPU: 1.58 GB

HF æ ¼å¼ï¼ˆåˆå¹¶åï¼‰:
  model-00001-of-00008.safetensors (100 GB)
  model-00002-of-00008.safetensors (100 GB)
  ...
  model-00008-of-00008.safetensors (100 GB + 10 GB)
  model.safetensors.index.json
```

---

## 10. è°ƒè¯•ä¸ä¼˜åŒ–

### 10.1 å¸¸è§é—®é¢˜

**Q1: Checkpoint ä¿å­˜å¾ˆæ…¢**

```
ç—‡çŠ¶:
  Checkpoint ä¿å­˜è€—æ—¶ > 10 minutes

åŸå› ï¼š
1. ä½¿ç”¨ async_mode = "disabled"
2. ç£ç›˜ IO å¸¦å®½ä¸è¶³
3. æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒå¹¶è¡Œå†™å…¥

è§£å†³ï¼š
1. å¯ç”¨ async_mode = "async" æˆ– "async_with_pinned_mem"
2. æ£€æŸ¥ç£ç›˜: iostat -x 1
3. ä½¿ç”¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆLustre, GPFSï¼‰
4. å¢åŠ  intervalï¼Œå‡å°‘ä¿å­˜é¢‘ç‡
```

**Q2: OOM during checkpoint**

```
ç—‡çŠ¶:
  Checkpoint æ—¶ CUDA out of memory

åŸå› ï¼š
1. async_mode = "disabled" éœ€è¦ä¸´æ—¶å†…å­˜
2. åŒæ—¶æœ‰å¤šä¸ª async checkpoint åœ¨è¿›è¡Œ
3. Pinned Memory ä¸è¶³

è§£å†³ï¼š
1. ä½¿ç”¨ async_mode = "async_with_pinned_mem"
2. ç­‰å¾…ä¸Šä¸€æ¬¡ checkpoint å®Œæˆ:
   checkpointer.maybe_wait_for_staging()
3. è°ƒæ•´ GC ç­–ç•¥
```

**Q3: Checkpoint æ— æ³•æ¢å¤è®­ç»ƒ**

```
ç—‡çŠ¶:
  load checkpoint å¤±è´¥æˆ–æ•°å€¼ä¸å¯¹

åŸå› ï¼š
1. å¹¶è¡Œåº¦å˜åŒ–ï¼ˆè®­ç»ƒæ—¶ DP=8, æ¢å¤æ—¶ DP=16ï¼‰
2. Model ç»“æ„å˜åŒ–
3. Checkpoint æŸå

è§£å†³ï¼š
1. ä¿æŒå¹¶è¡Œåº¦ä¸€è‡´
2. ä½¿ç”¨ initial_load_model_only=trueï¼ˆåªåŠ è½½æ¨¡å‹ï¼‰
3. æ£€æŸ¥ .metadata æ–‡ä»¶æ˜¯å¦å®Œæ•´
4. ä½¿ç”¨ keep_latest_k > 1 ä¿ç•™å¤šä¸ªå¤‡ä»½
```

**Q4: ç£ç›˜ç©ºé—´ä¸è¶³**

```
ç—‡çŠ¶:
  No space left on device

åŸå› ï¼š
1. keep_latest_k = 0ï¼Œä¿ç•™æ‰€æœ‰ checkpoint
2. Checkpoint å¤ªå¤§
3. æ¸…ç†çº¿ç¨‹æ²¡æœ‰åŠæ—¶åˆ é™¤

è§£å†³ï¼š
1. è®¾ç½® keep_latest_k = 3-5
2. åªåœ¨æœ€åä¿å­˜æ¨¡å‹ï¼šlast_save_model_only = true
3. æ‰‹åŠ¨æ¸…ç†æ—§ checkpoint:
   rm -rf checkpoint/step-*
   (ä¿ç•™æœ€æ–°çš„å‡ ä¸ª)
```

### 10.2 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**æŠ€å·§ 1: é€‰æ‹©åˆé€‚çš„ async_mode**

```toml
# å°æ¨¡å‹ (< 10B)
[checkpoint]
async_mode = "async"
interval = 1000

# ä¸­ç­‰æ¨¡å‹ (10B-70B)
[checkpoint]
async_mode = "async_with_pinned_mem"
interval = 500

# å¤§æ¨¡å‹ (> 70B)
[checkpoint]
async_mode = "async_with_pinned_mem"
interval = 500
enable_first_step_checkpoint = true  # ç¬¬ä¸€æ­¥ä¹Ÿä¿å­˜ï¼ˆéªŒè¯ç³»ç»Ÿï¼‰
```

**æŠ€å·§ 2: è°ƒæ•´ keep_latest_k**

```toml
# è°ƒè¯•é˜¶æ®µ
[checkpoint]
keep_latest_k = 3  # åªä¿ç•™ 3 ä¸ªï¼Œå¿«é€Ÿè¿­ä»£

# é•¿æœŸè®­ç»ƒ
[checkpoint]
keep_latest_k = 10  # ä¿ç•™ 10 ä¸ªï¼Œé˜²æ­¢æŸå

# ç£ç›˜ç©ºé—´å—é™
[checkpoint]
keep_latest_k = 2  # æœ€å°‘ 2 ä¸ªï¼ˆä¸èƒ½ä¸º 1ï¼‰
```

**æŠ€å·§ 3: åˆ†ç¦»è®­ç»ƒå’Œå¯¼å‡º checkpoint**

```toml
# è®­ç»ƒä¸­ï¼šæ¯ 500 æ­¥ä¿å­˜å®Œæ•´ checkpoint
[checkpoint]
interval = 500
last_save_model_only = false

# æœ€åï¼šåªä¿å­˜æ¨¡å‹ + HF æ ¼å¼
last_save_model_only = true
last_save_in_hf = true
export_dtype = "bfloat16"  # èŠ‚çœç©ºé—´

æ•ˆæœ:
  è®­ç»ƒä¸­: å¯ä»¥éšæ—¶æ¢å¤
  æœ€å: å¾—åˆ°æ¨ç†ç”¨çš„ HF checkpoint
```

**æŠ€å·§ 4: åˆ©ç”¨ GC ä¼˜åŒ–å†…å­˜**

```python
# æ¥è‡ª: torchtitan/components/checkpoint.py:423-424

# DCP ä¼šè‡ªåŠ¨åœ¨ checkpoint å GC
if enable_garbage_collection:
    GarbageCollection.collect("GC collection invoked by checkpointer.")

# å¯¹äº async checkpointï¼ŒGC åœ¨ _async_wait() åè°ƒç”¨
# å› ä¸º async æ—¶ CPU å†…å­˜ä»è¢«å ç”¨
```

### 10.3 ç›‘æ§æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**ï¼š

```python
# 1. Checkpoint ä¿å­˜æ—¶é—´
begin = time.monotonic()
checkpointer.save(curr_step)
checkpoint_time = time.monotonic() - begin

logger.info(f"Checkpoint took {checkpoint_time:.2f} seconds")

# 2. Checkpoint å¤§å°
checkpoint_id = f"checkpoint/step-{step}"
checkpoint_size = sum(
    os.path.getsize(os.path.join(checkpoint_id, f))
    for f in os.listdir(checkpoint_id)
)
logger.info(f"Checkpoint size: {checkpoint_size / 1e9:.2f} GB")

# 3. Staging æ—¶é—´ï¼ˆPinned Memory æ¨¡å¼ï¼‰
if async_mode == "async_with_pinned_mem":
    checkpointer.staging_future.result()  # ç­‰å¾… staging
    logger.info(f"Staging took {staging_time:.2f} seconds")

# 4. ç£ç›˜ä½¿ç”¨
du -sh checkpoint/
```

---

## 11. æ€»ç»“

### 11.1 DCP çš„æ ¸å¿ƒä¼˜åŠ¿

ç”¨**æ¬æ¡Œå­æ‹ç…§**çš„æ¯”å–»æ€»ç»“ï¼š

1. **åˆ†å¸ƒå¼æ‹ç…§**ï¼šæ¯äººæ‹è‡ªå·±çš„éƒ¨åˆ†ï¼Œå¹¶è¡Œä¿å­˜
   - âœ… å†…å­˜å ç”¨ä½ï¼ˆä¸éœ€è¦æ”¶é›†å®Œæ•´æ¨¡å‹ï¼‰
   - âœ… é€Ÿåº¦å¿«ï¼ˆå¹¶è¡Œ IOï¼‰
   - âœ… å¯æ‰©å±•ï¼ˆæ”¯æŒä»»æ„å¤§æ¨¡å‹ï¼‰

2. **å¼‚æ­¥æ‹ç…§**ï¼šæ‹ç…§çš„åŒæ—¶ç»§ç»­æ¬æ¡Œå­
   - âœ… è®­ç»ƒå‡ ä¹ä¸é˜»å¡
   - âœ… 3 ç§æ¨¡å¼é€‚åº”ä¸åŒåœºæ™¯

3. **æ™ºèƒ½ç®¡ç†**ï¼š
   - âœ… è‡ªåŠ¨æ¸…ç†æ—§ç…§ç‰‡ï¼ˆkeep_latest_kï¼‰
   - âœ… æ”¯æŒ HF æ ¼å¼ï¼ˆå¯¼å‡ºæ¨ç†ï¼‰
   - âœ… ä¸æ‰€æœ‰å¹¶è¡Œç­–ç•¥æ— ç¼é…åˆ

### 11.2 ä½¿ç”¨å»ºè®®

```
å°æ¨¡å‹è®­ç»ƒ (< 10B, å•æœº):
  â†’ async_mode = "async"
  â†’ interval = 1000
  â†’ keep_latest_k = 5

ä¸­ç­‰æ¨¡å‹ (10B-70B, å¤šæœº):
  â†’ async_mode = "async_with_pinned_mem"
  â†’ interval = 500
  â†’ keep_latest_k = 5

å¤§æ¨¡å‹ (> 70B, å¤§è§„æ¨¡):
  â†’ async_mode = "async_with_pinned_mem"
  â†’ interval = 500
  â†’ keep_latest_k = 3
  â†’ last_save_in_hf = true (å¯¼å‡º HF)
```

### 11.3 é…ç½®é€ŸæŸ¥

```toml
# å®Œæ•´é…ç½®ç¤ºä¾‹
[checkpoint]
# åŸºç¡€
enable = true
folder = "checkpoint"
interval = 500

# Async æ¨¡å¼
async_mode = "async_with_pinned_mem"  # æˆ– "async", "disabled"

# æ¸…ç†ç­–ç•¥
keep_latest_k = 5  # ä¿ç•™æœ€æ–° 5 ä¸ª

# åˆå§‹åŠ è½½
initial_load_path = "/path/to/pretrained"  # å¯é€‰
initial_load_model_only = true  # åªåŠ è½½æ¨¡å‹
initial_load_in_hf = false  # æ˜¯å¦ä» HF åŠ è½½

# æœ€åä¿å­˜
last_save_model_only = true  # æœ€ååªä¿å­˜æ¨¡å‹
last_save_in_hf = true  # ä¿å­˜ä¸º HF æ ¼å¼
export_dtype = "bfloat16"  # å¯¼å‡ºç²¾åº¦

# å…¶ä»–
enable_first_step_checkpoint = false  # ç¬¬ä¸€æ­¥æ˜¯å¦ä¿å­˜
exclude_from_loading = []  # åŠ è½½æ—¶æ’é™¤çš„ç»„ä»¶
```

### 11.4 ä¸å¹¶è¡Œç­–ç•¥çš„å…³ç³»

```
FSDP:
  å‚æ•°å·²ç»åˆ†ç‰‡ â†’ DCP ç›´æ¥ä¿å­˜åˆ†ç‰‡ â†’ å®Œç¾é…åˆ

TP:
  å•å±‚æƒé‡åˆ‡åˆ† â†’ DCP ä¿å­˜åˆ‡åˆ†åçš„ â†’ å®Œç¾é…åˆ

PP:
  å¤šä¸ª model parts â†’ ModelWrapper ç»Ÿä¸€ç®¡ç† â†’ å®Œç¾é…åˆ

FSDP + TP + PP (3D):
  å‚æ•°åœ¨ 3 ä¸ªç»´åº¦åˆ‡åˆ† â†’ DCP è‡ªåŠ¨æ¨æ–­ placement â†’ å®Œç¾é…åˆ

ç»“è®º: DCP ä¸æ‰€æœ‰å¹¶è¡Œç­–ç•¥æ— ç¼é›†æˆï¼
```

### 11.5 å…³é”®æºç 

```
æ ¸å¿ƒæ–‡ä»¶:
- torchtitan/components/checkpoint.py:118-846
  - CheckpointManager: ä¸»ç±»
  - ModelWrapper: PP æ”¯æŒ
  - save/load: ä¿å­˜å’ŒåŠ è½½

é…ç½®:
- torchtitan/config/job_config.py:421-550
  - Checkpoint é…ç½®ç±»

PyTorch DCP API:
- torch.distributed.checkpoint.save
- torch.distributed.checkpoint.async_save
- torch.distributed.checkpoint.load
```

---

## 12. å‚è€ƒèµ„æ–™

**æºç æ–‡ä»¶**ï¼š
- `torchtitan/components/checkpoint.py` - CheckpointManager å®ç°
- `torchtitan/config/job_config.py:421-550` - Checkpoint é…ç½®
- `torchtitan/protocols/state_dict_adapter.py` - HF æ ¼å¼è½¬æ¢

**PyTorch å®˜æ–¹æ–‡æ¡£**ï¼š
- [Distributed Checkpoint](https://pytorch.org/docs/stable/distributed.checkpoint.html)
- [Async Checkpoint](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)

**ç›¸å…³æ–‡æ¡£**ï¼š
- [01_fsdp2_per_parameter_sharding.md](./01_fsdp2_per_parameter_sharding.md) - FSDP2 å®ç°
- [02_tensor_parallel_implementation.md](./02_tensor_parallel_implementation.md) - TP å®ç°
- [05_pipeline_parallel.md](./05_pipeline_parallel.md) - PP å®ç°

**å­¦æœ¯è®ºæ–‡**ï¼š
- PyTorch Distributed Checkpoint: Efficient State Persistence for Large-Scale Training

---

**æœ€åæ›´æ–°**ï¼š2025å¹´1æœˆ

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0
